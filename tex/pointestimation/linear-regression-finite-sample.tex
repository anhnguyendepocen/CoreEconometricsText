% Copyright Â© 2013, Gray Calhoun.  Permission is granted to copy,
% distribute and/or modify this document under the terms of the GNU
% Free Documentation License, Version 1.3 or any later version
% published by the Free Software Foundation; with no Invariant
% Sections, no Front-Cover Texts, and no Back-Cover Texts.  A copy of
% the license is included in the section entitled "GNU Free
% Documentation License."

\part*{Finite sample properties of Linear regression}%
\addcontentsline{toc}{part}{Finite sample properties of linear regression}

   \textit{2012-09-18 Tue}, \textit{2012-09-20 Thu}, \textit{2012-09-25 Tue}, 
   \textit{2012-09-27 Thu}, \textit{2012-10-02 Tue}
\section{Lin. regression motivation}
\label{sec-1}
\subsection{more formal discussion}
\label{sec-1-1}
\paragraph{how to think about $\varepsilon$}
\label{sec-1-1-1}

\begin{itemize}
\item have a variable you want to understand
\begin{itemize}
\item $Y_i$
\end{itemize}
\item have a model and variables you think explain it
\begin{itemize}
\item $g(X_{i,0},\dots,X_{i,k})$
\item in our case, $g(\cdot) = X_{i,0}\beta_0 + \dots + X_{i,k}\beta_k$
\end{itemize}
\item error is the difference between the variable and the model
\begin{itemize}
\item $\varepsilon_i = Y_i - g(X_{i,0},\dots,X_{i,k})$
\end{itemize}
\item why is this a good way to think about it?
\begin{itemize}
\item any model you write down is going to give you an error
\item the model \underline{doesn't have to be true}
\item different models will give you different errors
\item so, when you think about properties and assumptions about the
          error term, we're talking about the difference between the
          thing we're modeling (Y) and the model we've written down.
\begin{itemize}
\item we're \textbf{not} talking about ``the truth''
\end{itemize}
\end{itemize}
\item so you can think of $\varepsilon(g)$ where $g$ is the function
\begin{itemize}
\item then $Y_i = g(X_{i,0},\dots,X_{i,k}) + \varepsilon_i(g)$
          \underline{always} holds.
\item for any given $g$, we can calculate the errors.
\item ``regression'' means using a function of the $\varepsilon_i(g)$-s
          to estimate $g$ subject to some sort of constraint
\begin{itemize}
\item ie (what you're used to)
            \[\hat g = argmin_g n^{-1} \sum_{i=1}^n \varepsilon_i(g)^2\]
            subject to the condition that
            \[g(X_{i,0}, \dots, X_{i,k}) = \beta_0 X_{i,0}+ \cdots +
            \beta_k X_{i,k}\]
            gives the OLS estimator.
\end{itemize}
\end{itemize}
\end{itemize}
\paragraph{usual assumptions on $\varepsilon$}
\label{sec-1-1-2}

      Discussed in Greene, Chapter 2.

\begin{enumerate}
\item $E(\varepsilon_i \mid X) = 0$ for all $i$
\item $var(\varepsilon_i \mid X) = \sigma^2$
\item $cov(\varepsilon_i, \varepsilon_j \mid X) = 0$ for $i \neq j$
\item $\varepsilon_i \sim iid(0,\sigma^2)$ given $X$
\item $\varepsilon_i \sim iid\; N(0,\sigma^2)$ given $X$
\end{enumerate}

      note that 5 implies 4 which implies 1, 2, and 3.
\paragraph{how to view this in light of the previous discussion:}
\label{sec-1-1-2-1}

       There exists a particular parameter value $\beta$ such that
\begin{enumerate}
\item $E(Y_i - X_i'\beta \mid X) = 0$ for all $i$
\item $var(Y_i - X_i'\beta \mid X) = \sigma^2$ for all $i$
\item etc.
\end{enumerate}

       And we define $\varepsilon = Y - X\beta$ for that particular value
       of $\beta$.
\subsection{Derivation of OLS as MLE}
\label{sec-1-2}

\begin{itemize}
\item Covered separately on
\begin{itemize}
\item \textit{2010-10-21 Thu}
\end{itemize}
\end{itemize}
\paragraph{from last time}
\label{sec-1-2-1}

      We have the model $Y = X\beta + \varepsilon$
\begin{itemize}
\item $Y$ is the response vector
\item $X$ is the design matrix
\item $\beta$ is a vector of unknown coefficients
\item for now, we'll assume that $\varepsilon \sim N(0,\sigma^2 I)$
\end{itemize}
\paragraph{Expected value lies in the plane generated by $X$}
\label{sec-1-2-2}

      The expected value of $Y_i$ is linear in $\beta$:
      \[ E(Y \mid X) = X\beta \]
\paragraph{do all of this for $n=2$ and then for $n = 3$}
\label{sec-1-2-2-1}

\begin{itemize}
\item draw plane for $n = 3$ and $k = 1$
\item draw spheres for the different epsilons (sphere here is a
         consequence of normality and independence)
\begin{itemize}
\item each layer of the sphere is an iso-probability line
\end{itemize}
\end{itemize}
\paragraph{estimation}
\label{sec-1-2-3}

\begin{itemize}
\item if that's the way $Y$ is generated, how would we think about
        estimating $\beta$?
\item draw a new picture:
\begin{itemize}
\item start with $Y$ and $X$ given
\item choose some $\beta = B_1$
\item draw different spheres for the ``epsilon'' corresponding to that mean
\item density (likelihood) evaluated for that $\beta$ has some value, ie
\begin{itemize}
\item $f(Y \mid X; \beta = B_1) = c_1$
\end{itemize}
\end{itemize}
\item draw a second picutre corresponding to $\beta = B_2$
\item we can judge which of the values of $\beta$ is more plausible by
        looking at the density function, and choosing the one that's larger.
\item this is just MLE.
\item informally, it's clear that the coefficients that maximize the
        likelihood are going to be the coefficients that minimize the
        distance between $Y$ and $\hat Y$
\end{itemize}
\paragraph{a few ways to motivate the estimator}
\label{sec-1-2-3-1}

\begin{enumerate}
\item We know that $P_X = X(X'X)^{-1}X'$
\begin{itemize}
\item is a projection matrix
\item ie if $Z$ is any $n\times1$ vector, $P_X Z$ is an element of
            $R^n$ such that
\begin{itemize}
\item $P_X Z = X \alpha$ for some $\alpha$
\item $| Z - P_X Z |$ is minimized
\end{itemize}
\end{itemize}
\item so, we can jump ahead and say:
\begin{itemize}
\item $\hat Y = P_X Y = X \hat\beta$
\item $\hat \beta = (X'X)^{-1} X'Y$
\end{itemize}
\item We can also solve the minimization problem directly:
\begin{itemize}
\item Define $\hat \beta = argmin_\beta \left(\sum_{i=1}^n (y_i -
            x_i'\beta)^2 \right)^{1/2}$
\item know that $\hat \beta$ solves the first order conditions:
            \[ 0 = \sum_{i=1}^n {\partial \over \partial \beta_j} (y_i -
            x_i'\beta)^2 = 2 \sum_{i=1}^n (y_i - x_i'\beta) x_{ij} \]
\item written out in matrix form: $0 = X'(Y - X\beta)$ which
            becomes $X'X\beta = X'Y$ or $\beta = (X'X)^{-1}X'Y$
\begin{itemize}
\item which requires that $X'X$ be invertible and that $X$ has
              full rank.
\end{itemize}
\end{itemize}
\item explain that $\hat Y$ is well defined without full rank, but
          $\hat \beta$ is not
\begin{itemize}
\item draw picture again, but with three regressors
\item third regressor lies in the same plane as the first two.
\end{itemize}
\item Finally, we can actually solve the maximization problem.
\begin{itemize}
\item under normality, conditional log-likelihood of $\beta$ given
            $X$ is 
            \[ \log L(\beta; Y \mid X) = const + n \log \sigma - {1
            \over 2 \sigma^2} \sum_{i = 1}^n (y_i - x_i'\beta)^2 \]
            and maximizing this with respect to $\beta$ is \textbf{exactly} the
            same problem as minimizing the SSR.
\end{itemize}
\item HOMEWORK - under these same assumptions, calculate the MLE of
          $\sigma^2$ and derive its bias.
\end{enumerate}
\paragraph{definitions}
\label{sec-1-2-4}

\begin{itemize}
\item $\hat\beta = (X'X)^{-1}X'Y = \left(\sum_i x_i x_i'\right) \sum_i
        x_i y_i$ -- OLS Estimator
\item $\hat Y = X \hat \beta$ -- Fitted Values
\item $\hat \epsilon = Y - \hat Y$ -- OLS Residuals
\end{itemize}
\section{algebraic properties}
\label{sec-2}

\begin{itemize}
\item \textit{2009-10-21 Wed}, \textit{2009-10-26 Mon}
\item \textit{2010-11-02 Tue}, \textit{2010-11-04 Thu}
\item \textit{2011-10-27 Thu}
\end{itemize}
\subsection{draw picture to explain multicollinearity}
\label{sec-2-1}

     start with writing $Y$ and $X$ explicitly (for n = 3)
\begin{itemize}
\item Y = (1.5, 3, 0.5)
\item X is given by

\begin{center}
\begin{tabular}{rrr}
 1  &  3  &    2  \\
 1  &  1  &    1  \\
 1  &  2  &  1.5  \\
\end{tabular}
\end{center}


\item draw plot and show that the third column is in the plane
       generated by the first two
\item obviously, we're not going to be able to estimate the
       coefficients separately
\item we can still estimate the fitted values and they're not going to
       be any different than if we dropped one of the columns.
\end{itemize}
\subsection{draw picture to explain adding an observation}
\label{sec-2-2}

     start with the first two observations from above, with just the 2nd
     column of $X$.
\begin{itemize}
\item draw projection in $R^2$.
\item add third observation and draw $R^3$.
\begin{itemize}
\item it seems like there should be a way to update the estimate of
         $\beta$ using the previous one and the new observation
\item there is (you get to derive it for homework, but you don't need
         to know the formula by memory).
\end{itemize}
\end{itemize}
\subsection{draw a picture to explain adding a variable}
\label{sec-2-3}

\begin{itemize}
\item suppose you regress $Y$ on $X_1$ and get an estimate
      $\tilde\beta_1$, then decide to regress $Y$ on $X_1$ and $X_2$,
      giving $\hat\beta_1$ and $\hat\beta_2$.
\item Frisch-Waugh-Lovel Theorem (useful for getting coefficent
      estimates for $X_2$
\begin{itemize}
\item let $e = Y - X_1\tilde \beta$.
\item define $Z = (I - X_1(X_1'X_1)X_1')X_2$
\begin{itemize}
\item residuals from regressing each column of $X_2$ on $X_1$.
\end{itemize}
\item $\hat\beta_2 = (Z'Z)^{-1}Z'e$
\end{itemize}
\item similar formulae exist for getting $\hat\beta_1$ from $\tilde
      \beta_1$
\item can be useful with massive datasets
\begin{itemize}
\item or for understanding code
\end{itemize}
\item Implication: what if $X_1$ and $X_2$ are orthogonal?
\begin{itemize}
\item reduces to OLS on the second regressor
\end{itemize}
\end{itemize}
\subsection{R-square}
\label{sec-2-4}

\begin{itemize}
\item draw picture again.
\item may want to know how far $Y$ is from $\hat Y$
\item a good comparison is how far relative to the distance between $Y$
       and $\iota \bar Y$
\item ie the ratio $\frac{ \lVert Y - \hat Y \rVert_2^2}{\lVert Y - \iota
       \bar Y \rVert^2}$
\begin{itemize}
\item centered R-squared
\item if model contains a constant, this will be between zero and one.
\end{itemize}
\end{itemize}
\section{statistical properties}
\label{sec-3}

\begin{itemize}
\item \textit{2009-10-14 Wed}, \textit{2009-10-19 Mon},
\item \textit{2010-10-26 Tue}
\item \textit{2011-10-27 Thu}
\item need to move a lot of this stuff to the ``stats'' part
\end{itemize}
\subsection{small sample properties of the estimator}
\label{sec-3-1}
\paragraph{unbiasedness}
\label{sec-3-1-1}

\begin{itemize}
\item Suppose that $Y = X\beta + \varepsilon$
\begin{itemize}
\item $E(\varepsilon \mid X) = 0$
\item $X$ has full rank
\end{itemize}
\item then $E \hat \beta = \beta$.
\end{itemize}
\paragraph{proof}
\label{sec-3-1-1-1}

\begin{itemize}
\item we're going to use the law of iterated expectations:
         $E(\hat \beta \mid X) = E((X'X)^{-1} X'Y \mid X) = (X'X)^{-1}
           X'E(Y \mid X) = (X'X)^{-1} X'E(X\beta + \varepsilon \mid X) =
         (X'X)^{-1} X'X \beta = \beta$
\item note that this is a slightly stronger result: the expected value
         of the OLS coefficient estimator is $\beta$ \emph{regardless} of the
         value of $X$
\end{itemize}
\paragraph{variance of $\hat \beta$}
\label{sec-3-1-2}

\begin{itemize}
\item Suppose that $Y = X\beta + \varepsilon$
\begin{itemize}
\item $E(\varepsilon \mid X) = 0$
\item $E(\varepsilon \varepsilon' \mid X) = \sigma^2 I$
\item $X$ has full rank
\end{itemize}
\item then $var(\hat \beta \mid X) = \sigma^2 (X'X)^{-1}$
\end{itemize}
\paragraph{proof}
\label{sec-3-1-2-1}

\begin{itemize}
\item note that $\hat\beta - \beta = (X'X)^{-1}X'\varepsilon$
\item $var(\hat \beta \mid X) = E((\hat\beta -
         \beta)(\hat\beta-\beta)' \mid X)$
\item = $E((X'X)^{-1}X'\varepsilon\varepsilon'X(X'X)^{-1} \mid X)$
\item = $(X'X)^{-1} X' E(\varepsilon\varepsilon' \mid X) X (X'X)^{-1}$
\item = \$$\sigma$$^2$ $(X'X)^{-1} X'X (X'X)^{-1}$
\item = $\sigma^2 (X'X)^{-1}$
\end{itemize}
\paragraph{remarks}
\label{sec-3-1-2-2}

\begin{itemize}
\item depends on values of the regressors
\item unconditional variance: $var \hat \beta = \sigma^2 E
         (X'X)^{-1}$
\end{itemize}
\paragraph{results on quadratic forms}
\label{sec-3-1-3}

\begin{itemize}
\item Let $X$ be a random $k$-vector and let $A$ be a $k \times k$
        deterministic matrix.  $X'A X$ is a (random) quadratic form.
\begin{itemize}
\item normally we view each element of $X$ as coming from the same (or
          similar) distributions
\end{itemize}
\item $X'AX = \sum_{i,j} X_i X_j A_{ij}$
\begin{itemize}
\item write this out using element-by-element representation of $X$ and $A$
\end{itemize}
\item usually we only think about symmetric matrices:
        \[ X' A X = X'(A/2 + A'/2)X \quad a.s.\]
        which is always symmetric
\end{itemize}
\paragraph{mean (from Seber and Lee) \textbf{:hw:}}
\label{sec-3-1-3-1}

\begin{description}
\item[result] Let $X$ be an $n\times 1$ vector of random variables,
                  and let $A$ be an $n \times n$ symmetric matrix.  If $E
                  X = \mu$ and $var(X) = \Sigma$, then \[ E X'AX = tr(A
                  \Sigma) + \mu'A\mu \]
\item[proof] we're going to use three things: a scalar equals its own
                 trace, linearity of the expectation, and a property of
                 the trace: trace(XY) = trace(YX) as long as both are
                 conformable.
\begin{itemize}
\item $E(X'AX) = tr(E(X'AX))$
\item $= E(tr(X'AX))$
\item $= E(tr(AXX'))$
\item $= tr(E(AXX'))$
\item $= tr(A E(XX'))$
\item $= tr(A (var(X) + \mu\mu'))$
\item $= tr(A\Sigma) + tr(A \mu\mu')$
\item $= tr(A\Sigma) + \mu'A\mu$
\end{itemize}
\end{description}
\paragraph{variance (also from Seber and Lee)}
\label{sec-3-1-3-2}

\begin{description}
\item[result] let $X_1,\dots,X_n$ be independent r.v. with means
                  $\theta_1,\dots,\theta_n$, and the same 2nd, 3rd and
                  4th central moments $\mu_2$, $\mu_3$, $\mu_4$.  If $A$
                  is an $n\times n$ symmetric matrix and $a$ is a column
                  vector of the diagonal elements of $A$, then
                  \[ var(X'AX) = (\mu_4 - 3 \mu_2^2)a'a + 2 \mu_2^2
                  tr(A^2) + 4 \mu_2 \theta'A^2 \theta + 4 \mu_3 \theta' A
                  a \]
\item[proof] Take $X'A X = \sum_{ij} ((X_i - theta_i) + \theta_i) ((X_j - \theta_j) + \theta_j) A_{ij}$, multiply out and take the variance.
\end{description}
\paragraph{\textbf{homework} Prove that $\hat\beta$ and $\hat\varepsilon$ are uncorrelated.}
\label{sec-3-1-4}
\paragraph{how does $\hat\beta$ compare to other estimators of $\beta$?}
\label{sec-3-1-5}

\begin{itemize}
\item what other estimators might we want to compare $\beta$ to?
\item linear estimators first: OLS can be viewed as a weighted average:
        $\hat \beta = (X'X) X'Y = \sum_{i=1}^n w_i y_i$ where $w_i$ is a
        $k$-vector, $w_i = (X'X)^{-1} x_i$
\item how does OLS compare to all other unbiased weighted averages:
        $\sum_i v_i y_i$ where $v_i$ is a function of $X$?
\end{itemize}
\paragraph{Gauss Markov Theorem}
\label{sec-3-1-5-1}

\begin{itemize}
\item Assume that $Y = X\beta + \varepsilon$ where $\varepsilon \sim
         (0, \sigma^2 I)$ given $X$.  Then $\hat\beta$ is the unique
         estimator with minimum variance (given $X$) among all linear,
         unbiased estimators (OLS is BLUE).
\end{itemize}
\paragraph{proof}
\label{sec-3-1-5-1-1}

\begin{itemize}
\item First, what does ``minimum variance'' mean?  Just like it did
          earlier in the semester
\begin{itemize}
\item means that $\var(V'Y \mid X) - \var(\hat\beta \mid X)$ is
            positive definite.
\end{itemize}
\end{itemize}

        Suppose that $V$ is $k \times n$ matrix s.t. $V'Y$ is unbiased
        for $\beta$.
\begin{itemize}
\item here's how the proof will work:
\begin{itemize}
\item we can write down immediately that $V'Y = (X'X)^{-1}X'Y +
            [V'Y - (X'X)^{-1} X'Y]$
\item We're going to show that these two parts are uncorrelated.
\item That way, $var(V'Y \mid X) = var(\hat \beta \mid X) +
            var(V'Y - (X'X)^{-1}X'Y \mid Y)$ and so we get the result
            automatically.
\end{itemize}
\item basic setup
\begin{itemize}
\item We know that $V'Y = V'X\beta + V'\varepsilon$
\item Then $\beta = E(V'Y \mid X) = E(V'X\beta + V'\varepsilon \mid
            X) = V'X \beta + V'E(\varepsilon \mid X) = V'X\beta$
\item This holds for any choice of $\beta$, so $V'X = I$
\item as a result, $V'Y - \beta = V'\varepsilon$
\end{itemize}
\item main step
\begin{itemize}
\item so now we just calculate the covariance between
            $(X'X)^{-1}X'Y$ and $(V - (X'X)^{-1}X')Y$
\item equals $E((X'X)^{-1}X'\varepsilon \varepsilon'(V -
            X(X'X)^{-1}) \mid X)$
\item $= \sigma^2 (X'X)^{-1}X'(V - X(X'X)^{-1})$
\item $= \sigma^2 [(X'X)^{-1}X'V - (X'X)^{-1} X'X (X'X)^{-1}]$
\item both equal $(X'X)^{-1}$, so the whole term is zero and we've
            shown that the covariance is zero.
\end{itemize}
\end{itemize}
\paragraph{discussion}
\label{sec-3-1-5-1-2}

\begin{itemize}
\item this illustrates a \textbf{key} strategy
\begin{itemize}
\item took an arbitrary linear unbiased estimator for $\beta$
\item showed that it could be broken down into two pieces
\begin{itemize}
\item the ols estimator $\hat\beta$
\item some orthogonal noise component
\end{itemize}
\item so these estimators are just the efficient estimator plus
            some noise.
\end{itemize}
\item proved result \textbf{conditional on X}, unconditional extension is easy:
\begin{itemize}
\item If $var(\hat\beta \mid X) \leq var(V'Y \mid X)$ for all
            values of $X$, then $E var(\hat\beta \mid X) \leq E var(V'Y
            \mid X)$ as well
\item $var(\hat\beta) = E(var(\hat\beta \mid X)) + var(E(\hat\beta
            \mid X))$
\begin{itemize}
\item second term equals zero
\end{itemize}
\item $var(V'Y) = E(var(V'Y \mid X)) + var(E(V'Y \mid X))$
\begin{itemize}
\item second term also equals zero
\end{itemize}
\end{itemize}
\item under normality, can prove an even better result: the ols
          estimator is the best unbiased estimator (even allowing for
          nonlinear estimators)
\begin{itemize}
\item do this for a single regressor for homework.
\end{itemize}
\end{itemize}
\subsection{Results for Normal random variables}
\label{sec-3-2}

\begin{itemize}
\item reading
\begin{itemize}
\item Greene A.7 (maybe A.8)
\item Greene B.4, B.9, B.11
\item CB ch 3 and 5.3 (5.3 covers normal r.v.)
\item CB has an appendix listing common densities, etc.
\end{itemize}
\item covered on
\begin{itemize}
\item \textit{2009-09-14 Mon}, \textit{2009-09-09 Wed}
\item \textit{2010-09-09 Thu}, \textit{2010-09-14 Tue}
\item \textit{2011-09-06 Tue}, \textit{2011-09-08 Thu}
\end{itemize}
\item Density, etc. are in Casella Berger appendix
\item multivariate: mean $\mu$ and variance $\Sigma$:
      \[ f(x) = \frac{1}{\sqrt{2^n \pi^n det(\Sigma)}} exp(-1/2(x -
      \mu)'\Sigma^{-1}(x- \mu)) \]
\item written $N(\mu, \Sigma)$
\item assumes $\Sigma$ is p.d.; can be extended to allow for positive
      semi-definite as well.
\item you should draw the shape of the density and some elliptical
      contour plots
\end{itemize}
\paragraph{important properties: \textbf{:hw:}}
\label{sec-3-2-1}

\begin{itemize}
\item If $X$ is a $k$-dimensional multivariate normal with mean $\mu$
        and variance $\Sigma$, $A X + b$ is also multivariate normal for
        any constant $n \times k$ matrix $A$ and $n$-vector $b$.
\begin{itemize}
\item mean is $A \mu + b$
\item variance is $A \Sigma A'$
\item if $n > k$, it will be a degenerate distribution.
\item example: $X \sim N(0,1)$, $A = (1/\sqrt{2}, 1/\sqrt{2})$ and
            $b = 0$.
\begin{itemize}
\item draw picture (rotation)
\end{itemize}
\end{itemize}
\item If $X$ is multivariate normal with mean $\mu$ and variance
        $\Sigma$, and $\Sigma^{1/2}$ is a symmetric matrix such that
        $\Sigma^{1/2} \Sigma^{1/2} = \Sigma$, then
        $(\Sigma^{1/2})^{-1} (X - \mu)$ is multivariate standard
        normal.
\item independence
        If $X$ and $Y$ are uncorrelated normal random vectors, then they
        are independent
\begin{itemize}
\item follows from multiplying out square terms in density and
          factorizing.
\end{itemize}
\item if $(X_1,X_2) \sim N((\mu_1,\mu_2), \begin{pmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{12}' & \Sigma_{22} \end{pmatrix}$ then
\begin{itemize}
\item $X_1 \sim N(\mu_1, \Sigma_{11})$
\item $X_2 \sim N(\mu_2, \Sigma_{22})$
\item $X_1 \mid X_2 \sim N(\mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (X_2 - \mu_2), \Sigma_{11} - \Sigma_{12}'\Sigma_{22}^{-1} \Sigma_{12})$
\begin{itemize}
\item note that the conditional mean of $X_1$ depends on $X_2$, but the conditional variance doesn't.
\end{itemize}
\end{itemize}
\item a few consequences of independence:
        Let $X$ be a $k$-dimensional standard normal random vector.
\begin{enumerate}
\item If $A$ is a $k \times j$ matrix and $B$ a $k \times l$ matrix
           such that $A'B = 0$, then $A'X$ and $B'X$ are independent.
\item If $P$ is a $k\times k$ idempotent matrix and $PB = 0$ then
           $X'PX$ and $B'X$ are independent
\begin{itemize}
\item A matrix $P$ is idempotent if $P = PP$
\item These represent projections in $\mathbb{R}^k$
\begin{itemize}
\item suppose some matrix $P$ represents a projection mapping $z$ to a linear subspace of $z$.
\item clearly, applying $P$ to $Pz$ shouldn't do anything, since $Pz$ is already in the subspace
\item this implies that a projection should satisfy $Pz = P^2 z$ for all $z$, or $P = P^2$
\end{itemize}
\end{itemize}
\item if $P$ and $Q$ are idempotent and $PQ = 0$ then $X'PX$ and
           $X'QX$ are independent.
\end{enumerate}
\end{itemize}
\paragraph{derived distributions}
\label{sec-3-2-2}
\paragraph{chi-squared distribution}
\label{sec-3-2-2-1}
\paragraph{definition}
\label{sec-3-2-2-1-1}

       Let $X_1,\dots,X_k$ be independent standard normal.  The
       chi-squared distribution with $k$ degrees of freedom is the
       distribution of $\sum_{i=1}^k X_i^2$.
\paragraph{theorem}
\label{sec-3-2-2-1-2}

       Let $X \sim N(0, I_n)$ and let $P$ be a symmetric $n\times n$
       matrix.  Then $X'PX$ is chi-squared with $k$ degrees of freedom
       iff $P$ is idempotent with rank $k$.
\paragraph{other results}
\label{sec-3-2-2-1-3}

\begin{itemize}
\item if $X$ and $Y$ are independent chi-squared with $df_X$ and
         $df_Y$ degrees of freedom, then $X + Y$ is chi-square with
         $df_X + df_Y$ degrees of freedom.
\end{itemize}
\paragraph{t-distribution}
\label{sec-3-2-2-2}

      If $X$ is a standard normal and $Y$ is a chi-squared with $n$ dof
      and $X$ and $Y$ are independent, then $X/\sqrt{Y/n}$ is a $t(n)$
      random variable.
\paragraph{F-distribution}
\label{sec-3-2-2-3}

       if $X$ and $Y$ are independent chi-squared with $df_X$ and $df_Y$
       degrees of freedom, then $\frac{X/df_X}{Y/df_Y}$ is an $F(df_X,
       df_Y)$ random variable.
\subsection{finite-sample distribution under normality of the errors.}
\label{sec-3-3}

\begin{itemize}
\item We've discussed unbiasedness and variance
\begin{itemize}
\item neither depend on the distribution of the errors
\end{itemize}
\item but we usually care about the distribution of our estimators too
\item working out unconditional distribution will be tricky
\begin{itemize}
\item remember variance depends on $X$
\item so distribution will depend on the distribution of $X$
\end{itemize}
\item rembember $\hat\beta = (X'X)^{-1} X'Y$
\item new estimator $s^2 = (n-k-1)^{-1} \sum_{i=1}^n \hat\epsilon_i^2$
\begin{itemize}
\item can also be written as
\begin{itemize}
\item $(n-k-1)^{-1} \sum_i (y_i - x_i'\hat\beta)^2$
\item $= (n-k-1)^{-1} (Y - X\hat\beta)'(Y - X\hat\beta)$
\item $= (n-k-1)^{-1} (Y - X(X'X)^{-1}X'Y)'(Y - X(X'X)^{-1}X'Y)$
\item $= (n-k-1)^{-1} Y'(I - X(X'X)^{-1}X')'(I - X(X'X)^{-1}X')Y$
\item $= (n-k-1)^{-1} Y'(I - X(X'X)^{-1}X')'Y$
\item $= (n-k-1)^{-1} (X\beta + \epsilon)'(I -
           X(X'X)^{-1}X')'(X\beta + \epsilon)$
\item $= (n-k-1)^{-1} \epsilon'(I - X(X'X)^{-1}X')'\epsilon$
\end{itemize}
\end{itemize}
\item from hw, you know that $E s^2 = \sigma^2$.
\item We'll work out the distribution of $\hat\beta$ and $s^2$
       conditional on the regressors under the assumption that the
       errors are normal
\begin{itemize}
\item corresponds to MLE
\item will give intuition that continues to hold without normality.
\end{itemize}
\end{itemize}
\paragraph{$\hat\beta$}
\label{sec-3-3-1}

\begin{itemize}
\item Assume $Y = X\beta + \epsilon$ with $\epsilon \sim N(0,\sigma^2 I)$.
\item $\hat\beta - \beta = (X'X)^{-1} X'\epsilon$
\item conditional on $X$, we have a constant matrix ($(X'X)^{-1}X'$)
        times a normal vector.
\item So $\hat\beta$ is $N(\beta, \sigma^2 (X'X)^{-1})$ given $X$.
\item to get the unconditional distribution, would need to know the
        density of $X$, then integrate over the joint density, which we
        won't do.
\item a useful unconditional result:
\begin{itemize}
\item let $q_i$ be the $i,i$ element of $(X'X)^{-1}$.
\item we know that $\hat\beta_i \sim N(\beta_i, \sigma^2 q_i)$ given $X$.
\item so we can look at the unconditional density of
          $(\hat\beta_i - \beta_i)/\sqrt{q_i \sigma^2}$:
\begin{itemize}
\item $f_{(\hat\beta_i - \beta_i)/\sqrt{q_i \sigma^2}}(b) = \int f_{(\hat\beta_i -
            \beta_i)/\sqrt{q_i \sigma^2}, q_i} (b, q) dq$
\item $= \int f_{(\hat\beta_i - \beta_i)/\sqrt{q_i \sigma^2}}(b \mid q_i = q)
            f_{q_i}(q) dq$
\item $= \int \phi(b) f_{q_i}(q) dq$
\item $= \phi(b) \int f_{q_i}(q) dq$
\item $= \phi(b)$
\end{itemize}
\item points
\begin{itemize}
\item once we divide by the (random) variance, we get an
            unconditional result
\item seems kind of obvious, but is worth emphasizing.
\end{itemize}
\end{itemize}
\end{itemize}
\paragraph{residuals and fitted values}
\label{sec-3-3-2}

\begin{itemize}
\item Defined the residuals as a function of $\hat\beta$, so if we
        know the distribution of the coefficient estimators, we know the
        distribution of the residuals
\begin{itemize}
\item again, need to condition on $X$
\end{itemize}
\item directly:
\begin{itemize}
\item $\hat\epsilon = (I - X(X'X)^{-1}X') \epsilon$
\item Given $X$, this is $N(0, I - X(X'X)^{-1}X')$
\end{itemize}
\item fitted values
\begin{itemize}
\item $\hat Y \sim N(X\beta, X(X'X)^{-1}X')$
\end{itemize}
\end{itemize}
\paragraph{$s^2$}
\label{sec-3-3-3}

\begin{itemize}
\item here's where the work on quadratic forms comes into play.
\item $s^2 = (n-k-1)^{-1} \epsilon' (I - X(X'X)^{-1}X') \epsilon$
\begin{itemize}
\item $\epsilon/\sigma$ is standard normal
\item $I - X(X'X)^{-1}X'$ is a projection matrix
\begin{itemize}
\item trace = rank = $n - k - 1$ (show)
\end{itemize}
\item so $(n-k-1)/\sigma^2 \sim \chi^2_{n-k-1}$
\end{itemize}
\item what do we know about relationship with $\hat\beta$?
\begin{itemize}
\item $s^2$ is a function of (only) the residuals
\item you showed (for homework) that the residuals and the
          coefficient estimators are uncorrelated
\item both are normal so they're independent
\item so $s^2$ and $\hat\beta$ are also independent.
\end{itemize}
\end{itemize}
\paragraph{$t$-ratio}
\label{sec-3-3-4}

\begin{itemize}
\item we can put those results to use.
\item suppose we want to test the hypothesis $\beta_j = b$ vs. the
        alternative that $\beta_j \neq b$ (so a two-sided test).
\begin{itemize}
\item let $q_j$ be the $jj$ element of $(X'X)^{-1}$ just like before
\end{itemize}
\item construct the statistic
        \[ \frac{\hat\beta_j - b}{\sqrt{q_{jj} s^2}} \]
\begin{itemize}
\item simple algebra:
          \[ \frac{(\hat\beta_j - b)/\sqrt{q_i \sigma^2}}{\sqrt{s^2/\sigma^2}} \]
\begin{itemize}
\item we know that, conditional on $X$ and under the null, this is
            equal in distribution to
            \[ \frac{Z}{\chi^2_{n-k-1} / (n-k-1)} \]
\item so, conditional on $X$, this is $t$ with $(n-k-1)$ degrees
            of freedom.
\end{itemize}
\item does this distribution depend on $X$?
\begin{itemize}
\item no!
\item implies that it is the unconditional distribution as well.
\end{itemize}
\end{itemize}
\end{itemize}
\paragraph{vector extension of $t$-ratio}
\label{sec-3-3-5}

\begin{itemize}
\item remember, for a pd matrix $\Omega$, there exists another pd
        matrix $\Omega^{1/2}$ such that $\Omega^{1/2}(\Omega^{1/2})' =
        \Omega$
\item So, what is the distribution of $(X'X)^{1/2}(\hat\beta - \beta)$?
\begin{itemize}
\item we know $\hat\beta - \beta$ is asymptotically normal (given
          $X$) with mean zero and variance $\sigma^2 (X'X)^{-1}$
\item So, we know the variance of $(X'X)^{1/2}(\hat\beta - \beta)$
          is $\sigma^2 I_k$
\item so this r.v. is asymptotically normal with mean zero and
          variance $\sigma^2 I$.
\end{itemize}
\end{itemize}
\section{making predictions from regression models}
\label{sec-4}

\begin{itemize}
\item Reading is 5.6 in Greene
\item \textit{2011-11-01 Tue}
\end{itemize}
\subsection{motivation and setup}
\label{sec-4-1}

\begin{itemize}
\item Say we've estimated a regression model.  What do we do with it?
\item Easiest possible use: make forecasts
\item Setup:
\begin{itemize}
\item have observations $(y_i, x_i)$ for $i = 1,\dots,n$
\item observe regressors for period $n+1$
\item want to predict $y_{n+1}$
\end{itemize}
\item model
       \[ y_i  = x_i'\beta + \epsilon_i \]
\item Estimated by OLS (or, maybe, GLS)
\end{itemize}
\subsection{natural forecast}
\label{sec-4-2}

\begin{itemize}
\item The natural forecast for $y_{n+1}$ is $x_{n+1}'\hat\beta$
\begin{itemize}
\item $x_{n+1}'\beta$ would be the forecast that minimizes expected
         squared error
\item $\hat\beta$ is our best (minimum-variance) estimator of $\beta$
\item Expected value of $\hat y_{n+1}$ is $y_{n+1}$
\end{itemize}
\item How reliable is our forecast?
\begin{itemize}
\item more precisely, what is the variance of the forecast error?
\item Define the forecast error as 
         \[ e_{n+1} = y_{n+1} - \hat y_{n+1} = \epsilon_{n+1} -
         x_{n+1}'(\hat\beta - \beta) \]
\end{itemize}
\item Variance of forecast erro is going to reflect
\begin{itemize}
\item variance of $\hat\beta$
\item variance of $\epsilon_{n+1}$
\end{itemize}
\end{itemize}
\subsection{calculation of variance}
\label{sec-4-3}

     We're going to assume that the errors are uncorrelated (like we
     have) and homoskedastic.
\begin{itemize}
\item conditionally heteroeskedastic errors can be dealt with by GLS
\item The usual calculation gives us
       \[ var(e_{n+1} \mid X, x_{n+1}) = E(e_{n+1}^2 \mid X_{n+1}) =
       E(\epsilon_{n+1}^2 \mid X, x_{n+1}) + E(((\hat\beta -
       \beta)'x_{n+1})^2 \mid X, x_{n+1})\]
\item the first term is just $\sigma^2$
\item the second is 
       \[ x_{n+1}' var(\hat\beta \mid X) x_{n+1} = \sigma^2
       x_{n+1}'(X'X)^{-1}x_{n+1}\]
\item draw a scatterplot to illustrate why $\hat y$ will depend on
       the particular value of $x$.
\end{itemize}
\subsection{common use of this variance}
\label{sec-4-4}

\begin{itemize}
\item \textbf{if the errors are normal}, we can construct a confidence
       interval for $y_{n+1}$ from this result
\item $\hat y_{n+1} \pm t_{\alpha/2} \sqrt{s^2 (1 +
       x_{n+1}'(X'X)^{-1}x_{n+1})}$
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../pointestimation"
%%% End: 
