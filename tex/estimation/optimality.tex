% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Optimality properties of point estimation}%
\addcontentsline{toc}{part}{Optimality properties of point estimation}

\begin{itemize}
\item given that we have a ``goal'' for our estimator, how do we judge
     whether it measures the parameter it is supposed to well or not?
\item So, we derive the theoretical properties of the estimator.
\begin{itemize}
\item remember, an estimator is a function of data, $g(x₁,...,x_n)$
\item we look at the distribution of the \emph{random variable} $g(X₁,...,X_n)$
\item this distribution is called the \emph{sample distribution}
\item we usually care about the properties of $\θh -
       θ₀$ where $θ₀$ is the population parameter that
       we're trying to measure.
\end{itemize}
\item generally want
\begin{itemize}
\item correct location (more or less)
\item low dispersion (ie variance)
\end{itemize}
\end{itemize}

\section{Unbiasedness}

\begin{itemize}
\item definition: If $\θh$ is an estimator of $θ₀$,
       the bias of $\θh$ equals $E \θh - θ₀$.
       An estimator is \emph{unbiased} if its bias equals zero for any
       value of $θ₀$
\item motivates definition of efficient: If $\θh₁$ and $\hat
       θ₂$ are two unbiased estimators of a parameter $θ$,
       $\θh₁$ is more efficient than $\θh₂$ if the
       variance of $\θh₁$ is less than that of $\θh₂$.
\begin{itemize}
\item for multivariate $\θh$, we need $\var(\θh₂) -
         \var(\θh₁)$ to be positive definite.
\end{itemize}
\item An estimator $\θh$ of $θ$ is the best unbiased
       estimator if it is unbiased and has smaller variance than any
       other unbiased estimator
\item an interesting question is, what is the smallest variance that
       an unbiased estimator could have?
\begin{itemize}
\item if we have an unbiased estimator that has that variance, we
         know that it is ``the best'' estimator possible in a certain sense.
\end{itemize}
\end{itemize}

\section{Cramer-Rao Lower bound}

\begin{itemize}
\item Statement of result:
\begin{itemize}
\item Suppose $X₁,...,X_n$ have a joint density with likelihood
         $L(θ; X)$ and let $\θh$ be an estimator of
         $θ$ with finite variance and \[\tfrac{d}{dθ} ∫_\mathcal{X} \θh(x) L(θ; x) dx = ∫_\mathcal{X} \θh(x) \tfrac{d}{dθ} L(θ; x) dx\]
         where $\mathcal{X}$ is the support of $X₁,...,X_n$.
\item Then \[\var(\θh) ≥ \frac{(\tfrac{d}{dθ} \E \θh(X))²}{\E (\tfrac{d}{dθ} \log L(θ; X))²}\]
\item If $X_i ∼ i.i.d. f$, then this becomes
         \[\var(\θh) ≥ \frac{(\tfrac{d}{dθ} \E \θh(X))²}{n \E (\tfrac{d}{dθ} \log f(x; θ))²}\]
\end{itemize}
\item $X₁,...,X_n ∼$ i.i.d. $N(μ, σ²)$
\begin{itemize}
\item MLE is sample mean; mean $μ$ variance $σ²/n$
\item $d/dμ \log L(μ, σ²; X) = ∑_i (x_i - μ)/σ²$
\item So CR lower-bound is $\dfrac{1}{n \E (x_i-μ)²/σ^4} = σ²/n$
\item And sample mean achieves the CR lower bound, so it's the best
         unbiased estimator.
\end{itemize}
\item Proof: students should read the Casella-Berger proofs on their own.
\item Multivariate version (for unbiased estimators):
       \[\var(\θh) ≥ \Big[\E (\tfrac{d}{dθ} \log L(θ; X)) (\tfrac{d}{dθ} \log L(θ; X))'\Big]^{-1}\]
\item Discussion
\begin{itemize}
\item If any unbiased estimator attains this variance, it is MVUE and
         we should use it.
\item The inequality may be strict (bound might not be obtained by any estimator or by the best estimator)
\end{itemize}
\end{itemize}

\section{loss function based optimality:}

\begin{itemize}
\item Suppose that we have some measure of dis-utility: if our
       estimator is far from the true value, it causes us some
       (hypothetical) pain.
\item how should we think about this?
\begin{itemize}
\item take a convex function $L$.
\item The pain we feel from mis-estimating $θ₀$ is $L(\θh - θ₀)$
\item we can compare the \emph{risk} of two estimators, and choose the
         estimator with the smallest risk
\begin{itemize}
\item risk is defined as expected loss
\item if $\E L(\θh₁ - θ₀)$ is smaller than $\E L(\θh₂ - θ₀)$, we're
  going to be better off on average by using estimator 1
\item can be motivated by utility theory and expected utility;
           then the best estimator is linked to certainty equivalence.
           (see \emph{The Handbook of Economic Forecasting} (2006) chapter
           by Machina and Granger
\end{itemize}
\end{itemize}
\item one loss function that is very popular: $L(x) = x²$
\begin{itemize}
\item gives MSE criterion.
\end{itemize}
\item another: L(x) = abs(x)
\item usually loss functions are used to compare estimators only when
       you have a specific situation where you know that certain types
       of errors are more costly than others.
\end{itemize}

\subsection{another problem:}

\begin{itemize}
\item when  you're explicitly worried about the loss, imposing
        unbiasedness doesn't make sense
\item the same problem we ran into when we didn't impose it and just
        looked at variance shows up
\begin{itemize}
\item $p₁ = \Xb$
\item $p₂ = 1/2$
\item $mse(p₁) = bias(p₁)² + variance(p₁) = p(1-p)/n$
\item $mse(p₂) = (1/2 - p)²$
\end{itemize}
\item sometimes $p₁$ will have smaller mse; sometimes $p₂$ will.
\item so, not really a good general way to construct estimators, but
        you'll often see estimators that are otherwise ``reasonable''
        compared by MSE too
\item if you have a good idea about what $p$ is going to be, MSE can
        be more useful
\begin{itemize}
\item seen in Bayesian statistics.
\end{itemize}
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../estimation"
%%% End: 
