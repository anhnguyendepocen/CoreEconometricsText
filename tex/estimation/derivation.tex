% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Deriving point estimators}%
\addcontentsline{toc}{part}{Overview of point estimation}

I'd like to add a form of loss-function minimization after the Method
of Moments section.

\section{Method of Moments}
\begin{itemize}
\item Suppose we have $X₁,…,X_n ∼ f(x; θ)$
\begin{itemize}
\item $f$ is known
\item $θ$ is an unknown $p$-vector that we want to estimate
\end{itemize}
\item estimator is based on a simple idea:
  \begin{itemize}
  \item if we want to estimate an expectation, use a sample average
  \item $\Pr[X_i ≤ c]$ for known $c$
    \begin{itemize}
    \item $=\E \1\{X_i ≤ x\}$
    \item estimate with $n^{-1} ∑_{i=1}^n 1\{X_i ≤ c\}$
    \end{itemize}
  \item Estimate $\E X_i$ with $n^{-1} ∑_{i=1}^n X_i$
  \item etc
  \end{itemize}
\item in general, relate $θ$ to the population moments:
  \begin{itemize}
  \item if $θ$ has $p$ elements, we calculate the first $p$ moments of $X_i$
    \begin{align*}
    μ₁  &= g₁(θ), &
    μ₂  &= g₂(θ), &
    & \dots &
    μ_p &= g_p(θ),
    \end{align*}
  \end{itemize}
\item calculate the sample moments:
  \begin{align*}
    \μh₁ &= \ov{n} ∑_{i=1}^n X_i, &
    \μh₂ &= \ov{n} ∑_{i=1}^n X_i², &
    & \dots &
    \μh_p &= \ov{n} ∑_{i=1}^n X_i^p,
  \end{align*}
\item estimate $θ$ by setting the equations equal:
  \begin{equation*}
    (g₁(\θh),…, g_p(\θh)) =  (\μh₁,…, \μh_p) = 
    \Big( \ov{n} ∑_{i=1} X_i,…,\ov{n} ∑_{i=1} X_i^p \Big).
  \end{equation*}
  And then \[\θh = g^{-1} (n^{-1} ∑_{i=1}^n X_i,…,n^{-1} ∑ X_i^p).\]
  Obviously, $g$ needs to be invertible for this to work.
\item Sometimes $\θh$ is easy to write out analytically as a function
  of the sample averages
  \begin{itemize}
  \item when it's not, you can find $\θh$ numerically.
  \end{itemize}
\item if each sample moment is close to the population moment and
  $g^{-1}$ is continuous, then $\θh$ should be close to $θ$.
\end{itemize}

\section{Examples}

\begin{itemize}
\item normal$(μ,σ²)$
  \begin{itemize}
  \item first moment of a normal random variable is $\E X_i = μ$
  \item second moment is $\E X_i² = σ² + μ²$
  \item method of moments estimator is
    \begin{itemize}
    \item $\μh = \ov{n} ∑_i X_i$
    \item $\σh² = \ov{n} ∑_i X_i² - (n^{-1}∑_i X_i)²
      = \ov{n} ∑_i(X_i - \Xb)²$
    \item which is not the usual estimator, but is close.
    \end{itemize}
  \end{itemize}
\item uniform($a$,$b$)
  \begin{itemize}
  \item Let's say that $X₁,…,X_n$ are iid uniform$(a,b)$, and we want
    to calculate the method of moments estimator for $a$ and $b$.
    \begin{itemize}
    \item density of $X_i$ is $1/(b-a)$ in $[a,b]$, zero otherwise.
    \end{itemize}
  \item Calculate the first two moments:
    \begin{itemize}
    \item $\E X_i = ∫_a^b x /(b-a) dx = \frac{b+a}{2}$
    \item $\E X_i² = ∫_a^b x² /(b-a) dx = \frac{b^3 - a^3}{3(b - a)}
      = \frac{b + ab + a²}{3}$
    \end{itemize}
  \item Gives the estimators:
    \begin{itemize}
    \item $\hat b+\hat a = 2 n^{-1} ∑_i X_i$
    \item $\frac{\hat b³-\hat a³}{\hat b- \hat a} = 3 n^{-1} ∑_i X_i²$
    \item solve for $\hat b$ and $\hat a$, gives
      \begin{itemize}
      \item $\hat b = \Xb + s \sqrt{3}$
      \item $\hat a = \Xb - s \sqrt{3}$
      \item $s = \sqrt{n^{-1} ∑_i (X_i - \Xb)²}$
      \end{itemize}
    \end{itemize}
  \end{itemize}
\item linear regression
  \begin{itemize}
  \item setup
    \begin{itemize}
    \item $(Y_i, X_i) ∼$ i.i.d.
    \item $X_i ∼ f$ (unspecified)
    \item $Y_i ∣ X_i ∼ N(β₀ + β₁ X_i, σ²)$
    \item want to estimate $β₀$ and $β₁$
    \item draw scatterplot (this is like estimating the slope and
      intercept)
    \item We'll often see this as $Y_i = β₀ + β₁ X_i + e_i$
      \begin{itemize}
      \item $e_i ∣ X_i ∼ N(0, σ²)$
      \item i.e. define $e_i = Y_i - β₀ - β₁ X_i$
      \end{itemize}
    \end{itemize}
  \item MoM estimator
    \begin{itemize}
    \item we know $\E \binom{Y_i}{X_i Y_i} =
      \begin{pmatrix} β₀ + β₁ \E X_i \\ β₀ \E X_i + β₁ \E X_i² \end{pmatrix}$
      \begin{itemize}
      \item $= \begin{pmatrix}
          1 & \E X_i \\ 
          \E X_i & \E X_i² \end{pmatrix} \begin{pmatrix}β₀ \\
          β₁
        \end{pmatrix}$
      \end{itemize}
    \item Assuming invertibility, $\begin{pmatrix}β₀\\β₁\end{pmatrix}
      = \begin{pmatrix}1& \E X_i \\ \E X_i & \E X_i²\end{pmatrix}^{-1}
      \begin{pmatrix} \E Y_i \\ \E X_iY_i\end{pmatrix}$
    \item This makes our estimator,
      $\begin{pmatrix}
        \βh₀ \\ \βh₁
      \end{pmatrix}
      =
      \begin{pmatrix} 
        1 & ∑_i x_i/n \\ ∑_i x_i/n & ∑_ix_i²/n 
      \end{pmatrix}^{-1}
      \begin{pmatrix}
        ∑_iy_i/n \\ ∑_ix_iy_i / n
      \end{pmatrix}$
      \begin{itemize}
      \item $=
        \begin{pmatrix}
          1 & ∑_i x_i \\ ∑_i x_i & ∑_i x_i²
        \end{pmatrix}^{-1}
        \begin{pmatrix}
          ∑_i y_i \\ ∑_i x_i y_i
        \end{pmatrix}$
      \item (write in matrix notation and explain)
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{Discussion}

\begin{itemize}
\item advantages
  \begin{itemize}
  \item it gets you an estimator
  \item easy to derive asymptotic properties of the estimator (it's a
    function of averages, which usually obey CLTs and LLNs).
  \end{itemize}
\item disadvantages
  \begin{itemize}
  \item can be very inefficient
  \item for it to be useful, requires that the moments tell you a lot
    about the distribution (for the uniform, they don't
    necessarily—we're trying to estimate the endpoints, but our
    estimator is to look for the center and double it.)
    \begin{itemize}
    \item a big problem with our estimates here, is we might have $X$s
      in our data that are outside the interval $[a,b]$
    \end{itemize}
  \item kind of ad hoc.
  \item for the uniform distribution, we have \emph{all} of the
    moments defined… should we calculate the first $p$ moments and
    average all of them?
    \begin{itemize}
    \item how do we pick $p$?
    \item are some moments better than others?
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{GMM}

\begin{itemize}
\item Hansen (1982) shows that a modification of the method of moments
  can be very useful in macro:
  \begin{itemize}
  \item have periods $t=1,…,T$
  \item macro models tell you that $\E_t g(X_t, θ) = 0$ where $g(x_t,
    θ)$ is coming from Euler equations (ie, an agent is optimizing in
    period $t$, and $g$ captures the difference between what they
    expect to happen in period $t+1$ and what actually happens in
    period $t+1$ -- under rationality, they choose an action that
    makes that difference unpredictable.
  \item LIE tells you that $\E g(X_t, θ) = \E \E_t g(X_t,θ)$ which
    equals zero, so you have the condition
    \[\E g(X_t, θ) = 0\]
  \item Hansen shows that you can often estimate $θ$ by solving
    \[ T^{-1} ∑_{t=1}^T g(X_t,θ) = 0 \]
    for $θ$. (and says how)
  \item when you have more moments than parameters, gives a weighting
    scheme.
  \end{itemize}
\end{itemize}

\section{Loss-function minimization}

Add!

\section{Introduction to Maximum Likelihood Estimation}

\begin{itemize}
\item look at the uniform$(0,b)$ example suppose $n = 1$
  \begin{itemize}
  \item draw these densities for $b$ = 1, 2, 4
    \begin{itemize}
    \item density is $1/b$ in $[0,b]$.
    \item pick a point $x$ on the density
    \end{itemize}
  \item have (as a function of $b$) $1/b$ as long as $b ≥ x$
  \item if we observe $X = 0.5$ (for example), we know that $b ≥ 0.5$
  \item In addition, values like $b = 500$ are implausible
    \begin{itemize}
    \item as $b$ gets larger, there are more possible values that $X$ is
      likely to take on.
    \end{itemize}
  \item in a sense, $b = 0.5$ is the most plausible
    \begin{itemize}
    \item smaller values are impossible
    \item as $b$ increases from $0.5$, the plausiblity decreases.
    \end{itemize}
  \item we can get $b = 0.5$ directly by maximizing $f(0.5; b)$ as a
    function of $b$
    \begin{itemize}
    \item called the likelihood and written $L(b; x)$
    \end{itemize}
  \end{itemize}
\end{itemize}

\section{Definition}

\begin{itemize}
\item for the MLE estimator, we start with the density, but view it as
  a function of $θ$
  \begin{itemize}
  \item the value of $θ$ that maximizes the likelihood is (in a sense)
    the most plausible/defensible value.
  \end{itemize}
\item the MLE of $θ$ is $\argmax_θ L(θ; x₁,…,x_n)$
\end{itemize}

\section{Examples}

\subsection{iid draws from uniform$(a,b)$}
\begin{itemize}
\item $L(a,b; x₁,…,x_n) = ∏_i 1\{x_i ∈ [a,b]\} (b-a)^{-1}$
\item $= ({1 \over b-a})^{n}$ if all $x_i ∈ [a,b]$ and zero otherwise.
\item we can find the maximum easily
  \begin{itemize}
  \item likelihood decreases as $b$ increases or $a$ decreases
  \item likelihood becomes zero if $b < \max x_i$ or if $a > \min x_i$
  \item so $\hat b = \max x_i$ and $\hat a = \min x_i$
  \end{itemize}
\end{itemize}

\subsection{linear regression.}
\begin{itemize}
\item $(Y_i,X_i) ∼ iid$
  \begin{itemize}
  \item $X_i ∼ f$ which is unspecified, $X_i$ is a $k × 1$ vector.
  \item $Y_i ∣ X_i ∼ N(X_i'β, σ²)$
  \end{itemize}
\item want to estimate $β$ and $σ²$
\item Draw fitted values, densities
\item $L(μ,σ²; y, x) = ∏_i {1 \over \sqrt{2 π σ²}}
  e^{- {(y_i - x_i'β)² \over 2 σ²}} f(x_i)$
\item step 1: take logs:
  \begin{itemize}
  \item $\log L(μ,σ²; x₁,…,x_n) = const - n\log (\sqrt{σ²}) -
    ∑_i {(x_i - μ)² \over 2 σ²} + ∑_i f(x_i)$
  \end{itemize}
\item First order conditions:
  \begin{itemize}
  \item for mean:
    \begin{itemize}
    \item $\frac{\partial}{\partial β} \log L(μ, σ²; x, y) = ∑_{i=1}^n x_i (y_i - x_i'β) = 0$
    \item so $\βh=(∑_i x_i x_i')^{-1} ∑_i x_i y_i$
    \end{itemize}
  \item for variance:
    \begin{itemize}
    \item $\frac{\partial}{\partial σ²} \log L(μ, σ²; x, y) = -\frac{n}{2σ²} + \frac{1}{2 σ^4}∑_i (y_i - x_i'β)² = 0$
    \item so $\hat σ² = \frac{1}{n} ∑_{i=1}^n (y_i - x_i'β)²$
    \end{itemize}
  \end{itemize}
\item students should verify that this is a maximum on their own.
\end{itemize}

\section{More remarks on mle}

\begin{itemize}
\item unlike method of moments, where we connect our parameters to the
  mean, variance, etc. regardless of the distribution; here we look at
  the features of the data that the distribution tells us are the most
  relevant.
  \begin{itemize}
  \item for normal, this \emph{is} the mean and variance, so MLE and
    MoM give us the same statistics
  \item for uniform, and others, this is \emph{not} the mean and
    variance.  - the derivative of the log likelihood is called the
    \emph{score}.  
    $S(θ; x) = {\partial \over \partial θ} \log L(θ; x)$
  \end{itemize}
\item has a nice invariance property: say you're not interested in the
  parameters per se, but care about a transformation of the parameters
  $T(θ)$.  If $\θh$ is the maximum likelihood estimator of $θ$,
  then $T(\θh)$ is the MLE of $T(θ)$.
\item we'll see later that you can use MLE to get an estimator, even
  if you don't believe that the distribution is true
  \begin{itemize}
  \item called quasi-maximum likelihood
  \item obviously, you then need to check that the estimator works well.
  \end{itemize}
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../estimation"
%%% End: 
