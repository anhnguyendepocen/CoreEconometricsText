% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Estimation of linear regression}%
\addcontentsline{toc}{part}{Estimation of linear regression}

\section{Lin. regression motivation}
\subsection{more formal discussion}

\paragraph{how to think about $\ep$}
\begin{itemize}
\item have a variable you want to understand
\begin{itemize}
\item $Y_i$
\end{itemize}
\item have a model and variables you think explain it
\begin{itemize}
\item $g(X_{i,0},...,X_{i,k})$
\item in our case, $g(·) = X_{i,0}β₀ + ... + X_{i,k}β_k$
\end{itemize}
\item error is the difference between the variable and the model
\begin{itemize}
\item $\ep_i = Y_i - g(X_{i,0},...,X_{i,k})$
\end{itemize}
\item why is this a good way to think about it?
\begin{itemize}
\item any model you write down is going to give you an error
\item the model \underline{doesn't have to be true}
\item different models will give you different errors
\item so, when you think about properties and assumptions about the
          error term, we're talking about the difference between the
          thing we're modeling (Y) and the model we've written down.
\begin{itemize}
\item we're \textbf{not} talking about ``the truth''
\end{itemize}
\end{itemize}
\item so you can think of $\ep(g)$ where $g$ is the function
\begin{itemize}
\item then $Y_i = g(X_{i,0},...,X_{i,k}) + \ep_i(g)$
          \underline{always} holds.
\item for any given $g$, we can calculate the errors.
\item ``regression'' means using a function of the $\ep_i(g)$-s
          to estimate $g$ subject to some sort of constraint
\begin{itemize}
\item ie (what you're used to)
  \[\hat g = \argmin_g n^{-1} ∑_{i=1}ⁿ \ep_i(g)²\]
  subject to the condition that
  \[g(X_{i,0}, ..., X_{i,k}) = β₀ X_{i,0}+ ⋯ + β_k X_{i,k}\]
  gives the OLS estimator.
\end{itemize}
\end{itemize}
\end{itemize}

\paragraph{usual assumptions on $\ep$}
\begin{enumerate}
\item $\E(\ep_i ∣ X) = 0$ for all $i$
\item $\var(\ep_i ∣ X) = σ²$
\item $\cov(\ep_i, \ep_j ∣ X) = 0$ for $i ≠ j$
\item $\ep_i ∼ iid(0,σ²)$ given $X$
\item $\ep_i ∼ iid\; N(0,σ²)$ given $X$
\end{enumerate}
      note that 5 implies 4 which implies 1, 2, and 3.

\paragraph{how to view this in light of the previous discussion:}
There exists a particular parameter value $β$ such that
\begin{enumerate}
\item $\E(Y_i - X_i'β ∣ X) = 0$ for all $i$
\item $\var(Y_i - X_i'β ∣ X) = σ²$ for all $i$
\item etc.
\end{enumerate}
And we define $\ep = Y - Xβ$ for that particular value of $β$.

\subsection{Derivation of OLS as MLE}


for now, we'll assume that $\ep ∼ N(0,σ² I)$

\paragraph{Expected value lies in the plane generated by $X$}
The expected value of $Y_i$ is linear in $β$:
\[ \E(Y ∣ X) = Xβ \]

\paragraph{do all of this for $n=2$ and then for $n = 3$}
\begin{itemize}
\item draw plane for $n = 3$ and $k = 1$
\item draw spheres for the different epsilons (sphere here is a
         consequence of normality and independence)
\begin{itemize}
\item each layer of the sphere is an iso-probability line
\end{itemize}
\end{itemize}

\paragraph{estimation}
\begin{itemize}
\item if that's the way $Y$ is generated, how would we think about
        estimating $β$?
\item draw a new picture:
\begin{itemize}
\item start with $Y$ and $X$ given
\item choose some $β = B₁$
\item draw different spheres for the ``epsilon'' corresponding to that mean
\item density (likelihood) evaluated for that $β$ has some value, ie
\begin{itemize}
\item $f(Y ∣ X; β = B₁) = c₁$
\end{itemize}
\end{itemize}
\item draw a second picutre corresponding to $β = B₂$
\item we can judge which of the values of $β$ is more plausible by
        looking at the density function, and choosing the one that's larger.
\item this is just MLE.
\item informally, it's clear that the coefficients that maximize the
        likelihood are going to be the coefficients that minimize the
        distance between $Y$ and $\Yh$
\end{itemize}

\paragraph{a few ways to motivate the estimator}
\begin{enumerate}
\item We know that $P_X = X(X'X)^{-1}X'$
\begin{itemize}
\item is a projection matrix
\item ie if $Z$ is any $n×1$ vector, $P_X Z$ is an element of
  $\RR^n$ such that
\begin{itemize}
\item $P_X Z = X α$ for some $α$
\item $| Z - P_X Z |$ is minimized
\end{itemize}
\end{itemize}
\item so, we can jump ahead and say:
\begin{itemize}
\item $\Yh = P_X Y = X \βh$
\item $\βh = (X'X)^{-1} X'Y$
\end{itemize}
\item We can also solve the minimization problem directly:
\begin{itemize}
\item Define $\βh = \argmin_β \left(∑_{i=1}ⁿ (y_i - x_i'β)² \right)^{1/2}$
\item know that $\βh$ solves the first order conditions:
  \[ 0 = ∑_{i=1}ⁿ {∂ \over ∂ β_j} (y_i - x_i'β)²
  = 2 ∑_{i=1}ⁿ (y_i - x_i'β) x_{ij} \]
\item written out in matrix form: $0 = X'(Y - Xβ)$ which becomes $X'Xβ
  = X'Y$ or $β = (X'X)^{-1}X'Y$
\begin{itemize}
\item which requires that $X'X$ be invertible and that $X$ has
              full rank.
\end{itemize}
\end{itemize}
\item explain that $\Yh$ is well defined without full rank, but $\βh$
  is not
\begin{itemize}
\item draw picture again, but with three regressors
\item third regressor lies in the same plane as the first two.
\end{itemize}
\item Finally, we can actually solve the maximization problem.
\begin{itemize}
\item under normality, conditional log-likelihood of $β$ given $X$ is
  \[ \log L(β; Y ∣ X) = const + n \log σ - {1 \over 2 σ²} ∑_{i = 1}ⁿ
  (y_i - x_i'β)² \] and maximizing this with respect to $β$ is
  \textbf{exactly} the same problem as minimizing the
  SSR.\sidenote{under these same assumptions, calculate the MLE of
    $σ²$ and derive its bias for homework.}
\end{itemize}
\end{enumerate}

\paragraph{definitions}
\begin{itemize}
\item $\βh = (X'X)^{-1}X'Y = \left(∑_i x_i x_i'\right) ∑_i x_i y_i$ --
  OLS Estimator
\item $\Yh = X \βh$ -- Fitted Values
\item $\εh = Y - \Yh$ -- OLS Residuals
\end{itemize}

\section{statistical properties}
\subsection{small sample properties of the estimator}

\paragraph{unbiasedness}
\begin{itemize}
\item Suppose that $Y = Xβ + \ep$
\begin{itemize}
\item $\E(\ep ∣ X) = 0$
\item $X$ has full rank
\end{itemize}
\item then $\E \βh = β$.
\end{itemize}

\paragraph{proof}
\begin{itemize}
\item we're going to use the law of iterated expectations:
  \begin{align*}
\E(\βh ∣ X) 
&= \E((X'X)^{-1} X'Y ∣ X) \\
&= (X'X)^{-1} X'\E(Y ∣ X) \\
&= (X'X)^{-1} X'\E(Xβ + e ∣ X) \\
&= (X'X)^{-1} X'X β \\
&= β
  \end{align*}
\item note that this is a slightly stronger result: the expected value
         of the OLS coefficient estimator is $β$ \emph{regardless} of the
         value of $X$
\end{itemize}

\paragraph{variance of $\βh$}
\begin{itemize}
\item Suppose that $Y = Xβ + \ep$
\begin{itemize}
\item $\E(\ep ∣ X) = 0$
\item $\E(\ep \ep' ∣ X) = σ² I$
\item $X$ has full rank
\end{itemize}
\item then $\var(\βh ∣ X) = σ² (X'X)^{-1}$
\end{itemize}

\paragraph{proof}
note that $\βh - β = (X'X)^{-1}X'\ep$, so
\begin{align*}
  \var(\βh ∣ X) &= \E((\βh - β)(\βh-β)' ∣ X) \\
  &= \E((X'X)^{-1}X'\ep\ep'X(X'X)^{-1} ∣ X) \\
  &= (X'X)^{-1} X' \E(\ep\ep' ∣ X) X (X'X)^{-1} \\
  &= σ² (X'X)^{-1} X'X (X'X)^{-1} \\
  &= σ² (X'X)^{-1}
\end{align*}

\paragraph{remarks}
\begin{itemize}
\item depends on values of the regressors
\item unconditional variance: $\var \βh = σ² \E (X'X)^{-1}$
\end{itemize}


\paragraph{\textbf{homework} Prove that $\βh$ and $\eph$ are uncorrelated.}

\paragraph{how does $\βh$ compare to other estimators of $β$?}
\begin{itemize}
\item what other estimators might we want to compare $β$ to?
\item linear estimators first: OLS can be viewed as a weighted average:
        $\βh = (X'X) X'Y = ∑_{i=1}ⁿ w_i y_i$ where $w_i$ is a
        $k$-vector, $w_i = (X'X)^{-1} x_i$
\item how does OLS compare to all other unbiased weighted averages:
        $∑_i v_i y_i$ where $v_i$ is a function of $X$?
\end{itemize}

\paragraph{Gauss Markov Theorem}
\begin{itemize}
\item Assume that $Y = Xβ + \ep$ where $\ep ∼ (0, σ² I)$ given $X$.  Then $\βh$ is the unique
         estimator with minimum variance (given $X$) among all linear,
         unbiased estimators (OLS is BLUE).
\end{itemize}

\paragraph{proof}
\begin{itemize}
\item First, what does ``minimum variance'' mean?  Just like it did
          earlier in the semester
\begin{itemize}
\item means that $\var(V'Y ∣ X) - \var(\βh ∣ X)$ is
            positive definite.
\end{itemize}
\end{itemize}

        Suppose that $V$ is $k × n$ matrix s.t. $V'Y$ is unbiased
        for $β$.
\begin{itemize}
\item here's how the proof will work:
\begin{itemize}
\item we can write down immediately that $V'Y = (X'X)^{-1}X'Y + [V'Y -
  (X'X)^{-1} X'Y]$
\item We're going to show that these two parts are uncorrelated.
\item That way, $\var(V'Y ∣ X) = \var(\βh ∣ X) + \var(V'Y - (X'X)^{-1}X'Y
  ∣ Y)$ and so we get the result automatically.
\end{itemize}
\item basic setup
  \begin{itemize}
  \item We know that $V'Y = V'Xβ + V'e$
  \item Then $β = \E(V'Y ∣ X) = \E(V'Xβ + V'e ∣ X) = V'X β + V'\E(e ∣ X)
    = V'Xβ$
  \item This holds for any choice of $β$, so $V'X = I$
  \item as a result, $V'Y - β = V'e$
  \end{itemize}
\item For the main step, we just calculate the covariance between
  $(X'X)^{-1}X'Y$ and $(V - (X'X)^{-1}X')Y$,
  \begin{align*}
    \cov((X'X)^{-1}X'Y, (V - (X'X)^{-1}X')Y ∣ X)
    &= \E((X'X)^{-1}X'e e'(V - X(X'X)^{-1}) ∣ X) \\
    &= σ² (X'X)^{-1}X'(V - X(X'X)^{-1}) \\
    &= σ² [(X'X)^{-1}X'V - (X'X)^{-1} X'X (X'X)^{-1}].
  \end{align*}
  Both equal $(X'X)^{-1}$, so the whole term is zero and we've shown
  that the covariance is zero.
\end{itemize}


\paragraph{discussion}
\begin{itemize}
\item this illustrates a \textbf{key} strategy
\begin{itemize}
\item took an arbitrary linear unbiased estimator for $β$
\item showed that it could be broken down into two pieces
\begin{itemize}
\item the ols estimator $\βh$
\item some orthogonal noise component
\end{itemize}
\item so these estimators are just the efficient estimator plus
            some noise.
\end{itemize}
\item proved result \textbf{conditional on X}, unconditional extension is easy:
\begin{itemize}
\item If $\var(\βh ∣ X) ≤ \var(V'Y ∣ X)$ for all values of $X$, then
  $\E \var(\βh ∣ X) ≤ \E \var(V'Y ∣ X)$ as well
\item $\var(\βh) = \E(\var(\βh ∣ X)) + \var(\E(\βh ∣ X))$
\begin{itemize}
\item second term equals zero
\end{itemize}
\item $\var(V'Y) = \E(\var(V'Y ∣ X)) + \var(\E(V'Y ∣ X))$
\begin{itemize}
\item second term also equals zero
\end{itemize}
\end{itemize}
\item under normality, can prove an even better result: the ols
          estimator is the best unbiased estimator (even allowing for
          nonlinear estimators)\sidenote{You should do this for a 
            single regressor for homework.}
\end{itemize}


\subsection{finite-sample distribution under normality of the errors.}

\begin{itemize}
\item We've discussed unbiasedness and variance
\begin{itemize}
\item neither depend on the distribution of the errors
\end{itemize}
\item but we usually care about the distribution of our estimators too
\item working out unconditional distribution will be tricky
\begin{itemize}
\item remember variance depends on $X$
\item so distribution will depend on the distribution of $X$
\end{itemize}
\item rembember $\βh = (X'X)^{-1} X'Y$
\item new estimator $s² = (n-k-1)^{-1} ∑_{i=1}ⁿ \εh²_i$
\begin{itemize}
\item can also be written as
  \begin{align*}
    s² &= (n-k-1)^{-1} ∑_i (y_i - x_i'\βh)² \\
    &= (n-k-1)^{-1} (Y - X\βh)'(Y - X\βh) \\
    &= (n-k-1)^{-1} (Y - X(X'X)^{-1}X'Y)'(Y - X(X'X)^{-1}X'Y) \\
    &= (n-k-1)^{-1} Y'(I - X(X'X)^{-1}X')'(I - X(X'X)^{-1}X')Y \\
    &= (n-k-1)^{-1} Y'(I - X(X'X)^{-1}X')'Y \\
    &= (n-k-1)^{-1} (Xβ + ε)'(I - X(X'X)^{-1}X')'(Xβ + ε) \\
    &= (n-k-1)^{-1} ε'(I - X(X'X)^{-1}X')'ε
  \end{align*}
\end{itemize}
\item from hw, you know that $\E s² = σ²$.
\item We'll work out the distribution of $\βh$ and $s²$
       conditional on the regressors under the assumption that the
       errors are normal
\begin{itemize}
\item corresponds to MLE
\item will give intuition that continues to hold without normality.
\end{itemize}
\end{itemize}

\paragraph{$\βh$}
\begin{itemize}
\item Assume $Y = Xβ + ε$ with $ε ∼ N(0,σ² I)$.
\item $\βh - β = (X'X)^{-1} X'ε$
\item conditional on $X$, we have a constant matrix ($(X'X)^{-1}X'$)
        times a normal vector.
\item So $\βh$ is $N(β, σ² (X'X)^{-1})$ given $X$.
\item to get the unconditional distribution, would need to know the
        density of $X$, then integrate over the joint density, which we
        won't do.
\item a useful unconditional result:
\begin{itemize}
\item let $q_i$ be the $i,i$ element of $(X'X)^{-1}$.
\item we know that $\βh_i ∼ N(β_i, σ² q_i)$ given $X$.
\item so we can look at the unconditional density of
          $(\βh_i - β_i)/\sqrt{q_i σ²}$:
  \begin{align*}
    f_{(\βh_i - β_i)/\sqrt{q_i σ²}}(b)
    &= ∫ f_{(\βh_i - β_i)/\sqrt{q_i σ²}, q_i} (b, q) dq \\
    &= ∫ f_{(\βh_i - β_i)/\sqrt{q_i σ²}}(b ∣ q_i = q) f_{q_i}(q) dq \\
    &= ∫ φ(b) f_{q_i}(q) dq \\
    &= φ(b) ∫ f_{q_i}(q) dq \\
    &= φ(b)
  \end{align*}
\item points
\begin{itemize}
\item once we divide by the (random) variance, we get an
            unconditional result
\item seems kind of obvious, but is worth emphasizing.
\end{itemize}
\end{itemize}
\end{itemize}

\paragraph{residuals and fitted values}
\begin{itemize}
\item Defined the residuals as a function of $\βh$, so if we
        know the distribution of the coefficient estimators, we know the
        distribution of the residuals
\begin{itemize}
\item again, need to condition on $X$
\end{itemize}
\item directly:
\begin{itemize}
\item $\εh = (I - X(X'X)^{-1}X') ε$
\item Given $X$, this is $N(0, I - X(X'X)^{-1}X')$
\end{itemize}
\item fitted values
\begin{itemize}
\item $\Yh ∼ N(Xβ, X(X'X)^{-1}X')$
\end{itemize}
\end{itemize}
\paragraph{$s²$}

\begin{itemize}
\item here's where the work on quadratic forms comes into play.
\item $s² = (n-k-1)^{-1} ε' (I - X(X'X)^{-1}X') ε$
\begin{itemize}
\item $ε/σ$ is standard normal
\item $I - X(X'X)^{-1}X'$ is a projection matrix
\begin{itemize}
\item trace = rank = $n - k - 1$ (show)
\end{itemize}
\item so $(n-k-1) s² /σ² ∼ χ²_{n-k-1}$
\end{itemize}
\item what do we know about relationship with $\βh$?
\begin{itemize}
\item $s²$ is a function of (only) the residuals
\item you showed (for homework) that the residuals and the
          coefficient estimators are uncorrelated
\item both are normal so they're independent
\item so $s²$ and $\βh$ are also independent.
\end{itemize}
\end{itemize}

\paragraph{$t$-ratio}
\begin{itemize}
\item we can put those results to use.
\item suppose we want to test the hypothesis $β_j = b$ vs. the
  alternative that $β_j ≠ b$ (so a two-sided test).
\begin{itemize}
\item let $q_j$ be the $jj$ element of $(X'X)^{-1}$ just like before
\end{itemize}
\item construct the statistic
  \[ \frac{\βh_j - b}{\sqrt{q_{jj} s²}} \]
\begin{itemize}
\item simple algebra:
  \[ \frac{(\βh_j - b)/\sqrt{q_i σ²}}{\sqrt{s²/σ²}} \]
\begin{itemize}
\item we know that, conditional on $X$ and under the null, this is
            equal in distribution to
            \[ \frac{Z}{χ²_{n-k-1} / (n-k-1)} \]
\item so, conditional on $X$, this is $t$ with $(n-k-1)$ degrees
            of freedom.
\end{itemize}
\item does this distribution depend on $X$?
\begin{itemize}
\item no!
\item implies that it is the unconditional distribution as well.
\end{itemize}
\end{itemize}
\end{itemize}

\paragraph{vector extension of $t$-ratio}
\begin{itemize}
\item remember, for a pd matrix $Ω$, there exists another pd matrix
  $Ω^{1/2}$ such that $Ω^{1/2}(Ω^{1/2})' = Ω$
\item So, what is the distribution of $(X'X)^{1/2}(\βh - β)$?
\begin{itemize}
\item we know $\βh - β$ is asymptotically normal (given $X$) with mean
  zero and variance $σ² (X'X)^{-1}$
\item So, we know the variance of $(X'X)^{1/2}(\βh - β)$ is $σ² I_k$
\item so this r.v. is asymptotically normal with mean zero and
  variance $σ² I$.
\end{itemize}
\end{itemize}

\section{making predictions from regression models}

\subsection{motivation and setup}
\begin{itemize}
\item Say we've estimated a regression model.  What do we do with it?
\item Easiest possible use: make forecasts
\item Setup:
\begin{itemize}
\item have observations $(y_i, x_i)$ for $i = 1,...,n$
\item observe regressors for period $n+1$
\item want to predict $y_{n+1}$
\end{itemize}
\item model \[ y_i = x_i'β + ε_i \]
\item Estimated by OLS (or, maybe, GLS)
\end{itemize}

\subsection{natural forecast}

\begin{itemize}
\item The natural forecast for $y_{n+1}$ is $x_{n+1}'\βh$
\begin{itemize}
\item $x_{n+1}'β$ would be the forecast that minimizes expected
         squared error
\item $\βh$ is our best (minimum-variance) estimator of $β$
\item Expected value of $\hat y_{n+1}$ is $y_{n+1}$
\end{itemize}
\item How reliable is our forecast?
\begin{itemize}
\item more precisely, what is the variance of the forecast error?
\item Define the forecast error as 
  \[ e_{n+1} = y_{n+1} - \hat y_{n+1} = ε_{n+1} - x_{n+1}'(\βh - β) \]
\end{itemize}
\item Variance of forecast erro is going to reflect
\begin{itemize}
\item variance of $\βh$
\item variance of $ε_{n+1}$
\end{itemize}
\end{itemize}

\subsection{calculation of variance}

     We're going to assume that the errors are uncorrelated (like we
     have) and homoskedastic.
\begin{itemize}
\item conditionally heteroeskedastic errors can be dealt with by GLS
\item The usual calculation gives us
  \[ \var(e_{n+1} ∣ X, x_{n+1}) = \E(e_{n+1}² ∣ X_{n+1}) = \E(ε_{n+1}²
  ∣ X, x_{n+1}) + \E(((\βh - β)'x_{n+1})² ∣ X, x_{n+1})\]
\item the first term is just $σ²$
\item the second is 
  \[ x_{n+1}' \var(\βh ∣ X) x_{n+1} = σ² x_{n+1}'(X'X)^{-1}x_{n+1}\]
\item draw a scatterplot to illustrate why $\hat y$ will depend on
       the particular value of $x$.
\end{itemize}

\subsection{common use of this variance}

\begin{itemize}
\item \textbf{if the errors are normal}, we can construct a confidence
       interval for $y_{n+1}$ from this result
\item $\hat y_{n+1} ± t_{α/2} \sqrt{s² (1 + x_{n+1}'(X'X)^{-1}x_{n+1})}$
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../finitesample"
%%% End: 
