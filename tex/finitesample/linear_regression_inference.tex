% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Inference on linear regression}%
\addcontentsline{toc}{part}{Inference on linear regression}

\section{Estimation of linear regression under restrictions}

\begin{itemize}
\item This material is based on \citet{Gre12} 5.1, 5.2, and 5.3.2.
\end{itemize}

\subsection{introduction}

We've been looking at the model $Y = Xβ + \ep$ or (equation by
equation) \[ y_i = x_{i,1} β₁ + ⋯ + x_{i,2} β₂ + \ep_i \]
\begin{itemize}
\item what if you thought that (say) $β₁ = β₂$
\item an example \citep[p. 81]{Gre12}; have a model of investment
  \[ \log I_t = β₁ + β₂ i_t + β₃ Δp_t + β₄ \log Y_t + β_5 t + \ep_t \]
\begin{itemize}
\item $I_t$ is investment in period $t$
\item $i_t$ is the nominal interest rate
\item $Δp_t$ is the rate of inflation
\item $Y_t$ is real output
\item time trend
\end{itemize}
\item so suppose you think that inflation and the nominal interest rate
      don't matter on their own, but combined they do matter as the
      \emph{real} interest rate
\begin{itemize}
\item model becomes
  \[ \log I_t = β₁ + β₂ (i_t - Δp_t) + β₄ \log Y_t + β_5 t + \ep_t \]
\item ie you want to estimate the first model under the restriction
  that $β₂ = - β₃$.
\item we will rewrite the restriction as $β₂ + β₃ = 0$.
\end{itemize}
\item So, how would you estimate the first equation under this
      restriction?
\end{itemize}

\subsection{LM restrictions for a simple example}

\begin{itemize}
\item to derive the OLS estimators, we minimized $∑_i (y_i - x_i'β)²$
\begin{itemize}
\item this is equivalent to MLE under normality
\end{itemize}
\item If we want to estimate the restricted model, we can just minimize
  $∑_i (y_i - x_i'β)²$ subject to the constraint that (in the previous
  example) $β₂ + β₃ = 0$.
\begin{itemize}
\item set up lagrangian: $∑_i (y_i - x_i'β)² + (β₂ + β₃)λ$
\item gives foc:
\begin{itemize}
\item $(0, 1, 1, 0) · β = 0$
\begin{itemize}
\item rewrite this as $Rβ = 0$
\end{itemize}
\item $0 = - 2 ∑_i x_i (y_i - x_i'β) + (0, 1, 1, 0)' λ$
\begin{itemize}
\item can be written $0 = 2 X'Xβ - 2 X'Y + R' λ$
\end{itemize}
\end{itemize}
\item we can start to solve:
\begin{itemize}
\item $\βh_R = \βh - 1/2 (X'X)^{-1} R' \λh_R$
\item $R \βh_R = 0$ (scalar)
\begin{itemize}
\item $= R \βh - 1/2 R (X'X)^{-1} R' \λh_R$
\end{itemize}
\end{itemize}
\item so $- \λh_R/2 = (R (X'X)^{-1} R')^{-1} R\βh$
\item $\βh_R = \βh - (X'X)^{-1} R' [R(X'X)^{-1}R']^{-1} R\βh$
\item For intuition, look at the fitted values
\end{itemize}
\end{itemize}

\paragraph{diagram that should give intuition:}

\subsection{LM restrictions on a general example}

\begin{itemize}
\item suppose now that we have the restriction $Rβ = q$
\item take first order conditions for the lagrangian
  \[ (Y - Xβ)'(Y - Xβ) + (Rβ - q)'λ \] (now $λ$ is a vector)
\begin{itemize}
\item $Rβ = q$
\item $0 = 2 X'X - 2 X'Y + R' λ$
\end{itemize}
\item mimic the same steps as before
\begin{itemize}
\item $\βh_R = \βh - \frac12 (X'X)^{-1} R'\λh_R$
\item $-\frac12 \λh_R = (R(X'X)^{-1}R')^{-1}(q - R\βh)$
\item $\βh_R = \βh + (X'X)^{-1} R'(R(X'X)^{-1}R')^{-1} (q - R\βh)$
\end{itemize}
\end{itemize}

\subsection{remarks}

\begin{itemize}
\item this is also the \textbf{restricted MLE} under normality
\item estimator of $σ²$ under this restriction is going to be
  $\ov{n - k - 1 + rank(R)} ∑_i (y_i - x_i'\βh_R)²$
\end{itemize}

\section{Hypothesis testing in finite samples}

\begin{itemize}
\item Reading for this material: \citet{Gre12} 4.7, 5.1-5.3
\end{itemize}

\subsection{introduction}

\begin{itemize}
\item Suppose that we want to test an arbitrary linear hypothesis about
  $β$; ie \[R β = q\] against the alternative \[R β ≠ q\]
\begin{itemize}
\item ie $R = (1, 0, 0, ...)$ and $q=0$ gives us a test that $β₀=0$
\item $R = I$ and $q = (0,0,...,0)$ gives us a test that all of the
         coefficients are equal to zero.
\end{itemize}
\item for now, assume we have normal errors
\end{itemize}

\subsection{a convenient test}

\paragraph{change in SSR under the null}
      we can look at the change in the SSR when we impose the null
        hypothesis
\begin{itemize}
\item ie ${SSR_R - SSR \over SSR}$
\begin{itemize}
\item $SSR = ∑_i \eph²_i$
\item don't need to present this, but can be written as \[
  {(R² - R²_R) / J \over (1-R²) / (n-k-1)} \]
\end{itemize}
\item Our test is actually a scaled version of that:
  \[ F = {(SSR_R - SSR)/J \over SSR / (n-k-1)} \]
\begin{itemize}
\item $J$ is the number of restrictions (ie dimension of $q$)
\end{itemize}
\end{itemize}

\paragraph{Distribution of F under null}
      now, suppose the null is true, what is the distribution of $F$?

\paragraph{distribution of numerator}
\begin{itemize}
\item reexpress the numerator
  \begin{align*}
    SSR_R - SSR
    &= (Y - X\βh_R)'(Y - X\βh_R) - (Y - X\βh)'(Y - X\βh) \\
    &= (\βh - \βh_R)'X'X(\βh - \βh_R)
  \end{align*}
  (you're proving this for homework).  Remember that
  \begin{align*}
    \βh_R &= \βh + (X'X)^{-1} R'(R(X'X)^{-1}R')(q - R\βh) \\
    &= (q - R\βh)' (R(X'X)^{-1}R')^{-1} R(X'X)^{-1}X'X(X'X)^{-1}R' (R(X'X)^{-1}R')^{-1} (q - R\βh) \\
    &= (q - R\βh)' (R(X'X)^{-1}R')^{-1} R(X'X)^{-1} R'(R(X'X)^{-1}R')^{-1} (q - R\βh) \\
    &= (q - R\βh)' (R(X'X)^{-1}R')^{-1} (q - R\βh)
  \end{align*}
\item distribution of numerator
\begin{itemize}
\item $q - R\βh$ is normal with mean $q - Rβ$ and variance $σ²
  R(X'X)^{-1}R'$
\item under the null, this mean is zero.
\item so we have a normal divided by its variance covariance matrix...
\item so $(q - R\βh)'(R(X'X)^{-1}R')^{-1}(q - R\βh)$ equals $σ²$
  chi-square r.v. with $J$ degrees of freedom
\end{itemize}
\end{itemize}

\paragraph{distribution of denominator}
\begin{itemize}
\item just like earlier, we know that $SSR = (n-k-1) s²$ and $s²$
         and $\βh$ are independent given $X$.
\item denominator is $σ²$ times a chi-square with $n-k-1$
         degrees of freedom and indpendent of the numerator.
\end{itemize}

\paragraph{distribution of statistic}
       distribution of $F$
\begin{itemize}
\item $F = {σ² χ²_J / J \over σ² χ²_{n-k-1} / (n-k-1)}$ in
  distribution.
\item numerator and denominator are independent
\item so this has an $F_{J, n-k-1}$ distribution under the null.
\end{itemize}

\paragraph{Distribution of F under alternative}
\begin{itemize}
\item Denominator is not affected
\item Numerator is
\begin{itemize}
\item for $R\βh - q$ to have mean zero, we need the null to
          be true
\item otherwise the numerator will get larger
\end{itemize}
\end{itemize}

\subsection{t-test}

\begin{itemize}
\item Suppose you wanted to test a single restriction, say $β_i =
       b$ for some known $b$.
\item We know that ${\βh_i - β_i \over \sqrt{s² q_i}}$ is
       the ratio of a standard normal r.v. and a chi-square/(n-k-1)
       random variable
\item so it is $t$ with (n-k-1) degrees of freedom
\item under the null, we know $β_i = b$, so we also have
       ${\βh_i - b \over \sqrt{s² q_i}}$
\item we can use this as a test statistic:
\begin{itemize}
\item calculate the value of the r.v.
\item get the appropriate critical values from the t-distribution
         table (or the computer)
\item reject if the statistic is farther from zero than the
         critical value
\end{itemize}
\item If we're testing an equality, this will give us exactly the
       same test as the F-test with 1 and $n-k-1$ degrees of freedom
\item if we're testing an inequality, this test can be a little
       easier to work with.
\end{itemize}

\subsection{likelihood ratio test}

\begin{itemize}
\item how would we test this hypothesis?
\begin{itemize}
\item maybe derive the likelihood ratio test.
\item remember, LRT is ${\sup_{β, σ²: Rβ = q} L(β, σ²; Y ∣ X) \over
    \sup_{β, σ²} L(β, σ²; Y ∣ X)}$ and we reject if this statistic is
  less than a critical value $c$.
\end{itemize}
\item we can plug in the MLEs
\begin{itemize}
\item $= {\σh^n \over \σh_{null}^n} exp((2\σh²_a)^{-1} ∑_i (y_i -
  x_i'\βh)² - (2\σh²₀)^{-1} ∑_i (y_i - x_i'\βh₀)²)$
\item note that $\σh²_a = n^{-1} ∑_i (y_i - x_i'\βh_a)²$
\item $\σh²₀ = n^{-1} ∑_i (y_i - x_i'\βh₀)²$
\item so the terms in the exponent cancel and we're left with
  $(\σh² / \σh²_R)^{n/2}$
\end{itemize}
\end{itemize}

\section{Hypothesis testing in the limit}

\subsection{Testing linear hypotheses}

\paragraph{t-test}

\paragraph{justifying the t-test}
\begin{itemize}
\item what does the previous asymptotic result tell us about the t-statistic?
\item suppose we want to test the null hypothesis
  \[ β_j ≤ 1 \] so we construct the t-statistic
\item can we use it?
\item we know that, if the null is true,
         \[ {β_j - 1 \over \sqrt{s² q_i}} → N(0,1) \] in
         distribution (where $q_i$ is the ii element of $(X'X)^{-1}$)
\item we also know that $t_{n-k-1} → N(0,1)$ as $n→∞$
\begin{itemize}
\item \[t_{n-k-1} = {Z₀ \over \sqrt{(n-k-1)^{-1} ∑_{i=1}^{n-k-1}
           Z²_i}}\] in distribution, where each of the $Z_i$ is an
           independent standard normal.
\item The sum in the denominator obeys a law of large numbers, so
           it converges to its mean, 1, in probability
\end{itemize}
\end{itemize}

\paragraph{interpretation/conclusion}
\begin{itemize}
\item We have two approaches for testing the hypothesis that
         $\βh_i = β_i^*$ (or has an inequality) based on
         asymptotic approximations
\begin{enumerate}
\item calculate ${\βh_i - β_i \over \sqrt{s² q_i}}$
            and compare to critical values from the normal distribution.
\item calculate and compare to critical values from the
            $t$-distribution.
\end{enumerate}
\item These will \textbf{almost always} give you the same answer.
\begin{itemize}
\item the critical values for the $t$-distribution will always be a
           little bit bigger
\item this happens because we replace the scaled chi-squared
           distribution in the denominator with the constant 1
\end{itemize}
\item If they \textbf{don't} give you the same answer, watch out.
\begin{itemize}
\item you should probably go with the t-distribution (since it's
           more conservative)
\item this discrepency is telling you that you don't have enough
           observations to get reliable asymptotic approximations
\item doesn't mean that you should \textbf{believe} the errors are
           normal, but it means that you should probably play it safe
           and \textbf{not reject}
\item If you run into this situation, you'll want to think about
           what you're doing a little more carefully.
\end{itemize}
\end{itemize}

\paragraph{F test/Wald test}

\paragraph{introduction}
\begin{itemize}
\item Suppose we want to test the general restriction $Rβ = q$
\item If the errors are normal, we know that the F-test is valid
\item What is the asymptotic distribution of the F-test if the
         errors aren't normal?
\end{itemize}

\paragraph{asymptotic distribution}
       We can rewrite the F-statistic as
       \[ J^{-1} (R\βh - q)'[s² R(X'X)^{-1}R']^{-1}(R\βh - q) \]
\begin{itemize}
\item under the null, we know that $q = Rβ$, so we also know
         that $R(\βh - β) = R\βh - q$
\item we know from earlier that $\sqrt{n} R(\βh - β)$
         converges in distribution to $N(0, σ² R Q^{-1} R')$
\item so, we know that \[\sqrt{n} (R\βh - q)' (σ² R
         Q^{-1} R')^{-1} \sqrt{n} (R\βh - q) \] converges in
         distribution to a chi-square with $J$ degrees of freedom
\begin{itemize}
\item $\sqrt{n}(R\βh - q)$ is a $J$-vector and is asymptotically
           normal.
\item $\sqrt{n}(σ² R Q^{-1} R')^{-1/2} (R\βh - q)$ is
           an asymptotically standard normal $J$ vector.
\end{itemize}
\item we want to show that \[ (R\βh - q)'[s² R(X'X)^{-1}R']^{-1}(R\βh
  - q) = J^{-1} (R\βh - q)'[σ² RQ^{-1}R']^{-1}(R\βh - q) + o_p(1)\]
\begin{itemize}
\item this will show that $J F$ is asymptotically chi-square with
           $J$ degrees of freedom.
\item multiply $(R\βh - q)$ by $\sqrt{n}$ and bring $n^{-1}$
           next to $X'X$.
\item we know that $s² → σ²$ and $n^{-1} X'X → Q$ in probability
\item so, we get the convergence we need.
\end{itemize}
\item So, we know that the F statistic is equal in distribution
  to a $J^{-1} χ²_J$ random variable as $n → ∞$.
\end{itemize}

\paragraph{conclusion/interpretation}

\paragraph{behavior of F distribution}

\begin{itemize}
\item have the same issue we saw with the t-distribution
\item the $F_{J, n-k-1}$ distribution is defined to be the same as
          the distribution of
          \[{J^{-1} ∑_{i=1}^J Z²_i \over (n-k-1)^{-1} ∑_j W_j²}\] 
          where each $Z$ and $W$ is an independent standard normal.
\item as $n→ ∞$, the denominator converges to 1 in probability
\item the numerator is unaffected and is a $χ²_J / J$ random variable.
\item so the F-distribution becomes more and more like the chi
          square as well.
\end{itemize}

\paragraph{conclusion}
\begin{itemize}
\item You can use either the $F_{J, n-k-1}$ or the Chi-square $J$ to
          construct a hypothesis test.
\begin{itemize}
\item calculate the F test statistic
\item get the critical values from a table or the computer
\item reject if test stat is greater than critical values
\end{itemize}
\item Just like with t-test, using the finite sample test (F) is
          more conservative than the asymptotic test
\begin{itemize}
\item critical values are a little bigger
\item this is a bigger deal when you're testing lots of
            restrictions simultaneously, so you are more likely to get
            disagreement than you did with the t-test
\item \textbf{use the F critical values}
\end{itemize}
\end{itemize}

\subsection{Testing nonlinear hypotheses}

     Reading is based on \citet{Gre12} Section 5.5.

\paragraph{motivation}
What if we want to test the hypothesis that $c(β) = q$ where $c$ is a
function from $\RR^{k+1}$ to $\RR^J$?
\begin{itemize}
\item Do we know anything about the finite-sample distribution of
  $c(\βh)$ that we could use to construct a test statistic?
\begin{itemize}
\item as usual, if we specify a distribution for the errors, we
          could crank through the algebra and derive the distribution of
          $c(\βh)$
\item won't give us a general procedure
\item won't necessarily be easy to impose the null hypothesis.
\end{itemize}
\item Maybe we can get a procedure that uses the asymptotic
  distribution of $c(\βh)$ under the null
\end{itemize}

\paragraph{initial asymptotics}
\begin{itemize}
\item suppose that $c$ is differentiable
\item we can use the delta-method to show that
  $\sqrt{n}(c(\βh) - c(β))$ is asymptotically normal
\item a first-order taylor expansion gives us
  \[ \sqrt{n} c(\βh) = \sqrt{n} c(β) + {∂ c(β) \over ∂ β'} \sqrt{n}
  (\βh - β) + o_p(1) \] where $∂ c(β) \over ∂ β'$ is a $J × k+1$
  matrix of partial derivatives.
\item We know that \[{∂ c(β) \over ∂ β'} \sqrt{n} (\βh - β)\]
  converges in distribution to a normal r.v. with mean zero and
  variance \[ \left({∂ c(β) \over ∂ β'}\right) σ² Q^{-1} \left({∂ c(β)
      \over ∂ β'}\right)'\]
\item Then $\sqrt{n} (c(\βh) - c(β))$ also converges to 
  \[ N\left(0, \left({∂ c(β) \over ∂ β'}\right) σ² Q^{-1} \left({∂
        c(β) \over ∂ β'}\right) \right)\]
\end{itemize}

\paragraph{hypothesis testing}
\begin{itemize}
\item Under the null hypothesis, we know that $c(β) = q$, so
  $\sqrt{n}(c(\βh) - q)$ has the same asymptotic distribution as
  $\sqrt{n}(c(\βh) - c(β))$
\item so we can do the same trick as before to get a chi-square test
        statistic
\begin{itemize}
\item we estimate the variance with
  \begin{equation*}
    \left(\frac{∂c(\βh)}{∂\βh'}\right) s²(n^{-1}X'X)^{-1} \left(\frac{∂c(\βh)}{∂\βh'}\right)
  \end{equation*}
\begin{itemize}
\item know $\βh →^p β$ so $\left({∂ c(\βh) \over ∂ \βh'}\right) →
  \left({∂c(β) \over ∂ β'}\right)$ in probability
\item know that $s² (n^{-1} X'X)^{-1} → σ²Q^{-1}$ in probability
\end{itemize}
\end{itemize}
\item Our test statistic is 
  \[ (c(\βh) - q)' \left[\left({∂ c(\βh) \over ∂ \βh'}\right)s²
    (n^{-1}X'X)^{-1} \left({∂ c(\βh) \over ∂ \βh'}\right) \right]^{-1}
  (c(\βh) - q) \]
\item asymptotically chi-square with $J$ degrees of freedom
\item one additional point
\begin{itemize}
\item we need to verify that the population variance matrix is
          invertible
\item ie we need \[\left({∂ c(β) \over ∂ β'}\right)\] to have full rank
\item in ``standard'' applications this is true, but it \textbf{may} not be true
\item can use a similar procedure but higher order expansion and
          usually won't wind up with a chi-square statistic.
\end{itemize}
\end{itemize}

\section{Testing multiple hypotheses}

Some special issues come up when we have many hypotheses.  I need to
figure out some simulations to motivate this stuff.

\subsection{Intersection-Union Test}

\begin{itemize}
\item Suppose the null can be written as an intersection of simpler nulls:
\begin{itemize}
\item $H₀: θ ∈ ⋃_{γ ∈ Γ} Θ_γ$ vs.  $H_A: θ ∉ ⋂_{γ ∈ Γ} Θ_γ$
\item It is easy to find tests for each individual set:
\begin{itemize}
\item $T_γ$ tests the null $θ ∈ Θ_γ$ vs. $θ ∉ Θ_γ$ for each $γ$.
\end{itemize}
\item If $T_γ$ is a level-$α$ test with rejection region $R_γ$ (ie it
  rejects if $(x₁,...,x_n) ∈ R$), then the test with rejection region
  $⋂_{γ ∈ Γ} R_γ$ is the IU Test and has level $α$
\end{itemize}
\item Note that we don't need to correct the critical values here
\end{itemize}

\paragraph{Proof of validity}
\begin{itemize}
\item We know that, under the null, $θ ∈ Θ_γ'$ for at least one $γ'$
\item $\Pr_θ[(X₁,...,X_n) ∈ ⋂_γ R_γ] ≤ \Pr_θ[(X₁,..., X_n) ∈ R_{γ'}] ≤ α$
\end{itemize}

\subsection{Union-Intersection Test}

\paragraph{Setup of UIT}
\begin{itemize}
\item Suppose the null can be written as an intersection of simpler nulls:
\begin{itemize}
\item $H₀: θ ∈ ⋂_{γ ∈ Γ} Θ_γ$ vs.  $H_A: θ ∉ ⋂_{γ ∈ Γ} Θ_γ$
\item It is easy to find tests for each individual set:
\begin{itemize}
\item $T_γ$ tests the null $θ ∈ Θ_γ$ vs. $θ ∉ Θ_γ$ for each $γ$.
\end{itemize}
\item If $T_γ$ is a level-$\frac{α}{\# Γ}$ test with rejection region
  $R_γ$ (ie it rejects if $(x₁,...,x_n) ∈ R$), then the test with
  rejection region $⋃_{γ ∈ Γ} R_γ$ is the UI Test and has level $α$
\end{itemize}
\item Note that we don't need to use a stepdown procedure here
\item Could still do better if we used the joint distribution of $T_γ$
\end{itemize}

\paragraph{Show that testing at $α$ doesn't work}

\paragraph{Proof of validity}
\begin{itemize}
\item Follows from the same argument as in multiple comparisons
  \begin{align*}
    \Pr_θ[(X₁,...,X_n) ∈ ⋃_{γ} R_γ]
    &≤ ∑_{γ ∈ Γ} \Pr_θ[(X₁,...,X_n) ∈ R_γ] \\
    &≤ ∑_γ \frac{α}{\#Γ} \\
    &= α
  \end{align*}
\end{itemize}

\subsection{Multiple hypothesis testing}

\begin{itemize}
\item Suppose we have a bunch of hypotheses $θ ∈ Θ_γ$, but instead of
  being interested in knowing about compound hypotheses, we're
  interested in all of them
\begin{itemize}
\item have many different parameters
\item Put up empirical example where we're looking at regressors
\end{itemize}
\item Now we want to control the rate of ``familywise error''
\begin{itemize}
\item Let $I$ index the true nulls: $I = \{γ ∈ Γ : θ_γ ∈ Θ_γ\}$
\item FWE is \[\sup_{θ: θ_γ ∈ Θ_γ, γ ∈ I} P_θ[T_γ \text{ rejects for at least one } γ ∈ I]\]
\item want this error probability to be less than $α$
\end{itemize}
\item As with UIT, testing each one at $α$ doesn't work
\begin{itemize}
\item show this
\end{itemize}
\end{itemize}

\paragraph{Bonferroni bound}

\paragraph{Bonferroni-Holm stepdown}

\paragraph{Critical values from the joint distribution}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../finitesample"
%%% End:
