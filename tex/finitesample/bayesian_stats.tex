% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Bayesian statistics}%
\addcontentsline{toc}{part}{Bayesian statistics}

\section{Introduction}

\begin{itemize}

\item We're going to take a fairly moderate perspective on Bayesian
  statistics.  In contrast to point estimation or hypothesis testing,
  here we characterize our uncertainty about the unknown parameters of
  the likelihood function by modeling them as random parameters as
  well.  Note that this does ont require us to actually think that
  they are random; it's just a convenient way to characterize our
  uncertainty.

\item The density before we observe any data is called our
  \emph{prior} density.  The goal of the statistical exercise now is
  to update that density based on the observed data, giving us a
  \emph{posterior} density.

\end{itemize}

\subsection{mathematics}

\begin{itemize}
\item dead simple.
\item denote the whole sample as $X$ (ie $X₁,...,X_n$)
\begin{itemize}
\item we'll assume that this is iid, but that's not important
\end{itemize}
\item suppose that $X_i ∼ f_X(·; θ)$
\begin{itemize}
\item this specification is part of the model
\item this part looks like what we've done
\end{itemize}
\item now, suppose that $θ ∼ f_θ$
\begin{itemize}
\item so the previous line really should be
  \[X_i ∣ θ ∼ f_X(·, θ)\]
\item this density represents what we believe about the unknown
  parameters $θ$ before we look at any data
\item called the \emph{prior} distribution
\item parameters on this density are called ``hyperparameters''
\end{itemize}
\item We're going to base decisions on the posterior distribution of
  $θ$, which is updated to reflect what we've learned from the data
\begin{itemize}
\item posterior distribution is the conditional distribution
  $f_θ(t ∣ X)$
\item we get this using Bayes's rule:
  \[f_θ(t ∣ X = x) = \frac{f_X(x ∣ θ = t) f_θ(t)}{f_X(x)}\]
\end{itemize}
\item If we need to represent the posterior by a single number, we
       can do this as long as we have a loss function
\begin{itemize}
\item Remember, a loss function is some function $L$
\begin{itemize}
\item convex
\item $L(0) = 0$
\item these conditions can be generalized
\end{itemize}
\item estimator is the value $\θh$ that minimizes
  \[ \E(L(θ - \θh ∣ X₁,...,X_n) \]
\begin{itemize}
\item expectation is over $θ$
\end{itemize}
\item similar to certainty-equivalence.
\end{itemize}
\end{itemize}

\subsection{example}

\begin{itemize}
\item Suppose $X_i ∼ i.i.d.\ bernoulli(θ)$
\begin{itemize}
\item we're interested in $∑_i X_i ∼\ bernoulli(n, θ)$
\item $f(x ∣ θ) = \binom{n}{x} θ^x (1-θ)^{n-x}$
\end{itemize}
\item A prior might be that $θ ∼\ uniform(0,1)$
\end{itemize}

\section{Choice of prior}

\begin{itemize}
\item There are three ``standard'' methods for choosing a priod
\begin{itemize}
\item Uninformative prior
\begin{itemize}
\item ad hoc
\item formalized
\end{itemize}
\item Conjugate prior
\item carefully considered prior (this is less common)
\end{itemize}
\end{itemize}

\subsection{Uninformative prior}

\begin{itemize}
\item One chosen to (in a way) minimize the knowledge embodied in
       the prior
\item Often people use ad hoc uninformative priors
\begin{itemize}
\item can give misleading results
\end{itemize}
\item Formal method of getting uninformative prior:
\begin{itemize}
\item prior density should be proportional to $\sqrt{|I(θ)|}$
\begin{itemize}
\item ``abs'' is determinant
\item $I(θ)$ is the `information matrix' from the Cramer-Rao lower bound
  \[I_{ij}(θ) = \E\Big[ \frac{∂}{∂ θ_i} \log f_X(X ∣ θ) · \frac{∂}{∂ θ_j}
  \log f_X(X ∣ θ) \Big]\]
\end{itemize}
\end{itemize}
\item Reference prior
\begin{itemize}
\item Chooses the prior distribution that maximizes the expected
  distance between prior ($f_θ(·)$) and posterior ($f_θ(· ∣ X)$)
  distributions
\item often gives same result as Jeffreys Prior
\end{itemize}
\end{itemize}

\subsection{Conjugate prior}

\begin{itemize}
\item A conjugate prior is one that is chosen so that the prior and
       the posterior are in the same family
\begin{itemize}
\item computationally convenient
\end{itemize}
\item Definition: \citep[From][7.2.3]{CaB_2001} Let $\Fs$ denote the
  class of pdfs $f_X(x ∣ θ)$.  A class $Π$ of prior distributions is a
  \emph{conjugate family} for $\Fs$ if the posterior is in the class
  $Π$ for all $f$ in $\Fs$, all priors in $Π$, and all $x$.
\item Uniform is a special case of beta distribution with
       parameters (1,1)
\item consider beta prior with parameters $α$ and $β$ and
  $X ∼ binomial(n,p)$ DGP
\begin{itemize}
\item $p_θ(t) = \frac{Γ(α + β) t^{α-1} (1-t)^{β-1}}{Γ(α) Γ(β)}$
\item $p_{X}(S ∣ θ) = \binom{n}{S} θ^S (1-θ)^{n-S}
  = \frac{Γ(n + 1)}{Γ(S + Γ(n - S + 1)} θ^S (1-θ)^{n-s}$
\item To get posterior, combine the distributions as before
\begin{itemize}
\item $p_θ(t ∣ data) 
           = K × \frac{Γ(n + 1)}{Γ(S + 1) Γ(n - S + 1)} t^S (1-t)^{n-S} × \frac{Γ(α + β) t^{α-1} (1-t)^{β-1}}{Γ(α) Γ(β)}$
\begin{itemize}
\item $= K × \frac{Γ(n + 1) Γ(α + β)}{Γ(S + 1) Γ(n - S + 1) Γ(α) Γ(β)} t^{α + S - 1} (1 - t)^{β + n - S - 1}$
\end{itemize}
\item we can recognize that this is the beta($α + S, β + n - S$) density again; so we must have 
  \[ K = (\frac{Γ(n + 1) Γ(α + β)}{Γ(S + 1) Γ(n - S + 1)Γ(α)Γ(β)})^{-1} \frac{Γ(α + β + n)}{Γ(α + S)Γ(β + n)} \]
\end{itemize}
\item This is the beta distribution again:
\begin{itemize}
\item mean is $(S + α) / (n + α + β)$
\end{itemize}
\end{itemize}
\item for any bernoulli, if the prior is $beta(α,β)$ the posterior is
  $beta(S + α, N - S + β)$
\end{itemize}

\paragraph{advantages}
\begin{itemize}
\item mathematical convenience
\item interpretation as an augmented dataset
\begin{itemize}
\item Another way to interpret the prior in this case
\begin{itemize}
\item suppose we'd observed other data previously
\item $α$ is the number of successes we observed
\item $β$ is the number of failures
\end{itemize}
\item Increasing $α$ and $β$ decreases the variance of the prior
\begin{itemize}
\item makes the prior have a larger effect on our estimator
\end{itemize}
\item we see this for some families of distributions (exponential family)
\end{itemize}
\end{itemize}

\subsection{Asymptotics}

\begin{itemize}
\item What happens as $N → ∞$ (for conjugate prior)
\item $α$ and $β$ are fixed, so $\frac{R + α}{R + G + α + β}$ behaves
  like $\frac{R}{R+G}$
\item ie the prior doesn't matter and the Bayesian estimator
       (squared error loss) and MLE behave the same
\item typically the case for general priors, but one can construct
       counterexamples
\end{itemize}

\section{Implementing the prior numerically}

\begin{itemize}
\item for conjugate priors, everything is easy (if you know the analytical form)
\item otherwise, finding the density is kind of a bitch:
  \[f_θ(t ∣ X = x) ∝ f_X(x ∣ θ = t) f_θ(t)\]
\item On the computer, though, it's easy using metropolis-hastings algorithm
\begin{itemize}
\item remember the accept-reject algorithm we used for homework
\begin{itemize}
\item want to draw from some $X ∼ f$ w/ support between 0 and 1,
         height between zero and one.
\begin{itemize}
\item draw candidate $X$ from uniform distribution
\item draw another variable $V$ from uniform distribution
\item if $V ≤ f(X)$, keep $X$, otherwise draw another $X$ and $V$.
\end{itemize}
\item can be extended to non-uniform for both
\item problem, though:
\begin{itemize}
\item requires candidate density to be ``larger'' than real density
\begin{description}
\item[.] really talking about in the tails.
\item[.] draw
\end{description}
\end{itemize}
\item metropolis algorithm gets around this restriction
\begin{itemize}
\item one problem: gives asymptotic/approximate results
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{description of algorithm \citep[page 254]{CaB_2001}}

Suppose you can generate $V ∼ f_V$, and the density $f_Y$ has the same
support
\begin{enumerate}
\item Generate $Z₀ ∼ f_V$
\item To generate $Z_i$, $i > 0$:
\begin{itemize}
\item Generate $V_i ∼ f_V$
\item Set $Z_i$
\begin{itemize}
\item $= V_i$ with probability $\min\{ f_Y(V_i)/ f_V(V_i)
  × f_V(Z_{i-1}) / f_Y(Z_{i-1}), 1\}$
\end{itemize}
\item $= Z_{i-1}$ otherwise
\end{itemize}
\item As $i → ∞$, $Z_i → f_Y$ in distribution.
\end{enumerate}

\subsection{simple example}

\paragraph{ergodicity}
\begin{itemize}
\item let $V =$
\begin{itemize}
\item 0 with probability 1/4
\item 1 with probability 3/4
\end{itemize}
\item let $Y =$
\begin{itemize}
\item 0 with probability 1/2
\item 1 with probability 1/2
\end{itemize}
\item suppose $Z_{i-1} = 0$ and draw $V_i$
\begin{itemize}
\item if $V_i = 0$, $Z_i = 0$ always
\item if $V_i = 1$, $Z_i$ equals
\begin{itemize}
\item 1 with probability 1/3
\item 0 with probability 2/3
\end{itemize}
\item so $Z_i ∣ Z_{i-1} = 0$ is
\begin{itemize}
\item 0 with prob $1/4 + 2/3 × 3/4 = 3/4$
\item 1 with prob $1/3 × 3/4 = 1/4$
\end{itemize}
\end{itemize}
\item suppose $Z_{i-1} = 1$ and draw $V_i$
\begin{itemize}
\item if $V_i = 0$, $Z_i$ is
\begin{itemize}
\item 0 with probability $\min(.5/.25 × .75/.5, 1) =
            \min(3,1) = 1$
\item 1 with prob 0
\end{itemize}
\item if $V_i = 1$, $Z_i$ is 1 always
\item so $Z_i ∣ Z_{i-1} = 1$ is
\begin{itemize}
\item 0 with prob 1/4
\item 1 with prob 3/4
\end{itemize}
\end{itemize}
\item Now, suppose we have $Z_{i-1} ∼ f_Y$
\begin{itemize}
\item 0 with prob 1/2
\item 1 with prob 1/2
\end{itemize}
\item Then $\Pr[Z_i = 0] = \Pr[Z_i = 0 ∣ Z_{i-1}=0] \Pr[Z_{i-1} = 0] + \Pr[Z_i = 0 ∣ Z_{i-1}=1] \Pr[Z_{i-1} = 1]$
\begin{itemize}
\item $= \frac{3}{4} \frac{1}{2} + \frac{1}{4} \frac{1}{2} = \frac12$
\end{itemize}
\item So once we have a draw from $f_Y$, we can use the
        metropolis-hastings algorithm to generate another draw from $f_y$
\end{itemize}

\paragraph{getting the initial draw from $f_Y$}
\begin{itemize}
\item we can represent the ``transitions'' as a matrix
\item set \[P = \begin{pmatrix} 3/4 & 1/4 \\ 1/4 & 3/4 \end{pmatrix}\]
\item each row represents $\Pr[Z_i = z ∣ Z_{i-1} = z']$
\begin{itemize}
\item different rows give different $z'$
\end{itemize}
\item we can let the row-vector $Z₀ = (1-p, p)$ represent the
  probabilities that the initial $Z₀$ is 0 or 1.
\item Then, $Z₀ P$ gives probabilities that $Z₁$ is zero or 1
\begin{itemize}
\item $Z₀ P²$ gives probs that $Z₂$ is zero or 1
\item dot dot dot
\item $Z₀ P^i$ gives probs that $Z_i$ is zero or 1
\end{itemize}
\item we can diagonalize $P$
        \[P = Q D Q^{-1}\]
        with
        \[Q = \begin{pmatrix} 1 & -1/4 \\ 1 &
        1/4 \end{pmatrix},\qquad 
        Q^{-1} = \begin{pmatrix} 1/2 & 1/2 \\ -2 & 2 \end{pmatrix},
        \qquad 
        D = \begin{pmatrix} 1 & 0 \\ 0 & 1/2 \end{pmatrix}\]
\begin{itemize}
\item Q gives the right-eigenvectors of P
\item $Q^{-1}$ gives the left eigenvectors of $P$
\item D gives the eigenvalues of P
\end{itemize}
\item note that 
  \[P^i = (Q D Q^{-1})ⁿ = Q Dⁿ Q^{-1} = Q \begin{pmatrix} 1ⁿ & 0 \\
    0 & 0.5ⁿ \end{pmatrix} Q^{-1} → Q \begin{pmatrix} 1ⁿ & 0 \\ 0 &
    0 \end{pmatrix} = \begin{pmatrix} 0.5 & 0.5 \\ 0.5 &
    0.5 \end{pmatrix}\]
\item so $Z₀ Pⁿ = (1-p, p) Pⁿ → ((1-p)/2 + p/2, (1-p)/2 + p/2) =
  (1/2, 1/2)$
\begin{itemize}
\item no matter what density we start with, after a large number
          of steps we'll get draws from close to the marginal
          distribution.
\item quick numeric example (going to be much better behaved than
          typical mcmc application)
\begin{itemize}
\item \texttt{mpower <- function(M, power) \{}
\item \texttt{Mp <- M}
\item \texttt{for (i in seq\_len(power-1)) Mp <- Mp \%*\% M}
\item \texttt{Mp\}}
\item \texttt{P <- matrix(c(3/4,1/4,1/4,3/4), 2, 2)}
\item \texttt{c(1,0) \%*\% P}
\item \texttt{c(1,0) \%*\% mpower(P, 3)}
\item \texttt{c(1,0) \%*\% mpower(P, 10)}
\end{itemize}
\end{itemize}
\end{itemize}

\paragraph{wrap up of example}
\begin{itemize}
\item we made some simplifications
\begin{itemize}
\item finite number of possible values
\end{itemize}
\item basic result holds much more generally (ie continuous rv)
\end{itemize}

\subsection{numeric example}

\begin{itemize}
\item data: $X ∣ θ ∼$ binom($10, θ$)
\begin{itemize}
\item \texttt{n <- 10}
\item \texttt{x <- rbinom(1, n, runif(1))}
\item \texttt{x}
\end{itemize}
\item want to find
\begin{enumerate}
\item posterior distribution
\item minimum risk estimtor for squared-error (ie the mean of the posterior)
\item estimator that minimizes $\E(L(p - \hat p) ∣ X$ where
  $L(e) = \exp(-e) - 1$ if $e < 0$, \emph{e} if $e ≥ 0$
\begin{itemize}
\item \texttt{L2 <- function(e) ifelse(e < 0, exp(-e) - 1, e)}
\item \texttt{curve(L2(x), from = -1, to = 1)}
\end{itemize}
\end{enumerate}
\item prior: $θ ∼ k φ(x)$ if $x ∈ (1/4, 3/4)$, 0 otherwise
\begin{itemize}
\item $k$ chosen so that it integrates to 1
\item not necessarily realistic
\item note that Bayes rule tells us that anywhere the prior is zero,
         the posterior will be zero as well, so the posterior's going to be 
         between 1/4 and 3/4
\item need a function that will generate from prior:
\begin{itemize}
\item \texttt{rprior <- function() \{}
\begin{itemize}
\item \texttt{repeat \{}
\item \texttt{x <- rnorm(1)}
\item \texttt{if (x > 0.25 \& x < 0.75) break}
\item \texttt{\}}
\item \texttt{x}
\item \texttt{\}}
\end{itemize}
\item \texttt{hist(replicate(600, rprior()), 40, freq = FALSE, xlim = c(0,1))}
\end{itemize}
\item need a function that will evaluate (proportional to) prior:
\begin{itemize}
\item \texttt{dprior <- function(p) ifelse(p > 0.25 \& p < 0.75, dnorm(p), 0)}
\item \texttt{curve(dprior(x), from = 0, to = 1, n = 1000, add = TRUE)}
\end{itemize}
\item need to evaluate the density of the posterior:
\begin{itemize}
\item \texttt{dposterior <- function(p, x, n) dbinom(x, n, p) * dprior(p)}
\item \texttt{curve(dposterior(x), from = 0, to = 1, col = "red", add = TRUE, n = 1000)}
\end{itemize}
\item note: this is a toy example, so we really could just use this 
         function to calculate the mean, minimal loss estimator, etc.
\begin{itemize}
\item \texttt{mass <- integrate(function(p) dposterior(p, x, n), lower = 0.25, upper = 0.75)}
\item expected value: \texttt{integrate(function(p) p * dposterior(p, x, n) / mass\$value, lower = 0.25, upper = 0.75)}
\item minimum loss: \texttt{optimize(f = function(phat) integrate(function(p) L2(p - phat) * dposterior(p, x, n) / mass\$value, lower = 0.25, upper = 0.75)\$value, lower = 0.25, upper = 0.75)}
\item \textbf{but} in bigger problems (more parameters, more complicated distributions), these
           direct solutions won't be feasible... they'll take too long.
\item metropolis hastings can still work in those bigger problems
\end{itemize}
\end{itemize}
\item to implement, initialize vector of draws (``burn'' first 1000, use second 1000)
\begin{itemize}
\item \texttt{nsim <- 2000}
\item \texttt{Z <- c(rprior(), rep(NaN, nsim))}
\end{itemize}
\item make function that generates $Z_i$ given $Z_{i-1}$
\begin{itemize}
\item \texttt{rZ <- function(Zprev, rv, fv, fy) \{}
\begin{itemize}
\item \texttt{V <- rv()}
\item \texttt{U <- runif(1)}
\item \texttt{probV <- fy(V) * fv(Zprev) / (fv(V) * fy(Zprev))}
\item \texttt{if (U < probV) return(V)}
\item \texttt{else return(Zprev)\}}
\end{itemize}
\end{itemize}
\item run mcmc
\begin{itemize}
\item \texttt{for (i in seq\_len(nsim) + 1)}
\begin{itemize}
\item \texttt{Z[i] <- rZ(Z[i-1], rv = rprior, fv = dprior, fy = function(p) dposterior(p, x, n))}
\end{itemize}
\end{itemize}
\item get posterior:
\begin{itemize}
\item \texttt{hist(Z[-(1:1000)], 40)}
\item \texttt{curve(dposterior(x)/mass\$value, from = 0, to = 1, col = "red", add = TRUE, n = 1000)}
\end{itemize}
\item get estimates:
\begin{itemize}
\item expected value: \texttt{mean(Z[-(1:1000)])}
\item minimum loss: \texttt{optimize(function(phat) mean(L2(Z[-(1:1000)] - phat)), lower = 0.25, upper=0.75)}
\end{itemize}
\item look at the actual draws of $Z$
\begin{itemize}
\item \texttt{plot(Z[1500:1700], type = "o")}
\item really dependent (which makes sense), ie not iid draws
\item doesn't matter, because there is weak enough dependence that we
         can still get averages, etc.
\end{itemize}
\item what happens if our prior doesn't actually contain the ``true'' $p$?
\begin{itemize}
\item draw $x$
\begin{itemize}
\item \texttt{x <- rbinom(1, n, .9)}
\end{itemize}
\item reset $Z$ and rerun mcmc (same as before)
\begin{itemize}
\item \texttt{Z <- c(rprior(), rep(NaN, nsim))}
\item \texttt{for (i in seq\_len(nsim) + 1)}
\begin{itemize}
\item \texttt{Z[i] <- rZ(Z[i-1], rv = rprior, fv = dprior, fy = function(p) dposterior(p, x, n))}
\end{itemize}
\end{itemize}
\item \texttt{hist(Z[-(1:1000)], 40)}
\item bunches up at edge of support
\end{itemize}
\end{itemize}

\section{Bayesian modeling for OLS}

\begin{itemize}
\item For OLS, we have to specify a joint prior for
\begin{itemize}
\item $σ$ (ie $\E(\ep²_i ∣ X)$)
\item $β$
\item For convenience, we'll specify a prior for $β$ conditional on $σ²$.
\end{itemize}
\item We'll discuss conjugate and noninformative priors
\item Need to specify distribution of the random variables (Y): $N(Xβ,
  σ² I)$ given $X$
\begin{itemize}
\item We'll continue to condition on $X$ (assume X and $β$ and $γ$ are
  independent).
\end{itemize}
\end{itemize}

\subsection{densities for beta}

\begin{itemize}
\item Finite sample theory and asymptotics indicate that
  $\βh$ is going to behave as though it is approximately normal.
\item Suggests that getting a normal posterior might be a good idea
\item suggests that starting with a normal prior (for congugate) is too.
\item Prior :: $p(β ∣ 1/σ²) = N(β₀, σ² Σ₀)$
\begin{itemize}
\item ie prior is normal with known mean and variance
\end{itemize}
\end{itemize}

\paragraph{posterior distribution}
\begin{itemize}
\item Use Bayes' rule to get posterior:
  \[ p(β ∣ X, Y, σ²) = \frac{p(Y ∣ X, β, σ²) p(β ∣ X, σ²)}{p(Y ∣ X, σ²)}\]
\item $Y$ given $X$, $β$, and $σ²$ is (obviously) normal
\begin{itemize}
\item mean $Xβ$
\item variance $σ² I$
\end{itemize}
\item posterior is
  \[p(β ∣ X, Y, σ²) ∝ (\frac{1}{\sqrt{2πσ²}})ⁿ 
  \exp(-\frac{1}{2σ²}(Y - Xβ)'(Y - Xβ)) × (\frac{1}{\sqrt{2^k π^k \det(σ² Σ₀)}})
  \exp(- \frac{1}{2} (β - β₀)'(σ²Σ₀)^{-1} (β - β₀))\]
\begin{itemize}
\item terms inside the exponentials can be written as ($\σh²$ and
  $\βh$ are the MLE estimators)
  \[\exp(- \frac{1}{2 σ²} \{(\eph - X(β - \βh))' (\eph - X(β - \βh))\]
\begin{itemize}
\item \[(β - β₀)'Σ₀^{-1}(β - β₀)\})\]
\end{itemize}
which can be rewritten as 
\[\exp(- \frac{n \σh²}{2 σ²}) \exp(-\frac{1}{2σ²}
  \{(β - \βh)'X'X(β - \βh) + (β - β₀)Σ₀^{-1}(β - β₀)\})\]
\item To get this into something more useful, we want to rewrite
  \[(β - \βh)'X'X(β - \βh) + (β - β₀)Σ₀^{-1}(β - β₀)
  = (β - a β₀ - b \βh)'V(β - a β₀ - b \βh)\]
\begin{itemize}
\item expand both sides and match to find $a$, $b$, and $V$, giving
\begin{itemize}
\item $β'Vβ = β'X'Xβ + β Σ₀^{-1} β$
\begin{description}
\item[.] $V = X'X + Σ₀^{-1}$
\end{description}
\item $β'V a β₀ = β'Σ₀^{-1}β₀$
\begin{description}
\item[.] $V a = Σ₀^{-1}$
\item[.] $a = V^{-1} Σ₀^{-1} = (X'X + Σ₀^{-1})^{-1} Σ₀^{-1}$
\end{description}
\item $β'V b \βh = β'X'X \βh$
\begin{description}
\item[.] $V b = X'X$
\item[.] $b = V^{-1} X'X = (X'X + Σ₀^{-1})^{-1} X'X = I - a$
\end{description}
\end{itemize}
\item so we have 
  \[β'X'Xβ + β Σ₀^{-1} β
  = (β - aβ₀ - (I-a) \βh)'(X'X + Σ₀^{-1}) (β - aβ₀ - (I-a) \βh)\]
  with $a = (X'X + Σ₀^{-1})^{-1}Σ₀^{-1}$
\item the posterior of $β$ is
  $N(a β₀ + (I - a)\βh, σ² (X'X + Σ₀^{-1})^{-1})$
\end{itemize}
\end{itemize}
\end{itemize}

\paragraph{interpretation from a classical perspective}
\begin{itemize}
\item We can get a classical estimator by using the posterior mean:
  \[ \βh_{bayes} = aβ₀ + (I-a)\βh \]
\item shrinkage estimator
\begin{itemize}
\item ridge is a special case with $β₀ = 0$ and $Σ₀ ∝ I$
\end{itemize}
\item Can think of the posterior as an average of the prior and MLE
\begin{itemize}
\item Will be biased (in general)
\item will have smaller variance than the OLS estimator.
\end{itemize}
\item In general, as sample size increases and as certainty in
        prior decreases, the ``Bayesian'' estimator behaves more like the
        OLS estimator
\end{itemize}

\paragraph{uninformative prior}
\begin{itemize}
\item $Σ₀$ denotes how strong our beliefs are
\begin{itemize}
\item small value indicates that we are very confident in our
           prior mean
\item large value indicates that we don't have a lot of
           confidence in our prior mean
\end{itemize}
\item As $Σ₀ → ∞$, $Σ₀^{-1}$ converges to zero
\begin{itemize}
\item mean of posterior converges to $\βh$
\item variance of posterior converges to $σ²(X'X)^{-1}$
\end{itemize}
\item ie, the Bayesian estimator converges to OLS as the prior
         becomes less informative
\item interpretation makes sense: when you don't have strong
         beliefs, you should weight the data heavily
\end{itemize}

\paragraph{asymptotics}
\begin{itemize}
\item As $n → ∞$, $X'X → ∞$ and $a → 0$
\item Same as with uninformative prior; bayesian estimator
         converges to OLS estimator as $n$ gets large
\item makes sense: when you have a lot of data, you shouldn't rely
         too heavily on your prior beliefs.
\end{itemize}

\subsection{densities for sigma-squared}

\begin{itemize}
\item Conjugate prior for $1/σ²$ is the gamma distribution,
  so $σ²$ has an inverse-gamma distribution:
  \[p_{σ²}(σ² ∣ α, δ) = \frac{δ^α}{Γ(α)} (\frac1{σ²})^{α+1} e^{-\frac{β}{σ²}},
  \quad α,δ > 0\]
\begin{itemize}
\item mean is $δ / (α - 1)$ for $α > 1$
\item variance is $β² / (α-1)²(α-2)$ for $α > 2$
\end{itemize}
\item Joint prior for $β$ and $1/σ²$ is called the
       Normal-Gamma prior:
       \[p_{β,σ²}(β, σ² ∣ Σ₀, α, δ) = 
       (\frac{1}{\sqrt{2^k π^k \det(σ² Σ₀)}})
         \exp(- \frac{1}{2} (β - β₀)'(σ² Σ₀)^{-1} (β - β₀))
       \frac{δ^α}{Γ(α)}(\frac1{σ²})^{α+1} e^{-\frac{δ}{σ²}}\]
\item We can work out the posterior as before:
  \[p(β, σ² ∣ X, Y) ∝ (\frac{1}{\sqrt{2 π σ²}})ⁿ \exp(-\frac{1}{2σ²}(Y - Xβ)'(Y - Xβ))
  × (\frac{1}{\sqrt{2^k π^k \det(σ² Σ₀)}}) \exp(-\frac{1}{2} (β - β₀)'(σ² Σ₀)^{-1} (β - β₀))
  × \frac{δ^α}{Γ(α)} (\frac1{σ²})^{α+1} \exp(-\frac{δ}{σ²}) = (\frac{1}{\sqrt{2 π σ²}})ⁿ
    (\frac{1}{\sqrt{2^k π^k \det(σ² Σ₀)}}) \exp(- \frac{n \σh²}{2 σ²})
  × \exp(- \frac{1}{2σ²} \{(β - \βh)'X'X(β - \βh) + (β - β₀)Σ₀^{-1}(β - β₀)\})
  × \frac{δ^α}{Γ(α)} (\frac1{σ²})^{α+1} \exp(-\frac{δ}{σ²}) \]
  which is proportional to
  \[ (\frac{1}{2 π σ²})^{k/2} \det(X'X + Σ₀^{-1})
  \exp( -\frac{1}{2σ²} (β - aβ₀ - (I-a)\βh)' (X'X + Σ₀^{-1})(β - a β₀ - (I - a)\βh))
  (\frac{1}{2 π σ²})^{\frac{n}{2}} \exp(- \frac{n\σh²}{2σ²})
  \frac{δ^α}{Γ(α)} (\frac1{σ²})^{α+1} \exp(-\frac{δ}{σ²})\]
\begin{itemize}
\item The first part is the posterior of $β$ conditional on $σ²$.
\item We can rewrite the second part to be more interpretable:
\item combining terms and dropping scale constants gives:
  \[(\frac{1}{σ²})^{n/2 + α + 1} \exp(- \frac{\σh² n/2 + δ}{σ²})\]
\item after thinking for a bit, we can see that this is the kernel of an inverse
         gamma with parameters
\begin{itemize}
\item $n/2+α$ and
\item $\σh² n/2 + δ$
\end{itemize}
\item mean is
  \[\frac{\σh² n/2 + δ}{n/2 + α - 1}
  = \σh² \frac{n}{n + 2α - 2} + δ(\frac{2}{n + 2α - 2})\]
\item to interpret this,
\begin{itemize}
\item define prior mean: $σ₀² = \frac{δ}{α - 1}$
\item $δ = σ²₀ (α - 1)$
\item posterior mean becomes 
  \[\σh² \frac{n}{n + 2α - 2} + σ₀²(\frac{2 α - 2}{n + 2α - 2}) 
  = w \σh² + (1-w) σ₀²\]
\item Bayes estimate is a weighted average of the MLE and the
           prior mean
\item as $n → ∞$, $w → 1$ and MLE dominates
\item we can write prior variance as $σ₀^4 / (α - 2)$
\begin{itemize}
\item for $σ₀$ fixed, as $α → ∞$ prior
             variance converges to zero and $w → 0$ as well, so
             posterior mean dominates.
\item for $σ₀$ fixed, as $α → 1$ from above, $w$
             converges to 1 again and MLE dominates
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{putting it all together}

After the analysis above, we can write the posterior density of $β$
and $σ²$ explicitly as
\[ (\frac{1}{2 π σ²})^{k/2} \det(X'X + Σ₀^{-1}) \exp( -\frac{1}{2σ²}
(β - a β₀ - (I - a)\βh)' × (X'X + Σ₀^{-1})(β - a β₀ - (I - a)\βh)) ×
\frac{(\σh² n/2 + δ)^{α + n/2}}{Γ(n/2 + α)} (\frac{1}{σ²})^{n/2 + α +
  1} \exp(-\frac{\σh² n/2 + δ}{σ²}) \]

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../finitesample"
%%% End:
