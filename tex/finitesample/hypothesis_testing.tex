% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Introduction to hypothesis testing}%
\addcontentsline{toc}{part}{Introduction to hypothesis testing}

\begin{itemize}
\item Motivation
\begin{itemize}
\item there are many settings where we care what the unknown
       coefficients are.
\begin{itemize}
\item forecasting
\item setting policy
\end{itemize}
\item there are plenty of other settings where we don't really care what
       the coefficients are, but just want to know whether the
       coefficient is above some threshold
\end{itemize}
\item Example: have a basic relationship: $wage_t = β₀ + β₁
     × education_t + \ep_t$
\begin{itemize}
\item may want to know whether $β₁$ is positive
\item may want to know whether $β₁$ is above some threshold that
       makes funding a program worthwhile
\item probably don't care about whether or not $β₁$ is exactly
       2.95 vs 2.97 -- they probably mean exactly the same thing as far
       as your decision is concerned.  But $-0.01$ vs 0.01 probably don't
       mean exactly the same thing as far as decisions are concerned
\item For (most) estimation, though, we would treat mismeasuring the
       true parameter by 0.02 the same, whether the true value was 2.95
       or $-0.01$
\end{itemize}
\end{itemize}

\section{Basic setup for testing}

\subsection{Definition of test statistic}

\begin{itemize}
\item Suppose that you have two different hypotheses about the
       unknown parameters of the DGP
\begin{description}
\item[null] $θ ∈ Θ₀$
\begin{itemize}
\item the null hypothesis is what we would believe about $θ$ if we
           had no data about the problem
\end{itemize}
\item[alternative] $θ ∈ Θ_a$
\begin{itemize}
\item the alternaive hypothesis is a candidate belief that might be true.
\item but we need \textbf{strong} evidence before we believe it
\end{itemize}
\end{description}
\item A \emph{test statistc} is a statistic that specifies the following:
\begin{enumerate}
\item For which samples do we make the decision to accept $θ_a$
          (our alternative) as true
\item For which samples do we \textbf{not} make that decision, and instead
          act as though $θ₀$ were true.
\end{enumerate}
\end{itemize}

\subsection{Example of a test statistic}

\begin{itemize}
\item Suppose $X₁,...,X_n ∼ iid N(μ,1)$ and we have the hypotheses:
\begin{description}
\item[null] $μ = 0$
\item[alt] $μ = 1$
\end{description}
\item we can use the standard one-sided test statistic:
\begin{description}
\item[reject 0] if $\sqrt{n} (\Xb - μ₀) / σ = \sqrt{n}
                     \Xb ≥ 1.64$
\item[accept 0] if $\sqrt{n} \Xb < 1.64$
\end{description}
\item tells us that we reject null for any sample that gives us a mean
       greater than $1.64 / \sqrt{n}$ and do not reject for other
       samples
\end{itemize}

\subsection{Error types and probabilities}

\begin{itemize}
\item Test statistics can't be foolproof.
\begin{itemize}
\item Even if $μ = 0$, we'll see some samples that have a sample
         mean greater than $1.64 / \sqrt{n}$
\item If $μ = 1$, we'll also see samples with a small mean
\item The key aspect of statistical testing is that we can calculate
         the probabilities of making different sorts of errors, and
         design procedures that make errors that we're comfortable with.
\end{itemize}
\item Possible outcomes:
\begin{itemize}
\item null is true, test statistic does not reject
\begin{itemize}
\item great!
\end{itemize}
\item null is true, test statistic does reject
\begin{itemize}
\item problem
\end{itemize}
\item null is false, test statistic does not reject
\begin{itemize}
\item problem
\end{itemize}
\item null is false, test statistic rejects
\begin{itemize}
\item great!
\end{itemize}
\end{itemize}
\item key philosophy behind testing:
\begin{itemize}
\item we set up the problems so that the null is what we'd do
         without data.
\item consequently, if the null is false, but the test statistic
         does not reject, it's not so bad
\item \textbf{but} if the null is true and we reject it, that is bad.
\begin{itemize}
\item we should need more evidence to change our behaviour than we
           would need to keep doing the same thing we were.
\end{itemize}
\item advantages of this approach:
\begin{itemize}
\item simplifies the mathematics considerably
\item intuitively appealing.
\item matches up with actual practice in ``academic'' scientific experiments
\end{itemize}
\end{itemize}
\item terminology
\begin{itemize}
\item type one error: reject when you shouldn't
\item type two error: don't reject when you should
\item power: probability that the test rejects (usually will depend
         on the value of the true parameter).
\begin{itemize}
\item formally, let $β(θ) = \Pr_θ[reject]$.  $β(θ)$ is the power function.
\end{itemize}
\item size: probability that the test rejects if the null hypothesis is true;
\begin{itemize}
\item i.e. probability of type I error
\item a test is \emph{size} $α$ if $\sup_{θ ∈ Θ₀} β(θ) = α$
\item a test is \emph{level} $α$ if $\sup_{θ ∈ Θ₀} β(θ) ≤ α$
\item \textbf{size} is what we care most about.  We want to set the size
\end{itemize}
\item valid: a test that has correct size -- ie rejects if the null
         is true at the percentages we want
\item unbiased: a test is unbiased if $\inf_{θ ∈ Θ_A} β(θ) ≥ α$
\end{itemize}
\end{itemize}

\subsection{Motivating example}

\begin{itemize}
\item sometimes we can monkey with a statistic to get a test
\item think back to normal distribution with $H₀: μ = 0$ vs
       $H_a: μ = 1$ and we want to test at 5\%
\begin{itemize}
\item statistic was $\sqrt{n} \Xb$.
\item if $μ = 0$, then we know that $\sqrt{n} \Xb$ is standard normal.
\item $\Xb > 0$ is evidence against the null, so let's try to
         find a threshold that preserves size
\begin{itemize}
\item find $c$ such that $\Pr[\sqrt{n} \Xb ≥ c] = 0.05$
  \begin{align*}
    \Pr[\sqrt{n} \Xb ≥ c] &= 1 - \Pr[\sqrt{n} \Xb < c] \\
    &= 1 - \Pr[\sqrt{n} \Xb ≤ c] \text{ (continuity)} \\
    &= 1 - Φ(c)
  \end{align*}
  so, $Φ(c) = 0.95$ and $c = Φ^{-1}(0.95) \approx 1.64$
\end{itemize}
\item So the probability that $\sqrt{n} \Xb ≥ 1.64 = 0.05$
\item so, the probability of rejecting if the null hypothesis is
         true is 0.05.
\begin{itemize}
\item the size of the test is 0.05
\item if we want to change the size of the test, all we have to
           do is change 1.64 to a different number.
\end{itemize}
\end{itemize}
\item power
\begin{itemize}
\item knowing the distribution under the null gives us size
\item to find out the power, we want to know the distribution under
         the alternative
\begin{itemize}
\item if $μ = 1$ then $\sqrt{n}(\Xb - 1)$ is standard normal.
\item $β(1) = P[\sqrt{n} \Xb ≥ 1.64] = P[\sqrt{n}(\bar
           X - 1) ≥ 1.64 - \sqrt{n}] = 1 - Φ(1.64 - \sqrt{n}) → 1$
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{p-values}

\begin{itemize}
\item Say you're working, do a test at 5\% and want to show your advisor
\item maybe your advisor is more or less tolerant of type 1 error
\item so you test at 10\%, 9.9\%, 9.8\%, etc and record rejections and acceptances
\item but, you could summarize that by looking at the smallest critical value for which you would reject
\item this critical value is the p-value
\end{itemize}

\section{Introduction to interval estimation}

\begin{itemize}
\item We can take the ``hypothesis testing mentality'' and apply it to
      point estimation
\begin{itemize}
\item apply the philosophy of hypothesis testing
\item apply the mathematics of hypothesis testing
\end{itemize}
\item point estimation
\begin{itemize}
\item want to get a value
\begin{itemize}
\item have an unknown parameter $θ$
\item use that data to construct a best guess for $θ$
\item we're going to use that value as though it were true
\end{itemize}
\end{itemize}
\item confidence intervals
\begin{itemize}
\item want to get a range of values that are \emph{possible}
\item we're going to consider those that are inside the interval
        indistinguishable from eachother and those outside
        indistinguishable from eachother too.
\end{itemize}
\end{itemize}

\section{Definitions}

\begin{itemize}
\item An \emph{interval estimator} of a parameter $θ$ is any pair of
      functions $L(X)$ and $U(X)$ s.t. $L(x) ≤ U(x)$ for all $x$
      and, when we observe $x$ we make the inference $θ ∈
      [L(x), U(x)]$
\item The \emph{coverage probability} of an interval estimator
      $[L(X),U(X)]$ is the probability $P_θ[θ ∈ [L(X), U(X)]]$
\item The \emph{confidence coefficient} is $\inf_θ P_θ[θ ∈ [L(X), U(X)]]$
\end{itemize}

\section{Relationship to hypothesis testing}

\begin{itemize}
\item we construct confidence intervals by inverting tests
  \citep[Section 9.2]{CaB_2001}
\begin{itemize}
\item For each $θ₀$, let $A(θ₀)$ be the acceptance region of a level
  $α$ test of $H₀: θ = θ₀$.  For each sample $x$, define a set $C(x)$
  in the parameter space by \[C(x) = \{θ₀: x ∈ A(θ₀)\}\].  Then the
  random set $C(X)$ is a $1-α$ confidence set.
\item prove this result for hw
\end{itemize}
\item plot interval generated by the indicator functions: 1 if test
      rejects, zero otherwise. (on computer?)
\item using this result
\begin{itemize}
\item suppose $X₁,...,X_n ∼ N(μ,σ²)$ and we want to construct a
  two-sided $1-α$ confidence interval for $μ$.
\begin{itemize}
\item $σ²$ is known
\end{itemize}
\item start by getting the family of tests:
\begin{itemize}
\item test of $H₀: μ = μ₀$ vs $μ ≠ μ₀$ is given by:
\begin{description}
\item[reject] if $μ₀ > \Xb + z_{α/2} σ / \sqrt{n}$ or $μ₀ < \Xb -
  z_{α/2} σ / \sqrt{n}$.
\item[accept] otherwise
\end{description}
\item gives $A(μ₀) = \{x :\Xb - z_{α/2} σ / \sqrt{n} ≤ μ₀ ≤ \Xb
  + z_{α/2} σ / \sqrt{n}\}$
\item then $C(x) = \{μ :\Xb - z_{α/2} σ / \sqrt{n} ≤ μ ≤ \Xb +
  z_{α/2} σ / \sqrt{n}\}$
\begin{itemize}
\item we're changing two things
\begin{description}
\item[.] the set $A(μ₀)$ has samples as its elements
\item[.] the set $C(x)$ has parameter values as its elements
\item[.] $A(μ₀)$ is different for different parameter values
\item[.] $C(x)$ is different for different samples
\end{description}
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}

\section{Pivoting the CDF}

\begin{itemize}
\item works for one parameter, continuous rv.
\item suppose you have a statistic $\θh$ and know that its distribution
      function, $F(t; θ)$, is a monotone function of $θ$
\begin{itemize}
\item e.g. if $\θh$ is normal with mean $θ$ and variance one, then
  $F(t; θ) = Φ(t - θ)$ which is decreasing in $θ$.
\end{itemize}
\item we can define a $1-α$ confidence interval of the form
  $[θ_L(\θh), θ_U(\θh)]$ by solving the following equations for $θ_L, θ_U$
\begin{description}
\item[decreasing in $θ$] \[F(\θh; θ_U) = α/2\] and \[F(\θh; θ_L) = 1 - α/2\]
\item[increasing in $θ$] \[F(\θh; θ_L) = α/2\] and \[F(\θh; θ_U) = 1 - α/2\]
\end{description}
note that the L and U are switched in the two formulae.
\item You can also do this with asymptotic distributions (and that's the
     usual approach in economics for presenting asymptotic CI
\item example: Let $X₁,...,X_n$ be iid $N(θ, 1)$ and we want to
     construct a two-sided 95\% interval for $θ$.
\begin{itemize}
\item know that $\sqrt{n} (\Xb - θ) → N(0,1)$ in distribution.
\item define $\θh = \sqrt{n} \Xb$
\begin{itemize}
\item $F(t; θ) = Φ(t - θ \sqrt{n})$
\item obviously, decreasing in $θ$, so solve
\begin{itemize}
\item $Φ(\sqrt{n} \Xb - θ_U \sqrt{n}) = 0.025$
\item $Φ(\sqrt{n} \Xb - θ_L \sqrt{n}) = 0.975$
\end{itemize}
\item this becomes:
\begin{itemize}
\item $\sqrt{n} \Xb - θ_U \sqrt{n} = Φ^{-1}(0.025)$
\item $\sqrt{n} \Xb - θ_L \sqrt{n} = Φ^{-1}(0.975)$
\end{itemize}
\item which in turn becomes:
\begin{itemize}
\item $θ_U = \Xb - Φ^{-1}(0.025) / \sqrt{n} = \Xb + 1.96/\sqrt{n}$
\item $θ_L = \Xb - Φ^{-1}(0.975) / \sqrt{n} = \Xb - 1.96/\sqrt{n}$
\end{itemize}
\item which is the standard confidence interval.
\end{itemize}
\end{itemize}
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../finitesample"
%%% End:
