% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Overview of point estimation}%
\addcontentsline{toc}{part}{Overview of point estimation}

\begin{itemize}

\item We're not learning probability for its own sake; we actually
  want to talk about statistical estimators and the probability tools
  that we've learned are going to help.

\item we want to think about datasets as a particular instance of an
  unobserved random process.  Most datasets have the form (and I'll do
  a macro dataset)
  \begin{description}
  \item[time period] ...,2009:01, 2009:02, 2009:3,...
  \item[unemployment] ...,7.6, 8.1, 8.5,...
  \item[CPI] 212.174, 213.007, 212.714,...
  \end{description}
  we can think of this as a sequence of 2-vectors
  \begin{itemize}
  \item t will be the index (if we have data starting in 1970, t would
    be 1 in Jan of 1970, 2 in Feb of 1970),...,468 in 2009:01, 469
    in 2009:02, etc.
  \item $x_t$ will be the \emph{observed values} of unemployment and cpi in
    month t
    \begin{itemize}
    \item $x_{468} = (7.6, 212.174)$
    \item $x_{469} = (8.1, 213.007)$
    \item etc.
    \end{itemize}
  \end{itemize}

  So, this is the data -- $x₁,...,x_T$.

\item The Working assumptions are
  \begin{itemize}
  \item data were generated by some sort of hypothetical experiment (DGP).
  \item we want to use the data to estimate features of the DGP
  \end{itemize}

  DGP might be $X_t ∼ N(μ, σ²)$; the $x_t$ are realizations of $X_t$
  and we want to estimate $μ$ and $σ²$,

\item or $Δ p_t = g(Δ p_{t-1}, Δ p_{t-2}, ...; u_{t-1}, u_{t-2}, ...) + \ep_t$;

  \begin{itemize}
  \item $\ep_t$ is independent of $Δ p_{t-1}, Δ p_{t-2}, ...$ and $u_{t-1}, u_{t-2},...$
  \item $g$ is smooth
  \item want to estimate $\frac{∂}{∂ u_{t-1}} g$
  \end{itemize}

\item The DGP we write down doesn't need to be true, but some
  statistics will be more sensitive than others to how ``true'' the
  model is.

\end{itemize}

\section{Definition of estimators}
Let $x₁,...,x_n$ be a dataset.  A function of that dataset,
$T(x₁,...,x_n)$ is called a \emph{statistic} or \emph{estimator}.

\begin{itemize}
\item examples:
\begin{itemize}
\item $T(x₁,...,x_n) = 1/2$
\item $T(x₁,...,x_n) = n^{-1} ∑_{i=1}ⁿ x_i$
\item histogram
\end{itemize}
\item Can't depend on the true DGP
\begin{itemize}
\item $T(x₁,...,x_n) = \E X₁$ is not an estimator
\end{itemize}
\item so a statistic just takes the numbers you see, and summarizes
      them.  Anything that does that is called a statistic.
\begin{itemize}
\item even if the ``summary'' is stupid.  for example, if you
        ``summarize'' the numbers with 0, it is a statistic (as far as the
        theory is concerned).
\item The ``summary'' can also be random.  We won't get into that too
        much this class.
\end{itemize}
\item So, there are lots of things that are bad or uninformative that
      we're going to treat as statistics/estimators.
\begin{itemize}
\item But that's okay, because we haven't developed any way to say
        that an estimator is ``bad'' or ``good'' yet.
\item until we do, we can't say that summarizing a dataset with the
        mean of each of its variables is ``good'' and summarizing the
        dataset with the number 0 is ``bad''.
\item moreover, letting ``bad'' things count as estimators helps us
        define what ``good'' means.
\end{itemize}
\item we are talking about the \textbf{function}, not the value it takes on.
\begin{itemize}
\item estimators are random variables
\item the realization is an \emph{estimate}.
\end{itemize}
\end{itemize}

\section{Method of Moments}

\begin{itemize}
\item Suppose we have $X₁,...,X_n ∼ f(x; θ)$
\begin{itemize}
\item $f$ is known
\item $θ$ is an unknown $p$-vector that we want to estimate
\end{itemize}
\item estimator is based on a simple idea:
\begin{itemize}
\item if we want to estimate an expectation, use a sample average
\item $\Pr[X_i ≤ c]$ for known $c$
\begin{itemize}
\item $=\E \1\{X_i ≤ x\}$
\item estimate with $n^{-1} ∑_{i=1}ⁿ \1\{X_i ≤ c\}$
\end{itemize}
\item Estimate $\E X_i$ with $n^{-1} ∑_{i=1}ⁿ X_i$
\item etc
\end{itemize}
\item in general, relate $θ$ to the population moments:
\begin{itemize}
\item if $θ$ has $p$ elements, we calculate the first $p$ moments of
  $X_i$
\begin{itemize}
\item $μ₁ = g₁(θ)$
\item $μ₂ = g₂(θ)$
\item dot dot dot
\item $μ_p = g_p(θ)$
\end{itemize}
\end{itemize}
\item calculate the sample moments:
\begin{itemize}
\item $\μh₁ = n^{-1}∑_{i=1}ⁿ X_i$
\item $\μh₂ = n^{-1}∑_{i=1}ⁿ X²_i$
\item dot dot dot
\item $\μh_p = n^{-1}∑_{i=1}ⁿ X_i^p$
\end{itemize}
\item estimate $θ$ by setting the equations equal:
\begin{itemize}
\item $g₁(\θh) = \μh₁ = n^{-1}∑_{i=1} X_i$
\item dots
\item $g_p(\θh) = \μh_p = n^{-1}∑_{i=1} X_i^p$
\item then $\θh = g^{-1}(n^{-1}∑_{i=1}ⁿ X_i,...,n^{-1}∑ X_i^p)$
\begin{itemize}
\item obviously, $g$ needs to be invertible for this to work.
\end{itemize}
\end{itemize}
\item Sometimes $\θh$ is easy to write out analytically as a
      function of the sample averages
\begin{itemize}
\item when it's not, you can find $\θh$ numerically.
\end{itemize}
\item if each sample moment is close to the population moment and
      $g^{-1}$ is continuous, then $\θh$ should be close to $θ$.
\end{itemize}

\section{Examples}

\begin{itemize}
\item normal$(μ,σ²)$
\begin{itemize}
\item first moment of a normal random variable is $\E X_i = μ$
\item second moment is $\E X²_i = σ² + μ²$
\item method of moments estimator is
\begin{itemize}
\item $\μh = n^{-1} ∑_i X_i$
\item $\σh² = n^{-1} ∑_i X²_i - (n^{-1}∑_i X_i)²
           = n^{-1} ∑_i(X_i - \Xb)²$
\item which is not the usual estimator, but is close.
\end{itemize}
\end{itemize}
\item uniform(a,b)
\begin{itemize}
\item Let's say that $X₁,...,X_n$ are iid uniform$(a,b)$, and we want
         to calculate the method of moments estimator for $a$ and $b$.
\begin{itemize}
\item density of $X_i$ is $1/(b-a)$ in $[a,b]$, zero otherwise.
\end{itemize}
\item Calculate the first two moments:
\begin{itemize}
\item $\E X_i = ∫_a^b x /(b-a) dx = \frac{b+a}{2}$
\item $\E X²_i = ∫_a^b x² /(b-a) dx = \frac{b^3 - a^3}{3(b - a)} = \frac{b + ab + a²}{3}$
\end{itemize}
\item Gives the estimators:
\begin{itemize}
\item $\hat b+\hat a = 2 n^{-1} ∑_i X_i$
\item $\frac{\hat b^3-\hat a^3}{\hat b- \hat a} = 3 n^{-1} ∑_i X²_i$
\item solve for $\hat b$ and $\hat a$, gives
\begin{itemize}
\item $\hat b = \Xb + s \sqrt{3}$
\item $\hat a = \Xb - s \sqrt{3}$
\item $s = \sqrt{n^{-1} ∑_i (X_i - \Xb)²}$
\end{itemize}
\end{itemize}
\end{itemize}
\item linear regression
\begin{itemize}
\item setup
\begin{itemize}
\item $(Y_i, X_i) ∼$ i.i.d.
\item $X_i ∼ f$ (unspecified)
\item $Y_i ∣ X_i ∼ N(β₀ + β₁ X_i, σ²)$
\item want to estimate $β₀$ and $β₁$
\item draw scatterplot (this is like estimating the slope and intercept)
\item We'll often see this as $Y_i = β₀ + β₁ X_i + \ep_i$
\begin{itemize}
\item $\ep_i ∣ X_i ∼ N(0, σ²)$
\item i.e. define $\ep_i = Y_i - β₀ - β₁ X_i$
\end{itemize}
\end{itemize}
\item MoM estimator
\begin{itemize}
\item we know 
  \begin{align*}
    \E \binom{Y_i}{X_i Y_i}
    &=\begin{pmatrix}
        β₀ + β₁ \E X_i \\ β₀ \E X_i + β₁ \E X²_i
      \end{pmatrix} \\
    &=\begin{pmatrix}
        1 & \E X_i \\ \E X_i & \E X²_i
      \end{pmatrix}
      \begin{pmatrix} β₀ \\ β₁ \end{pmatrix}
  \end{align*}
\item Assuming invertibility,
  \begin{equation*}
     \begin{pmatrix} β₀ \\ β₁ \end{pmatrix} =
     \begin{pmatrix} 1 & \E X_i \\ \E X_i & \E X²_i \end{pmatrix}^{-1}
     \begin{pmatrix} \E Y_i \\ \E X_i Y_i \end{pmatrix}
  \end{equation*}
\item This makes our estimator,
  \begin{align*}
    \begin{pmatrix} \βh₀ \\ \βh₁ \end{pmatrix}
    &=\begin{pmatrix}
        1 & ∑_i x_i/n \\ ∑_i x_i/n & ∑_ix²_i/n
      \end{pmatrix}^{-1}
      \begin{pmatrix} ∑_iy_i/n \\ ∑_ix_iy_i/n \end{pmatrix} \\
    &=\begin{pmatrix}
        1 & ∑_i x_i \\ ∑_i x_i & ∑_ix²_i
      \end{pmatrix}^{-1}
      \begin{pmatrix} ∑_i y_i \\ ∑_i x_iy_i \end{pmatrix}
  \end{align*}
  (write in matrix notation and explain)
\end{itemize}
\end{itemize}
\end{itemize}

\section{Discussion}

\begin{itemize}
\item advantages
\begin{itemize}
\item it gets you an estimator
\item easy to derive asymptotic properties of the estimator (it's a
         function of averages, which usually obey CLTs and LLNs).
\end{itemize}
\item disadvantages
\begin{itemize}
\item can be very inefficient
\item for it to be useful, requires that the moments tell you a lot
         about the distribution (for the uniform, they don't
         necessarily -- we're trying to estimate the endpoints, but our
         estimator is to look for the center and double it.)
\begin{itemize}
\item a big problem with our estimates here, is we might have \$X\$s
           in our data that are outside the interval $[a,b]$
\end{itemize}
\item kind of ad hoc.
\item for the uniform distribution, we have \emph{all} of the moments
         defined... should we calculate the first $p$ moments and
         average all of them?
\begin{itemize}
\item how do we pick $p$?
\item are some moments better than others?
\end{itemize}
\end{itemize}
\end{itemize}

\section{GMM}

\begin{itemize}
\item Hansen (1982) shows that a modification of the method of
         moments can be very useful in macro:
\begin{itemize}
\item have periods $t=1,...,T$
\item macro models tell you that $E_t g(X_t, θ) = 0$ where
           $g(x_t, θ)$ is coming from Euler equations (ie, an
           agent is optimizing in period $t$, and $g$ captures the
           difference between what they expect to happen in period
           $t+1$ and what actually happens in period $t+1$ -- under
           rationality, they choose an action that makes that
           difference unpredictable.
\item LIE tells you that $\E g(X_t, θ) = \E \E_t g(X_t,θ)$
           which equals zero, so you have the condition
           \[\E g(X_t, θ) = 0\]
\item Hansen shows that you can often estimate $θ$ by solving
  \[ T^{-1} ∑_{t=1}^T g(X_t,θ) = 0 \]
  for $θ$. (and says how)
\item when you have more moments than parameters, gives a
           weighting scheme.
\end{itemize}
\end{itemize}

\section{Introduction to Maximum Likelihood Estimation}

\begin{itemize}
\item look at the uniform$(0,b)$ example suppose $n = 1$
\begin{itemize}
\item draw these densities for b = 1, 2, 4
\begin{itemize}
\item density is $1/b$ in $[0,b]$.
\item pick a point $x$ on the density
\end{itemize}
\item have (as a function of $b$) $1/b$ as long as $b ≥ x$
\item if we observe $X = 0.5$ (for example), we know that $b ≥ 0.5$
\item In addition, values like $b = 500$ are implausible
\begin{itemize}
\item as $b$ gets larger, there are more possible values that $X$ is
         likely to take on.
\end{itemize}
\item in a sense, $b = 0.5$ is the most plausible
\begin{itemize}
\item smaller values are impossible
\item as $b$ increases from $0.5$, the plausiblity decreases.
\end{itemize}
\item we can get $b = 0.5$ directly by maximizing $f(0.5; b)$ as a
       function of $b$
\begin{itemize}
\item called the likelihood and written $L(b; x)$
\end{itemize}
\end{itemize}
\end{itemize}

\section{Definition}

\begin{itemize}
\item for the MLE estimator, we start with the density, but view it as a
     function of $θ$
\begin{itemize}
\item the value of $θ$ that maximizes the likelihood is (in a sense) the most plausible/defensible value.
\end{itemize}
\item the MLE of $θ$ is $argmax_θ L(θ; x₁,...,x_n)$
\end{itemize}

\section{Examples}

\subsection{iid draws from uniform$(a,b)$}
\begin{itemize}
\item
  \begin{align*}
    L(a,b; x₁,...,x_n)
    &= \prod_i 1\{x_i ∈ [a,b]\} (b-a)^{-1} \\
    &= \big(\ov{b-a}\big)^{n}
  \end{align*}
  if all $x_i ∈ [a,b]$ and zero otherwise.
\item we can find the maximum easily
\begin{itemize}
\item likelihood decreases as $b$ increases or $a$ decreases
\item likelihood becomes zero if $b < \max x_i$ or if $a > \min x_i$
\item so $\hat b = \max x_i$ and $\hat a = \min x_i$
\end{itemize}
\end{itemize}

\subsection{linear regression.}
\begin{itemize}
\item $(Y_i,X_i) ∼ iid$
\begin{itemize}
\item $X_i ∼ f$ which is unspecified, $X_i$ is a $k×1$ vector.
\item $Y_i ∣ X_i ∼ N(X_i'β, σ²)$
\end{itemize}
\item want to estimate $β$ and $σ²$
\item Draw fitted values, densities
\item $L(μ,σ²; y, x) = \prod_i {1 \over \sqrt{2 π σ²}} e^{- {(y_i - x_i'β)² \over 2 σ²}} f(x_i)$
\item step 1: take logs:
\begin{itemize}
\item $\log L(μ,σ²; x₁,...,x_n) = const - n\log
         (\sqrt{σ²}) - ∑_i {(x_i - μ)² \over 2 σ²} + ∑_i f(x_i)$
\end{itemize}
\item First order conditions:
\begin{itemize}
\item for mean:
\begin{itemize}
\item $\frac{∂}{∂ β} \log L(μ, σ²; x, y) = ∑_{i=1}ⁿ x_i (y_i - x_i'β) = 0$
\item so $\βh=(∑_i x_i x_i')^{-1} ∑_i x_i y_i$
\end{itemize}
\item for variance:
\begin{itemize}
\item $\frac{∂}{∂ σ²} \log L(μ, σ²; x, y) = -\frac{n}{2σ²} + \frac{1}{2 σ^4}∑_i (y_i - x_i'β)² = 0$
\item so $\σh² = \frac{1}{n} ∑_{i=1}ⁿ (y_i - x_i'β)²$
\end{itemize}
\end{itemize}
\item students should verify that this is a maximum on their own.
\end{itemize}

\section{More remarks on mle}

\begin{itemize}
\item unlike method of moments, where we connect our parameters to
      the mean, variance, etc. regardless of the distribution; here
      we look at the features of the data that the distribution tells
      us are the most relevant.
\begin{itemize}
\item for normal, this \emph{is} the mean and variance, so MLE
        and MoM give us the same statistics
\item for uniform, and others, this is \emph{not} the mean and
        variance.     - the derivative of the log likelihood is called the \emph{score}.
        $S(θ; x) = {∂ \over ∂ θ} \log L(θ; x)$
\end{itemize}
\item has a nice invariance property: say you're not interested in the
      parameters per se, but care about a transformation of the
      parameters $T(θ)$.  If $\θh$ is the maximum
      likelihood estimator of $θ$, then $T(\θh)$ is the MLE
      of $T(θ)$.
\item we'll see later that you can use MLE to get an estimator, even if
      you don't believe that the distribution is true
\begin{itemize}
\item called quasi-maximum likelihood
\item obviously, you then need to check that the estimator works well.
\end{itemize}
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../finitesample"
%%% End: 
