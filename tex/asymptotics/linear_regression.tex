% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Linear regression}%
\addcontentsline{toc}{part}{Linear regression}

\begin{itemize}
\item we've gone back and forth on assuming normality
\item some of the properties of the OLS estimator require normality
\item others don't
\end{itemize}

\subsection{OLS properties without normality}

\begin{itemize}
\item algebraic properties from chapter 3
\item unbiasedness of the estimators
\item formula for variance of the estimators
\item gauss-markov theorem
\item unbiasedness of $s²$
\item zero correlation between errors and beta-hat
\end{itemize}

\subsection{Properties of OLS that use normality}

\begin{itemize}
\item distribution
\item independence of errors and beta-hat
\item hypothesis testing and confidence intervals
\item does this mean we can't do hypothesis testing if we don't have
       normality?
\begin{itemize}
\item of course not
\end{itemize}
\end{itemize}

\subsection{asymptotics}

\begin{itemize}
\item we could plug in any distribution for $ε$ and then use
       transformation formulas to get the density of $\βh$ given $X$.
\item each distribution for the errors would give us a different
       distribution for $\βh$.
\item \textbf{open up R}
\begin{itemize}
\item make a function 
         getbeta = function(errdist, nerrs, nbetas = 800) \{
         errs = matrix(errdist(nerrs * nbetas, nerrs, nbetas))
         apply(errs, 2, mean)
         \}
\item plot hist for different densities with nerrs = 50
\begin{itemize}
\item rt (errdist = function(n) rt(n, df = 10))
\item rexp
\item runif
\end{itemize}
\end{itemize}
\item notice that these are pretty close to each other
\item so we could do one of two things in practice
\begin{itemize}
\item try to guess what the distribution of the errors is and
         calculate the distribution of beta-hat from that
\item try to choose a distribution for beta-hat that's going to be
         close to \textbf{most} of the different dists you'll get for
         different error dist
\end{itemize}
\item big insight
\begin{itemize}
\item these procedures are the same
\item it turns out that the distribution of beta-hat \textbf{usually}
         behaves pretty similar to the one you get assuming normal
         errors, even if the errors are not normal
\end{itemize}
\item a side point, you'll never get approximate distributions
       that are good for all error dists.
\begin{itemize}
\item do example with rcauchy
\end{itemize}
\end{itemize}

\section{asymptotic behavior of beta-hat}

\begin{itemize}
\item assumptions
\begin{itemize}
\item $Y = Xβ + ε$
\item $ε$ is iid $(0, σ²)$ given $X$ and has finite $2+δ$ moments for
  some positive number $δ$
\item $x_i$ is independent across $i$
\item for each i, $x_i x_i'$ has uniformly finite variance
\item $\E x_i x_i' = Q_i$ which is uniformly finite and positive definite.
\end{itemize}
\item conclusion
\begin{itemize}
\item $\βh → β$ in probability
\item $s² → σ²$ in probability
\item $\sqrt{n}(\βh - β) → N(0, σ² Q^{-1})$ in distribution.
\end{itemize}
\end{itemize}

\subsection{discussion of assumption on X'X}

\begin{itemize}
\item Greene makes the assumption that $n^{-1} X'X → Q$ in
       probability where $Q$ is a positive definite matrix.
\item this condition is implied by the one we're using
\begin{itemize}
\item $n^{-1} X'X = n^{-1} ∑_i x_i x_i'$, so define $Q = \lim_{n → ∞}
  n^{-1} ∑_i Q_i$ and we get $n^{-1} ∑_i x_i x_i' - Q → 0$ in
  probability as a consequence of Chebychev's weak law of large
  numbers
\end{itemize}
\end{itemize}

\subsection{proof of consistency}

     We want to prove that $\βh → β$ in probablilty.  We
     can do that by plugging through the algebra
\begin{itemize}
\item going to show that $\βh - β → 0$ in probability
  \begin{align*}
    \βh - β &= (∑_i x_i x_i')^{-1} ∑_i x_i \ep_i \\
    &= (n^{-1} ∑_i x_i x_i')^{-1} n^{-1} ∑_i x_i ε_i \\
    &= \left[(n^{-1} ∑_i x_i x_i')^{-1} n^{-1}
       - Q^{-1}\right] ∑_i x_i ε_i + Q^{-1} n^{-1} ∑_i x_i ε_i \\
    &= O_p(1) n^{-1} ∑_i x_i ε_i
  \end{align*}
\item remember what $n^{-1} ∑_i x_i x_i' = O_p(1)$ means -- it
           means that for each element of the matrix (we'll call the ij
           element $(n^{-1} X'X)_{ij}$) for every positive $δ$,
           there exists a finite constant $C_δ$ and a number
           $N_δ$ such that
           \[ \Pr[|(n^{-1} X'X)_{ij} | > C_δ] < δ \] for all $n > N_δ$
\item we know from chebychev's inequality that
  \begin{align*}
    \Pr[|(n^{-1} X'X)_{ij} | > C]
    &< \E |(n^{-1} X'X)_{ij} |² / C² \\
    &≤ n^{-1} ∑_t \E |(x_{it} x_{jt}| / C²
  \end{align*}
\item this is finite because we are assuming finite means and variances.
\item so, for any $δ$, we just choose $N$ and $C$ big enough
           to ensure that the inequality holds for all $n > N$.
\item now, $\E(x_i ε_i) = \E \E(ε_i ∣ x_i) = 0$
\item also,
  \begin{align*}
    \var(x_i ε_i) &= \E(x_i x_i' ε_i²) \\
    &= \E \E(x_i x_i' ε_i² ∣ x_i) \\
    &= σ² \E(x_i x_i') \\
    &= σ² Q_i 
  \end{align*}
\item so $n^{-1} ∑_i x_i ε_i$ satisfies Chebychev's LLN
         and converges to its mean, zero.
\item $\βh - β = O_p(1) o_p(1) = o_p(1)$ and converges to
       zero in probability.
\end{itemize}

\subsection{consistency of $s²$}

     we want to show that $s² → σ²$ in probability
\begin{itemize}
\item
  \begin{align*}
    s² &= \ov{n-k-1} ∑_i (y_i - x_i'\βh)² \\
    &= \ov{n-k-1} ∑_i (ε_i - x_i'(\βh-β))² \\
    &= \ov{n-k-1} ∑_i (ε_i² - 2 ε_i x_i'(\βh-β) + (x_i'(\βh-β))²) \\
    &= \ov{n-k-1} ∑_i ε_i²
       - 2 (\βh - β)' \ov{n-k-1} ∑_i  x_i ε_i
       + (\βh - β)' \ov{n-k-1} ∑_i x_i x_i' (\βh - β) \\
    &= \ov{n-k-1} n^{-1} ∑_i ε_i²
       + o_p(1) o_p(1) + o_p(1)'O_p(1)o_p(1) \\
    &= n^{-1} ∑_i ε_i² + o_p(1)
  \end{align*}
\item finally, $n^{-1} ∑_i ε_i²$ converges in probability to $σ²$ by
  Markov's lln.
\end{itemize}

\subsection{asymptotic normality of $\βh$}

     We'll do a similar argument for asymptotic normality
\begin{itemize}
\item
  \begin{align*}
    \sqrt{n}(\βh - β)
    &= (n^{-1} ∑_i x_i x_i')^{-1} n^{-1/2} ∑_i x_i ε_i \\
    &= ((n^{-1} ∑_i x_i x_i')^{-1} - Q^{-1}) n^{-1/2} ∑_i
       x_i ε_i + Q^{-1} n^{-1/2} ∑_i x_i ε_i
  \end{align*}
\item so, we're going to show two things
\begin{itemize}
\item $n^{-1/2} ∑_i x_i ε_i$ is asymptotically normal
\item $((n^{-1} ∑_i x_i x_i')^{-1} - Q^{-1}) → 0$ in
         probability (which we discussed earlier)
\end{itemize}
\item for normality
\begin{itemize}
\item we already showed that each $x_i ε_i$ has mean zero and variance
  $σ² Q_i$
\item we can appy the lindberg-feller CLT to show $n^{-1/2} ∑_i x_i
  ε_i$ converges in distribution to $N(0, Q)$
\end{itemize}
\item consequently, $\sqrt{n}(\βh - β) = o_p(1) + Q^{-1} ∑_i x_i ε_i$
  and converges in distribution to a normal with mean zero and
  variance $σ² Q^{-1}$ which is just what we wanted to show.
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../asymptotics"
%%% End: 
