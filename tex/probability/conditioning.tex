% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Conditioning}%
\addcontentsline{toc}{part}{Conditioning}

\section{introduction and motivation}

\begin{itemize}
\item Draw picture of joint (non-independent) density vs. independent
      density (call the variables X and Y) \textbf{leave up}
\item illustrate integrating the density along one axis
\item note: we also might want to know, if we observed the hypothetical
      value of $X$, $x₀$, what would the density of $Y$ look like.
\begin{itemize}
\item slice this out in the drawing
\item formally, want to use $f(x₀, y)$ as a function of $y$.
\end{itemize}
\item can we interpret this as a density?
\begin{itemize}
\item no -- doesn't integrate to 1.
\item so if we define a distribution function, doesn't satisfy
        limiting conditions.
\end{itemize}
\item solution:
\begin{itemize}
\item rescale so that the area is 1:
  \[ \frac{f(x₀, y)}{∫_{-∞}^∞ f(x₀, z) dz} = \frac{f(x₀, y)}{f_X(x₀)}\]
\end{itemize}
\end{itemize}

\section{definition}

    Let $X$ and $Y$ be random vectors.  The joint density of $Y$
    given that $X = x$ is given by
    \[ f_Y(y ∣ X = x) = \frac{f_{X,Y}(x, y)}{f_X(x)} \]
    if $f_X(x) ≠ 0$.  (if this last quantity equals zero, then
    conditioning on that particular value doesn't make sense.  note
    that we're allowing the probability of $X=x$ to be zero).

\section{properties}

\begin{itemize}
\item independence: if $X$ and $Y$ are independent then 
  $f_Y(y ∣ X = x) = f_Y(y)$
\begin{itemize}
\item makes sense from mathematical definition
\item makes sense from the picture
\end{itemize}
\item joint density: $f_{X,Y}(x,y) = f_Y(y ∣ X = x) f_X(x)$
\begin{itemize}
\item this gives a convenient way to build up joint densities
        (especially in time series or in computational Bayesian
        estimation).
\end{itemize}
\item can define conditional distribution as
      \[ F_Y(y ∣ X = x) = ∫_{-∞}^y f_Y(z ∣ X = x) dz \]
\end{itemize}

\section{conditional expectation}

\begin{itemize}
\item We talked earlier about getting the conditional density and
      distribution given hypothetical values of another random variable.
      We can also derive moments by integrating over these conditional
      densities.
\begin{itemize}
\item Let $X$ and $Y$ be two random vectors
\begin{itemize}
\item draw picture with joint density
\end{itemize}
\item $\E(Y ∣ X = x) = ∫ y f_Y(y ∣ X = x) dy$
\item there are other, more fundamental ways to derive conditional
         expectations that we're not going to worry about.
\item write $\E(Y ∣ X)$ when we don't want to focus on a specific
         value.
\end{itemize}
\item Also give information set interpretation
\end{itemize}

\subsection{properties}

     For all of these, suppose that $X$ and $Y$ are two random variables.
\begin{itemize}
\item Conditional expectation is a function of $x$.
\item $\E(Y ∣ X)$ is a function of $X$.
\item If $Y = g(X)$, then $\E(Y ∣ X) = Y$
\begin{itemize}
\item conditional density of $Y$ given $X$ is degenerate:
  \begin{equation*}
    \Pr[Y = c ∣ X = x] =
    \begin{cases}
      1 & c = g(x) \\
      0 & \text{otherwise}
    \end{cases}
  \end{equation*}
\item So $\E(Y ∣ X = x) = g(x) × 1$
\end{itemize}
\item If $X = (X₁, X₂)$ then we have the relationships
\begin{itemize}
\item $\E(\E(Y ∣ X) ∣ X₁) = \E(Y ∣ X₁)$ a.s.
\item $\E(\E(Y ∣ X₁) ∣ X) = \E(Y ∣ X₁)$ a.s.
\item interpret these as ``information sets''; $X₁$ contains
          less information than $X$
\item $\E(X₁ Y ∣ X) = X₁ \E(Y ∣ X)$
\end{itemize}
\item If $Y ≥ 0$ then $\E(Y ∣ X) ≥ 0$ almost surely.
\begin{itemize}
\item need to make the a.s. distinction because this is a random variable
\end{itemize}
\end{itemize}

\subsection{Law of Iterated Expectations}

     Give some intuition and a picture.
\begin{description}
\item[result] $\E \E(Y ∣ X) = \E Y$
\item[proof]
  \begin{align*}
    \E Y &= ∫ y f_Y(y) dy \\
    &= ∫ ∫ y f_{X,Y}(x,y) dx dy \\
    &= ∫ ∫ y f_Y(y ∣ X = x) f_X(x) dx dy \\
    &= ∫ ∫ y f_Y(y ∣ X = x) dy f_X(x) dx \\
    &= ∫ \E(Y ∣ X = x) f_X(x) dx \\
    &= \E \E(Y ∣ X)
  \end{align*}
\end{description}

\subsection{Analogue to LIE}\sidenote{prove for homework}
\[\var(X) = \var(\E(X|Y)) + \E(\var(X|Y))\]
\begin{itemize}
\item students should prove on their own
\item conditional expectation has lower variance than original rv
\item conditional dist has lower variance on average than original
         dist
\end{itemize}
         
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../probability"
%%% End: 