% Copyright Â© 2013, Gray Calhoun.  Permission is granted to copy,
% distribute and/or modify this document under the terms of the GNU
% Free Documentation License, Version 1.3 or any later version
% published by the Free Software Foundation; with no Invariant
% Sections, no Front-Cover Texts, and no Back-Cover Texts.  A copy of
% the license is included in the section entitled "GNU Free
% Documentation License."

\part*{Conditioning}%
\addcontentsline{toc}{part}{Conditioning}

\section{introduction and motivation}

\begin{itemize}
\item Draw picture of joint (non-independent) density vs. independent
      density (call the variables X and Y) \textbf{leave up}
\item illustrate integrating the density along one axis
\item note: we also might want to know, if we observed the hypothetical
      value of $X$, $x_0$, what would the density of $Y$ look like.
\begin{itemize}
\item slice this out in the drawing
\item formally, want to use $f(x_0, y)$ as a function of $y$.
\end{itemize}
\item can we interpret this as a density?
\begin{itemize}
\item no -- doesn't integrate to 1.
\item so if we define a distribution function, doesn't satisfy
        limiting conditions.
\end{itemize}
\item solution:
\begin{itemize}
\item rescale so that the area is 1:
        \[ \frac{f(x_0, y)}{\int_{-\infty}^{\infty} f(x_0, z) dz} =
        \frac{f(x_0, y)}{f_X(x_0)}\]
\end{itemize}
\end{itemize}

\section{definition}

    Let $X$ and $Y$ be random vectors.  The joint density of $Y$
    given that $X = x$ is given by
    \[ f_Y(y \mid X = x) = \frac{f_{X,Y}(x, y)}{f_X(x)} \]
    if $f_X(x) \neq 0$.  (if this last quantity equals zero, then
    conditioning on that particular value doesn't make sense.  note
    that we're allowing the probability of $X=x$ to be zero).

\section{properties}

\begin{itemize}
\item independence: if $X$ and $Y$ are independent then 
      $f_Y(y \mid X = x) = f_Y(y)$
\begin{itemize}
\item makes sense from mathematical definition
\item makes sense from the picture
\end{itemize}
\item joint density: $f_{X,Y}(x,y) = f_Y(y \mid X = x) f_X(x)$
\begin{itemize}
\item this gives a convenient way to build up joint densities
        (especially in time series or in computational Bayesian
        estimation).
\end{itemize}
\item can define conditional distribution as
      \[ F_Y(y \mid X=x) = \int_{-\infty}^y f_Y(z \mid X = x) dz \]
\end{itemize}

\section{conditional expectation}

\begin{itemize}
\item We talked earlier about getting the conditional density and
      distribution given hypothetical values of another random variable.
      We can also derive moments by integrating over these conditional
      densities.
\begin{itemize}
\item Let $X$ and $Y$ be two random vectors
\begin{itemize}
\item draw picture with joint density
\end{itemize}
\item $E(Y \mid X = x) = \int y f_Y(y \mid X = x) dy$
\item there are other, more fundamental ways to derive conditional
         expectations that we're not going to worry about.
\item write $E(Y \mid X)$ when we don't want to focus on a specific
         value.
\end{itemize}
\item Also give information set interpretation
\end{itemize}

\subsection{properties}

     For all of these, suppose that $X$ and $Y$ are two random variables.
\begin{itemize}
\item Conditional expectation is a function of $x$.
\item $E(Y \mid X)$ is a function of $X$.
\item If $Y = g(X)$, then $E(Y \mid X) = Y$
\begin{itemize}
\item conditional density of $Y$ given $X$ is degenerate:
\item $P[Y = c \mid X = x] =$
\begin{itemize}
\item 1 if $c = g(x)$
\item 0 otherwise
\end{itemize}
\item So $E(Y \mid X = x) = g(x) \times 1$
\end{itemize}
\item If $X = (X_1, X_2)$ then we have the relationships
\begin{itemize}
\item $E(E(Y \mid X) \mid X_1) = E(Y \mid X_1)$ a.s.
\item $E(E(Y \mid X_1) \mid X) = E(Y \mid X_1)$ a.s.
\item interpret these as ``information sets''; $X_1$ contains
          less information than $X$
\item $E(X_1 Y \mid X) = X_1 E(Y \mid X)$
\end{itemize}
\item If $Y \geq 0$ then $E(Y \mid X) \geq 0$ almost surely.
\begin{itemize}
\item need to make the a.s. distinction because this is a random variable
\end{itemize}
\end{itemize}

\subsection{Law of Iterated Expectations}

     Give some intuition and a picture.
\begin{description}
\item[result] $E E(Y \mid X) = E Y$
\item[proof] $E Y = \int y f_Y(y) dy$
\begin{itemize}
\item $= \int \int y f_{X,Y}(x,y) dx dy$
\item $= \int \int y f_Y(y \mid X = x) f_X(x) dx dy$
\item $= \int \int y f_Y(y \mid X = x) dy f_X(x) dx$
\item $= \int E(Y \mid X = x) f_X(x) dx$
\item $= E E(Y \mid X)$
\end{itemize}
\end{description}

\subsection{Analogue to LIE}\sidenote{prove for homework}

     \[var(X) = var(E(X|Y)) + E(var(X|Y))\]
\begin{itemize}
\item students should prove on their own
\item conditional expectation has lower variance than original rv
\item conditional dist has lower variance on average than original
         dist
\end{itemize}
         
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../probability"
%%% End: 