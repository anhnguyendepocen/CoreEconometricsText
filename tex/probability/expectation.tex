% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Univariate Moments}%
\addcontentsline{toc}{part}{Univariate moments}

\section{basic expectation}

\subsection{introduction and motivation}

\begin{itemize}
\item one view of the steps we've taken
\begin{itemize}
\item We've proposed random variables as a way to simplify experiments
         in a way that doesn't leave out any ``essential'' details.
\item density functions let us simplify further
\item continuing this goal, we're going to see how we can simplify even further.
\end{itemize}
\item ways to simplify
\begin{itemize}
\item what single number is a random draw from the given distribution
         going to be ``closest'' to ``on average''
\begin{itemize}
\item obviously, this depends on what we mean by ``closest''
\item different meanings will give us different summary values
\end{itemize}
\item what is the most likely value that an r.v. will take on
\end{itemize}
\end{itemize}

\subsection{definition}

\begin{itemize}
\item The mean (or expected value) of a random variable, $X$, denoted
  $\E X$, is defined for continuous r.v. by
  \[ \E X = ∫_{-∞}^{∞} x f_X(x) dx \]
\item for discrete rv, \[ \E X = ∑_x x f_X(x) dx \]
\item for random vectors, $\E Y = (\E Y₁, ..., \E Y_k)$
\end{itemize}

\subsection{interpretation}

\begin{itemize}
\item population analogue to sample average (and limit of sample
       average usually)
\item point where the densities would balance
\item ``closest'' to a random draw on average, in the sense that
  $\E(X - \E X)² ≤ \E(X - c)²$ for any point c.
\end{itemize}

\subsection{other points}

\begin{itemize}
\item if $\E|X| = ∞$, we say that the mean does not exist.
\begin{itemize}
\item happens if the density has ``fat tails''
\end{itemize}
\item the mean is sometimes written as $\E X = ∫_{-∞}^{∞} x dF(x)$
  since $dF/dx = f$
\item basic linearity: $\E(a X + b) = a \E X + b$ for constants $a$
       and $b$
\item The expectation of $Y = g(X)$ is given by
  \[ \E g(X) = ∫_{-∞}^∞ g(x) f_X(x) dx \] which is a much
       easier way to go than to calculate the density
       of the transformed rv.
\begin{itemize}
\item holds for random vectors too
\end{itemize}
\item as a consequence, we have the result: If $X$ and $Y$ are
  independent then $\E XY = \E X \E Y$
  \begin{align*}
    \E XY &= ∫∫ x y f_{X,Y}(x,y) dx dy \\
    &= ∫∫ xy f_X(x) f_Y(y) dx dy \\
    &= ∫ x f_X(x) dx ∫ y f_Y(y) dy \\
    &= \E X \E Y
  \end{align*}
\end{itemize}

\subsection{Jensen's inequality}

\begin{itemize}
\item a function $g(x)$ is convex if $g(λx + (1-λ)y) ≤ λg(x) + (1-λ)g(y)$
  for $λ ∈ [0,1]$
\item For any random variable $X$, $\E g(X) ≥ g(\E X)$ for any
       convex $g$.
\end{itemize}

\section{other measures of location}

\begin{itemize}
\item the mean is (by far) the most common way to summarize a random variable.
\item there are others
\end{itemize}

\subsection{median}

\begin{itemize}
\item Def: the median of a distribution F is the value c such that
  $\Pr(X ≤ c) ≥ 1/2$ and $\Pr(X ≥ c) ≥ 1/2$.
\item the median is not necessarily unique
\item if X is continuous, the median satisfies 
  \[ \E | X - c | ≥ \E | X - \median(X) | \]
       for any other constant $c$; so this is another way of chosing
       ``the closest value to the average draw'' for a different
       interpretation of close.
\begin{itemize}
\item the mean weights values further from the center more than
         the median
\end{itemize}
\item you can define the conditional median same as conditional expectation
\end{itemize}

\subsection{mode}

\begin{itemize}
\item Def: the mode of a distribution is the value $c$ that maximizes
       the density function $f_X$
\item the mode is not necessarily unique
\item conditional mode exisits too
\end{itemize}
     measures of location are unhelpful without measures of uncertainty

\section{second moments (variance)}

\begin{itemize}
\item measures ``how far'' an random draw is from its expected value
\item definition: $\var(X) = \E(X - \E X)²$
\begin{itemize}
\item equivalently, $\var(X) = \E(X²) - (\E X)²$
\item obviously this is nonnegative
\end{itemize}
\item standard deviation is the square root of the variance
\begin{itemize}
\item this is in the same units as $X$
\end{itemize}
\end{itemize}

\section{location scale transformations}

\begin{itemize}
\item Useful special case of the transformation formula:
\item Let $X$ be a random variable with density $f_X$ and let $f$ be
  the density of $σ X + μ$ for some constant $μ$ and positive constant
  $σ$.  Then \[f(x) = \frac{1}{σ} f_X\left(\frac{x - μ}{σ}\right)\]
  for all $x$.
\begin{itemize}
\item This family of pdfs is called the ``location-scale family with
  standard pdf $f(x)$.''
\item $μ$ is the \emph{location parameter}
\item $σ$ is the \emph{scale parameter}
\end{itemize}
\item The family $\{f_X(x - μ) : μ ∈ \RR\}$ is a ``location family''
  (i.e. only $μ$ varies)
\item The family $\{(1/σ) f_X(x/σ) : σ > 0\}$ is a ``scale family''
  (i.e. only $σ$ varies)
\item $1/σ f_X(\frac{x - μ}{σ})$ is a pdf for any density $f_X$ and
  constants $μ$ and $σ > 0$.
\end{itemize}

\section{other moments}

\begin{itemize}
\item If $X$ is a random variable, we say that the kth moment of $X$
  exists if $\E|X|^k$ is finite
\item skewness: (draw picture)
\begin{description}
\item[Greene's definition] $\E(X - \E X)³$
\item[more common] $\E(X - \E X)³ / σ³$
\end{description}
\item kurtosis: (draw picture; move mass from center to tails)
\begin{description}
\item[Greene's definition] $\E(X - \E X)⁴$
\item[more common] $\E(X - \E X)⁴ / σ⁴$
\item[excess] $\E(X - \E X)⁴ / σ⁴ - 3$
\end{description}
\item in general, we might look at powers like these
\begin{itemize}
\item kth moment: $\E X^k$
\item kth central moment: $\E(X - EX)^k$
\end{itemize}
\item Consequence of Jensen's inequality: if the \$p\$th moment is
      finite, so are all moments between 0 and $p$.
\begin{itemize}
\item Proof: let $q ∈ (0, p)$.
  \[\E |X|^p = \E[( |X|^q)^{p/q}] ≤ [\E(|X|^q)]^{p/q}\]
  since $p/q > 1$.  Since the last quantity is finite, so is the first.
\end{itemize}
\end{itemize}

\section{Multivariate moments}
\subsection{Expectation for a random vector}

\begin{itemize}
\item $\E X = (\E X₁,...,\E X_k)$
\item $\E g(X₁,...,X_k) = ∫ g(x₁,...,x_k) f_X(x₁,...,x_k) dx₁ ⋯ dx_k$
\item If $X$ and $Y$ are independent with finite expectations,
  $\E XY = \E X \E Y$ (proof automatic).
\end{itemize}

\subsection{second moments}

\paragraph{Variance}
\begin{itemize}
\item for vectors, $\var(X) = \E(X - \E X) (X - \E X)'$
\begin{itemize}
\item this is positive semi definite
\begin{itemize}
\item for any vector $α$, $α'\var(X)α ≥ 0$
\item proof: 
  \begin{align*}
    α'\var(X)α &= α' \E((X - \E X)(X - \E X)') α \\
    &= \E α'(X - \E X)(X - \E X)'α \\
    &= \E (α - \E(α'X))² \\
    &= \var(α'X) ≥ 0
  \end{align*}
\end{itemize}
\item Let $X$ be a random $k$-vector with vcv $Σ$ and let $A$ be a
  deterministic $j × k$ matrix.  Then $A X$ has vcv $A Σ A'$.
\end{itemize}
\end{itemize}

\paragraph{Covariance}
\begin{itemize}
\item $\cov(X,Y) = \E(X - \E X) (Y - \E Y) = \E XY - \E X \E Y$
\begin{itemize}
\item obviously, $\cov(X,X) = \var(X)$
\end{itemize}
\item diagonal elements of vcv matrix of $X$ are the variances of individual elements of $X$
\item we can see that the off-diagonal elements are the covariances
\item for vectors $X$ and $Y$, $\cov(X,Y) = \E(X - \E X)(Y - \E Y)'$
\item $\var(X)$ is positive semi definite
\begin{itemize}
\item i.e. for any nonzero vector $α$, $α'\var(X)α ≥ 0$
\item proof:
  \begin{align*}
   α'\var(X) α
   &= α' \E((X - \E X)(X - \E X)') α \\
   &= \E(α'(X - \E X)(X - \E X)' α) \\
   &= \E((α'(X - \E X))²) \\
   &≥ 0
  \end{align*}
\end{itemize}
\end{itemize}

\paragraph{Correlation}
\begin{itemize}
\item Definition: $\corr(X, Y) = \cov(X,Y) / \sqrt{\var(X)} \sqrt{\var(Y)}$
\item this is between -1 and 1, which will be an implication of the Cauchy-Schwarz inequality
\end{itemize}

\paragraph{Cauchy-schwarz inequality}
\begin{itemize}
\item Let $X$ and $Y$ be random variables.  Then $\E (|X Y|) ≤
  \sqrt{\E X²} \sqrt{\E Y²}$
\item we're going to show that this is equivalent to something like $0
  ≤ something²$
\item proof:
\begin{itemize}
\item we want to show $(\E |XY|)² ≤ \E X² \E Y²$
\item equivalent to $0 ≤ \E X² - (\E |X Y|)² / \E Y²$ (as long
            as $Y$ is not identically zero)
  \begin{align*}
    \E X² - (\E |X Y|)² / \E Y²
    &= \E X² - 2 (\E |XY|)²/ \E Y² + (\E |XY|)²/ \E Y² \\
    &= \E X² - 2(\E |XY| / \E Y²) \E |XY| + (\E |XY|/\E Y²)² \E Y² \\
    &= \E(X² - 2(\E |XY| / \E Y²) |X| |Y| + (\E |XY|/\E Y²)² Y²) \\
    &= \E(|X| - (\E |XY|/\E Y²) |Y|)²
  \end{align*}
\end{itemize}
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../probability"
%%% End: 