% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Expectation}%
\addcontentsline{toc}{part}{Expectation}

\section{Introduction to the expectation}

\begin{itemize}
\item There are a few ways to think about the \emph{expectation} of a
  random variable.  One of the more natural is to think about many
  independent repeated draws, $X₁, X₂,…,X_n$, from the same density
  function $f$—think of them as repeated measurements of some physical
  quantity, like a weight or a height.  Under some often plausible
  assumptions (that will be discussed later) these measurements
  will cluster around a particular point $μ$ in the following sense:
  as the number of observations grows ($n → ∞$), the average of the
  observations will settle down and converge to $μ$.  If the
  measurements are unbiased, then $μ$ will be the true quantity being
  measured; if they are unbiased, $μ$ will reflect both the true
  quantity and the systematic measurement error.  In either case, $μ$
  is the \emph{expected value} of the density $f$.

  Formally, write the expected value of the density $f$ as $\E X$
  where $X ∼ f$ is a representative draw from the distribution, and
  define the expected value as
  \begin{equation}\label{eq:3}
    \E X = ∫ x f(x) dx.
  \end{equation}
  For a random vector $X = (X₁,…,X_k)$, the expected value is just the
  vector of each element's expected value:
  $\E X = (\E X₁,… \E X_k)'$

  Now, the expected value does not necessarily exist for a given
  random variable: if the density's tails are too fat, the integral
  in~\eqref{eq:3} can be infinite.  Generally, if $\E |X| = ∞$ then
  the mean of $X$ is said to not exist.

\item Note that the expectation of an indicator function is just the
  probability of the event represented by that indicator function:
  \begin{equation*}
    \E \1\{ X ∈ A \} = \Pr[X ∈ A]
  \end{equation*}

  Also notice a fundamental and important property of the expectation
  operator, linearity.  For any constants $a$ and $b$, we have
  \begin{equation*}
    \E(a X + b) = a \E X + b
  \end{equation*}

\item The expected value of a function of $X$, $\E g(X)$, is
  surprisingly straightforward and does not require the density of
  $g(X)$ to be calculated:
  \begin{equation*}
    \E g(X) = ∫ g(x) f(x) dx.
  \end{equation*}
  This can be verified in special cases from the formula for the
  density of $g(X)$.  For the general case, this result follows from
  taking the limit of the expectation of step functions (it can be
  shown that the expectation operator is closed under certain limits,
  but we won't cover those properties here).

\item It is straightforward to see that if $X$ and $Y$ are
  independent then $\E XY = \E X \E Y$.  For proof:
  \begin{align*}
    \E XY
    &= ∫∫ x y f_{X,Y}(x,y) dx dy \\
    &= ∫∫ xy f_X(x) f_Y(y) dx dy \\
    &= ∫ x f_X(x) dx ∫ y f_Y(y) dy \\
    &= \E X \E Y.
  \end{align*}

\item An especially useful inequality based on the expectation is
  \emph{Jensen's inequality}.
  \begin{thm}
    For any random variable $X$, $\E g(X) ≥ g(\E X)$ for any convex
    $g$.
  \end{thm}

  Remember the definition of a convex function:
  \begin{defn}
    A function $g(x)$ is convex if 
    \begin{equation*}
      g( λ x + (1-λ) y ) ≤ λ g(x) + (1-λ) g(y) 
    \end{equation*}
    for all $λ ∈ [0,1]$ and any $x$ and $y$.
  \end{defn}

  [Add a picture.]

\item We can also (obviously) take the expectation of conditional
  densities.  If $X$ and $Y$ are two random vectors, the conditional
  expectation of $Y$ given $X$ is
  \begin{equation*}
    \E(Y ∣ x) = ∫ f_Y(y ∣ x) dy
  \end{equation*}
  where $f_Y$ is the density of $Y$.\sidenote{There are other, more
  fundamental ways to derive conditional expectations that we're not
  going to worry about.}  When we don't want to focus on a specific
  value of $X$, we just write $\E(Y ∣ X)$ (note that this is now a
  function of the random variable $X$).

\item The conditional expectation has some important properties:
  \begin{enumerate}
  \item If $Y = g(X)$ for some function $g$ then $\E(Y ∣ X) = Y$.
    This occurs because the conditional density of $Y$ given $X$ is
    degenerate.
  \item Similarly, If $X = (X₁, X₂)$ then we have the relationships
    \begin{equation*}
      \E(\E(Y ∣ X) ∣ X₁) = \E(Y ∣ X₁) \quad\text{a.s.}
    \end{equation*}
    and
    \begin{equation*}
      \E(\E(Y ∣ X₁) ∣ X) = \E(Y ∣ X₁) \quad\text{a.s.}
    \end{equation*}
    Here it's important to remember the connection between random
    variables and information sets.  The r.v. $X₁$ contains less
    information than $X$, so when we first condition on $X₁$ to get
    $\E(Y ∣ X₁)$, that step removes (integrates out) the information
    in $X₂$.  A second conditioning step can not restore that
    information.
  \item $\E(X₁ Y ∣ X) = X₁ \E(Y ∣ X)$; again, given the information in
    $X$, we can view $X₁$ as a known constant.  The result then
    follows from linearity of the expectation.
  \item The Law of Iterated Expectations
    \begin{thm}
      Suppose that $X$ and $Y$ are random variables and $Y$ has finite
      mean; then $\E \E(Y ∣ X) = \E Y$.
    \end{thm}
    The proof (assuming densities, etc) is pretty straightforward
    \begin{align*}
      \E Y &= ∫ y f_Y(y) dy \\
      &= ∫ ∫ y f_{X,Y}(x,y) dx dy \\
      &= ∫ ∫ y f_Y(y ∣ X = x) f_X(x) dx dy \\
      &= ∫ ∫ y f_Y(y ∣ X = x) dy f_X(x) dx \\
      &= ∫ \E(Y ∣ X = x) f_X(x) dx \\
      &= \E \E(Y ∣ X)
    \end{align*}
  \end{enumerate}

\end{itemize}

\section{other measures of location}

\begin{itemize}
\item the mean is (by far) the most common way to summarize a random variable.
\item there are others
\end{itemize}

\subsection{median}

\begin{itemize}
\item Def: the median of a distribution F is the value c such that
  $\Pr(X ≤ c) ≥ 1/2$ and $\Pr(X ≥ c) ≥ 1/2$.
\item the median is not necessarily unique
\item if X is continuous, the median satisfies 
  \[ \E | X - c | ≥ \E | X - \median(X) | \]
       for any other constant $c$; so this is another way of choosing
       ``the closest value to the average draw'' for a different
       interpretation of close.
\begin{itemize}
\item the mean weights values further from the center more than
         the median
\end{itemize}
\item you can define the conditional median same as conditional expectation
\end{itemize}

\subsection{mode}

\begin{itemize}
\item Def: the mode of a distribution is the value $c$ that maximizes
       the density function $f_X$
\item the mode is not necessarily unique
\item conditional mode exists too
\end{itemize}
     measures of location are unhelpful without measures of uncertainty

\section{second moments (variance)}

\begin{itemize}
\item measures ``how far'' an random draw is from its expected value
\item definition: $\var(X) = \E(X - \E X)²$
\begin{itemize}
\item equivalently, $\var(X) = \E(X²) - (\E X)²$
\item obviously this is nonnegative
\end{itemize}
\item standard deviation is the square root of the variance
\begin{itemize}
\item this is in the same units as $X$
\end{itemize}
\end{itemize}

\section{location scale transformations}

\begin{itemize}
\item Useful special case of the transformation formula:
\item Let $X$ be a random variable with density $f_X$ and let $f$ be
  the density of $σ X + μ$ for some constant $μ$ and positive constant
  $σ$.  Then \[f(x) = \frac{1}{σ} f_X\left(\frac{x - μ}{σ}\right)\]
  for all $x$.
\begin{itemize}
\item This family of pdfs is called the ``location-scale family with
  standard pdf $f(x)$.''
\item $μ$ is the \emph{location parameter}
\item $σ$ is the \emph{scale parameter}
\end{itemize}
\item The family $\{f_X(x - μ) : μ ∈ \RR\}$ is a ``location family''
  (i.e. only $μ$ varies)
\item The family $\{(1/σ) f_X(x/σ) : σ > 0\}$ is a ``scale family''
  (i.e. only $σ$ varies)
\item $1/σ f_X(\frac{x - μ}{σ})$ is a pdf for any density $f_X$ and
  constants $μ$ and $σ > 0$.
\end{itemize}

\section{other moments}

\begin{itemize}
\item If $X$ is a random variable, we say that the kth moment of $X$
  exists if $\E|X|^k$ is finite
\item skewness: (draw picture)
\begin{description}
\item[Greene's definition] $\E(X - \E X)³$
\item[more common] $\E(X - \E X)³ / σ³$
\end{description}
\item kurtosis: (draw picture; move mass from center to tails)
\begin{description}
\item[Greene's definition] $\E(X - \E X)⁴$
\item[more common] $\E(X - \E X)⁴ / σ⁴$
\item[excess] $\E(X - \E X)⁴ / σ⁴ - 3$
\end{description}
\item in general, we might look at powers like these
\begin{itemize}
\item kth moment: $\E X^k$
\item kth central moment: $\E(X - EX)^k$
\end{itemize}
\item Consequence of Jensen's inequality: if the \$p\$th moment is
      finite, so are all moments between 0 and $p$.
\begin{itemize}
\item Proof: let $q ∈ (0, p)$.
  \[\E |X|^p = \E[( |X|^q)^{p/q}] ≤ [\E(|X|^q)]^{p/q}\]
  since $p/q > 1$.  Since the last quantity is finite, so is the first.
\end{itemize}
\end{itemize}

\section{Multivariate moments}
\subsection{Expectation for a random vector}

\begin{itemize}
\item $\E X = (\E X₁,...,\E X_k)$
\item $\E g(X₁,...,X_k) = ∫ g(x₁,...,x_k) f_X(x₁,...,x_k) dx₁ ⋯ dx_k$
\item If $X$ and $Y$ are independent with finite expectations,
  $\E XY = \E X \E Y$ (proof automatic).
\end{itemize}

\subsection{second moments}

\paragraph{Variance}
\begin{itemize}
\item for vectors, $\var(X) = \E(X - \E X) (X - \E X)'$
\begin{itemize}
\item this is positive semi definite
\begin{itemize}
\item for any vector $α$, $α'\var(X)α ≥ 0$
\item proof: 
  \begin{align*}
    α'\var(X)α &= α' \E((X - \E X)(X - \E X)') α \\
    &= \E α'(X - \E X)(X - \E X)'α \\
    &= \E (α - \E(α'X))² \\
    &= \var(α'X) ≥ 0
  \end{align*}
\end{itemize}
\item Let $X$ be a random $k$-vector with vcv $Σ$ and let $A$ be a
  deterministic $j × k$ matrix.  Then $A X$ has vcv $A Σ A'$.
\end{itemize}
\end{itemize}

\paragraph{Covariance}
\begin{itemize}
\item $\cov(X,Y) = \E(X - \E X) (Y - \E Y) = \E XY - \E X \E Y$
\begin{itemize}
\item obviously, $\cov(X,X) = \var(X)$
\end{itemize}
\item diagonal elements of vcv matrix of $X$ are the variances of individual elements of $X$
\item we can see that the off-diagonal elements are the covariances
\item for vectors $X$ and $Y$, $\cov(X,Y) = \E(X - \E X)(Y - \E Y)'$
\item $\var(X)$ is positive semi definite
\begin{itemize}
\item i.e. for any nonzero vector $α$, $α'\var(X)α ≥ 0$
\item proof:
  \begin{align*}
   α'\var(X) α
   &= α' \E((X - \E X)(X - \E X)') α \\
   &= \E(α'(X - \E X)(X - \E X)' α) \\
   &= \E((α'(X - \E X))²) \\
   &≥ 0
  \end{align*}
\end{itemize}
\end{itemize}

\paragraph{Correlation}
\begin{itemize}
\item Definition: $\corr(X, Y) = \cov(X,Y) / \sqrt{\var(X)} \sqrt{\var(Y)}$
\item this is between -1 and 1, which will be an implication of the Cauchy-Schwarz inequality
\end{itemize}

\paragraph{Cauchy-schwarz inequality}
\begin{itemize}
\item Let $X$ and $Y$ be random variables.  Then $\E (|X Y|) ≤
  \sqrt{\E X²} \sqrt{\E Y²}$
\item we're going to show that this is equivalent to something like $0
  ≤ something²$
\item proof:
\begin{itemize}
\item we want to show $(\E |XY|)² ≤ \E X² \E Y²$
\item equivalent to $0 ≤ \E X² - (\E |X Y|)² / \E Y²$ (as long
            as $Y$ is not identically zero)
  \begin{align*}
    \E X² - (\E |X Y|)² / \E Y²
    &= \E X² - 2 (\E |XY|)²/ \E Y² + (\E |XY|)²/ \E Y² \\
    &= \E X² - 2(\E |XY| / \E Y²) \E |XY| + (\E |XY|/\E Y²)² \E Y² \\
    &= \E(X² - 2(\E |XY| / \E Y²) |X| |Y| + (\E |XY|/\E Y²)² Y²) \\
    &= \E(|X| - (\E |XY|/\E Y²) |Y|)²
  \end{align*}
\end{itemize}
\end{itemize}


\subsection{Analogue to LIE}\sidenote{prove for homework}
\[\var(X) = \var(\E(X|Y)) + \E(\var(X|Y))\]
\begin{itemize}
\item students should prove on their own
\item conditional expectation has lower variance than original rv
\item conditional dist has lower variance on average than original
         dist
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../probability"
%%% End: 
%  LocalWords:  nonnegative kth skewness
