% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Properties of random variables}%
\addcontentsline{toc}{part}{Properties of random variables}

\section{Expectation}

\begin{itemize}
\item There are a few ways to think about the \emph{expectation} of a
  random variable.  One of the more natural is to think about many
  independent repeated draws, $X₁, X₂,…,X_n$, from the same density
  function $f$—think of them as repeated measurements of some physical
  quantity, like a weight or a height.  Under some often plausible
  assumptions (that will be discussed later) these measurements
  will cluster around a particular point $μ$ in the following sense:
  as the number of observations grows ($n → ∞$), the average of the
  observations will settle down and converge to $μ$.  If the
  measurements are unbiased, then $μ$ will be the true quantity being
  measured; if they are unbiased, $μ$ will reflect both the true
  quantity and the systematic measurement error.  In either case, $μ$
  is the \emph{expected value} of the density $f$.

  Formally, write the expected value of the density $f$ as $\E X$
  where $X ∼ f$ is a representative draw from the distribution, and
  define the expected value as
  \begin{equation}\label{eq:3}
    \E X = ∫ x f(x) dx.
  \end{equation}
  For a random vector $X = (X₁,…,X_k)$, the expected value is just the
  vector of each element's expected value:
  $\E X = (\E X₁,… \E X_k)'$

  Now, the expected value does not necessarily exist for a given
  random variable: if the density's tails are too fat, the integral
  in~\eqref{eq:3} can be infinite.  Generally, if $\E |X| = ∞$ then
  the mean of $X$ is said to not exist.

\item The expected value of a function of $X$, $\E g(X)$, is
  surprisingly straightforward and does not require the density of
  $g(X)$ to be calculated:
  \begin{equation*}
    \E g(X) = ∫ g(x) f(x) dx.
  \end{equation*}
  This can be verified in special cases from the formula for the
  density of $g(X)$.  For the general case, this result follows from
  taking the limit of the expectation of step functions (it can be
  shown that the expectation operator is closed under certain limits,
  but we won't cover those properties here).

\item Note that the expectation of an indicator function is just the
  probability of the event represented by that indicator function:
  \begin{equation*}
    \E \1\{ X ∈ A \} = \Pr[X ∈ A]
  \end{equation*}

  Also notice a fundamental and important property of the expectation
  operator, linearity.  For any constants $a$ and $b$, we have
  \begin{equation*}
    \E(a X + b) = a \E X + b
  \end{equation*}

\item For a random vector, we define the expecation as the vector of
  expectations of the individual elements:
  \begin{equation*}
    \E X = (\E X₁,...,\E X_k)
  \end{equation*}

\item It is straightforward to see that if $X$ and $Y$ are
  independent then $\E XY = \E X \E Y$.  For proof:
  \begin{align*}
    \E XY
    &= ∫∫ x y f_{X,Y}(x,y) dx dy \\
    &= ∫∫ xy f_X(x) f_Y(y) dx dy \\
    &= ∫ x f_X(x) dx ∫ y f_Y(y) dy \\
    &= \E X \E Y.
  \end{align*}

\item An especially useful inequality based on the expectation is
  \emph{Jensen's inequality}.
  \begin{thm}
    For any random variable $X$, $\E g(X) ≥ g(\E X)$ for any convex
    $g$.
  \end{thm}

  Remember the definition of a convex function:
  \begin{defn}
    A function $g(x)$ is convex if 
    \begin{equation*}
      g( λ x + (1-λ) y ) ≤ λ g(x) + (1-λ) g(y) 
    \end{equation*}
    for all $λ ∈ [0,1]$ and any $x$ and $y$.
  \end{defn}

  [Add a picture.]

\end{itemize}

\section{Conditional expectation}

\begin{itemize}

\item We can also (obviously) take the expectation of conditional
  densities.  If $X$ and $Y$ are two random vectors, the conditional
  expectation of $Y$ given $X$ is
  \begin{equation*}
    \E(Y ∣ x) = ∫ y f_Y(y ∣ x) dy
  \end{equation*}
  where $f_Y$ is the density of $Y$.\sidenote{There are other, more
  fundamental ways to derive conditional expectations that we're not
  going to worry about.}  When we don't want to focus on a specific
  value of $X$, we just write $\E(Y ∣ X)$ (note that this is now a
  function of the random variable $X$).

\item The conditional expectation has some important properties (these
  properties actually make a lot more sense if you view the
  conditional expectation as a projection):
  \begin{enumerate}
  \item If $Y = g(X)$ for some function $g$ then $\E(Y ∣ X) = Y$.
    This occurs because the conditional density of $Y$ given $X$ is
    degenerate.
  \item Similarly, If $X = (X₁, X₂)$ then we have the relationships
    \begin{equation*}
      \E(\E(Y ∣ X) ∣ X₁) = \E(Y ∣ X₁) \quad\text{a.s.}
    \end{equation*}
    and
    \begin{equation*}
      \E(\E(Y ∣ X₁) ∣ X) = \E(Y ∣ X₁) \quad\text{a.s.}
    \end{equation*}
    Here it's important to remember the connection between random
    variables and information sets.  The r.v. $X₁$ contains less
    information than $X$, so when we first condition on $X₁$ to get
    $\E(Y ∣ X₁)$, that step removes (integrates out) the information
    in $X₂$.  A second conditioning step can not restore that
    information.
  \item $\E(X₁ Y ∣ X) = X₁ \E(Y ∣ X)$; again, given the information in
    $X$, we can view $X₁$ as a known constant.  The result then
    follows from linearity of the expectation.
  \item The Law of Iterated Expectations
    \begin{thm}
      Suppose that $X$ and $Y$ are random variables and $Y$ has finite
      mean; then $\E \E(Y ∣ X) = \E Y$.
    \end{thm}
    The proof (assuming densities, etc) is pretty straightforward
    \begin{align*}
      \E Y &= ∫ y f_Y(y) dy \\
      &= ∫ ∫ y f_{X,Y}(x,y) dx dy \\
      &= ∫ ∫ y f_Y(y ∣ X = x) f_X(x) dx dy \\
      &= ∫ ∫ y f_Y(y ∣ X = x) dy f_X(x) dx \\
      &= ∫ \E(Y ∣ X = x) f_X(x) dx \\
      &= \E \E(Y ∣ X)
    \end{align*}
  \end{enumerate}

\end{itemize}

\section{General measures of the center of a density}

\begin{itemize}

\item The mean has lots of advantages; it is mathematically convenient
  to work with and shows up often because of the Law of Large Numbers.
  But it has disadvantages too; it is extremely sensitive to events in
  the tails of the distribution.  So sometimes other measures of
  location are useful

\item %
  \begin{defn}
    The \emph{median} of a distribution $F$ is the value $c$ such that
    $\Pr[X ≤ c] ≥ 1/2$ and $\Pr[X ≥ c] ≥ 1/2$.
  \end{defn}
  Note that this definition means that the median may not be unique if
  $X$ is discrete.

\item %
  \begin{defn}
    The \emph{mode} of a distribution is the value that maximizes
    the density function $f_X$.
  \end{defn}
  The mode is not necessarily unique either.

\end{itemize}

\section{Measures of dispersion and other moments}

\begin{itemize}

\item The variance measures a random variable's dispersion about its
  mean, and is defined as
  \begin{equation*}
    \var(X) = \E(X - \E X)².
  \end{equation*}
  An equivalent formula is $\var(X) = \E(X²) - (\E X)²$, which can be
  easier to use at times.  Obviously the variance is nonnegative (it
  will be zero if $X$ is a constant).

  The standard deviation is the square root of the variance, which has
  the advantage that it's in the same units as $X$.

\item We can generalize the variance to measure the linear
  relationship between two variables.  Define the \emph{covariance} as
  \begin{equation*}
    \cov(X,Y) = \E(X - \E X) (Y - \E Y) = \E XY - \E X \E Y
  \end{equation*}
  and the \emph{correlation} as
  \begin{equation*}
    \corr(X, Y) = \cov(X,Y) / \sqrt{\var(X)} \sqrt{\var(Y)}.
  \end{equation*}

  The Cauchy-Schwarz inequality ensures that the correlation is
  between $-1$ and 1.
  \begin{thm}[Cauchy-Schwarz inequality]
    Let $X$ and $Y$ be random variables.  Then 
    \begin{equation}\label{eq:5}
      \E (|X Y|) ≤ \sqrt{\E X²} \sqrt{\E Y²}
    \end{equation}
  \end{thm}
  To prove this result, we're going to show that it is equivalent to
  something like $0 ≤ something²$;~\eqref{eq:5} is equivalent to
  \begin{equation*}
    0 ≤ \E X² - (\E |X Y|)² / \E Y²
  \end{equation*}
  as long as $Y$ is not identically zero (if $Y$ is identically zero
  then the result holds trivially).  Then
  \begin{align*}
    \E X² - (\E |X Y|)² / \E Y²
    &= \E X² - 2 (\E |XY|)²/ \E Y² + (\E |XY|)²/ \E Y² \\
    &= \E X² - 2(\E |XY| / \E Y²) \E |XY| + (\E |XY|/\E Y²)² \E Y² \\
    &= \E(X² - 2(\E |XY| / \E Y²) |X| |Y| + (\E |XY|/\E Y²)² Y²) \\
    &= \E(|X| - (\E |XY|/\E Y²) |Y|)²
  \end{align*}
  and this last quantity is clearly nonnegative.

\item The variance obeys a relationship similar to the Law of the
  Iterated Expectation:
  \begin{equation*}
    \var(X) = \var(\E(X|Y)) + \E(\var(X|Y)).
  \end{equation*}
  There are two implications of this formula: first, that a
  conditional expectation has lower variance than its original random
  variable; and second, that conditioning decreases the variance on
  average, but not almost surely.

\item We can define the variance for vectors as well: $\var(X) = \E(X
  - \E X) (X - \E X)'$.  For vectors, $\var(X)$ is called the
  \emph{variance-covariance matrix}.  Observe that if $X$ has
  variance-covariance matrix $Σ$ and is $A$ a deterministic $j × k$
  matrix then $A X$ has variance-covariance matrix $A Σ A'$.

\item We can verify that the variance-covariance matrix is always
  positive semi-definite, i.e. that $α'\var(X)α ≥ 0$ for any nonzero
  $α$:
  \begin{align*}
    α'\var(X)α &= α' \E((X - \E X)(X - \E X)') α \\
    &= \E α'(X - \E X)(X - \E X)'α \\
    &= \E (α'X - \E(α'X))² \\
    &= \var(α'X) ≥ 0
  \end{align*}

\item In general, we can refer to the $k$th moment or central moment.
  The $k$th moment of a random variable $X$ is just $\E X^k$ and it is
  said to exist if $\E |X|^k$ is finite.  The $k$th central moment is
  defined as $\E |X - \E X|^k$.

\item As a consequence of Jensen's inequality, we have the following
  result: if the $p$th moment of a random variable is finite, so are
  all moments between 0 and $p$.

  For a proof, let $q$ be between 0 and $p$ and observe that $g(x) ≡
  x^{p/q}$ is convex.  So, for any $Y$, Jensen's inequality implies
  that $\E g(Y) ≥ g(\E Y)$.  Now, let $Y = |X|^q$, so
  \begin{equation}\label{eq:4}
    \E g(|X|^q) ≥ g(\E |X|^q).
  \end{equation}
  But $\E g(|X|^q) = \E |X|^p$ and $g(\E |X|^q) = (\E |X|^q)^{p/q}$.
  Raising both sides of Equation~\eqref{eq:4} to the $q/p$ power
  completes the proof.

\item The first four moments can be interpreted to some degree; we
  have already discussed the first two.  The random variable $X$'s
  \emph{skewness} is its normalized third moment, $\E(X - \E X)³ /
  σ³$, but you will sometimes see slightly different definitions.
  Skewness measures the asymmetry of the density function: if the
  density is perfectly symmetric, it is zero; if there is a long right
  tail, the skewness is positive, etc.

  $X$'s \emph{kurtosis} is its normalized fourth moment, $\E(X - \E
  X)⁴ / σ⁴$, which measures to some extent the thickness of the tails
  (higher kurtosis implies thicker tails).  It is sometimes useful to
  discuss the \emph{excess kurtosis}, which is $\E(X - \E X)⁴ / σ⁴ -
  3$; ``3'' is the kurtosis of the normal distribution.

\end{itemize}

\section{General transformations of random variables}

\begin{itemize}

\item Notice that, if $g$ is a Borel-measurable function from $\RR$ to
  $\RR$, then $g(X)$ is a random variable as well.  For any $B ∈ \BB$,
  we know that $g^{-1}(B) ∈ \BB$ by measurability of $g$, and so
  $X^{-1}(g^{-1}(B)) ∈ \Fs$ by measurability of $X$.

  This can be useful when $g$ is smooth and when $X$ has a known
  density function.  If we start with the distribution functions, let
  $H$ be the CDF of $g(X)$ and $F$ the CDF of $X$.  Then, if $g$ is
  invertible and monotone increasing,
  \begin{equation*}
    H(c) = \Pr[g(X) ≤ c] = \Pr[X ≤ g^{-1} c] = F(g^{-1}(c)).
  \end{equation*}
  Now, assuming all of these operations go through, we can write the
  density of $g(X)$ as
  \begin{align*}
    h(z) &= (d/dz) H(z) \\
    &= (d/dz) F(g^{-1}(z)) \\
    &= f(g^{-1}(z)) (d/dz) g^{-1}(z).
  \end{align*}

  If $g$ is monotone decreasing, you can do almost exactly the same
  operations but need to account for the different sign.  More
  generally, it might be necessary to partition the domain of $g$ into
  segments where $g$ is monotone and work with the individual pieces.

  A reasonably formal but concise statement of this result is:
  \begin{thm}
    Suppose that $Y = g(X)$, where $X$ is a random variable with density
    $f_X$ and $g$ is a continuous, monotone function with continuously
    differentiable inverse.  The density of $Y$, $f_Y$, is given by the
    equation
    \begin{equation}
      f_Y(y) = f_X(g^{-1}(y)) | (d/dy) g^{-1}(y) |
    \end{equation}
    for $y$ in the range of $g$ and zero elsewhere.
  \end{thm}

\item \emph{Location-scale families} are a useful special case of the
  transformation formula.  If $X$ is a random variable with density
  $f_X$ and $f_Y$ is the density of $Y = σ X + μ$ for some constant
  $μ$ and positive constant $σ$, then
  \begin{equation*}
    f_Y(x) = \tfrac{1}{σ} f_X\big( \tfrac{x - μ}{σ} \big)
  \end{equation*}
  for all $x$.\footnote{Verify on your own that this follows from the
  general transformation theorem.}

  This family of pdfs is called the ``location-scale family with
  standard pdf $f_X(x)$.''
  \begin{itemize}
  \item $μ$ is the \emph{location parameter}
  \item $σ$ is the \emph{scale parameter}.
  \end{itemize}
  If only $μ$ or $σ$ varies then we have either a \emph{location
  family} or a \emph{scale family}.

\item The math also becomes more painful if we are dealing with random
  vectors.  A general theory is very painful to work with because of
  dimension changes in particular, but working with one-to-one
  transformations from $\RRᵏ$ to $\RRᵏ$ is reasonably straightforward.
  First we need a definition:
  
  \begin{defn}
    Let $g: \RRᵏ → \RRᵏ$ be a differentiable function.  The Jacobian
    of this function is the matrix of partial derivatives
    \begin{equation*}
      J(x) =
      \begin{pmatrix} ∂g₁(x)/∂x₁ & ∂g₁(x)/∂x₂ & ⋯ & ∂g₁/∂x_k \\
      ⋮ \\
      ∂g_k/∂x₁ & ⋯ & ⋯ & ∂g_k/∂x_k
      \end{pmatrix}.
    \end{equation*}
  \end{defn}
      
  Then the result is (note that this is the version presented by
  \citealp[B.7.7]{Gre12})
  \begin{thm}
    Suppose that $X$ is a random vector in $\RR^k$ with joint density
    $f_X$ and that $g: \RRᵏ → \RRᵏ$ is one-to-one from the support of
    $X$ to its image and let $Y = g(X)$.  Then the joint density of $Y$ is
    \begin{equation}
      f_Y(y) = f_X(g^{-1}(y)) \abs(\det( J(y) ))
    \end{equation}
    (where $J(y)$ is the Jacobian of $g^{-1}$) in the image of the support
    of $X$ and is zero elsewhere.
  \end{thm}

\item For transformations from a higher dimension to a lower
  dimension, one can sometimes create an artificial transformation
  that's easy to work out, then integrate out the other dimension.

\end{itemize}

\section{Properties of quadratic forms}

\begin{itemize}

\item Quadratic forms will come up a lot when we study linear
  regression.  If $X$ be a random $k$-vector and let $A$ be a $k × k$
  deterministic matrix then $X'A X$ is a (random) quadratic form.
  These are (relatively) easy to study because they can be written as
  summations:
  \begin{equation*}
    X'AX = ∑_{i,j} X_i X_j A_{ij}.
  \end{equation*}

\item Usually, when we discuss quadratic forms, we assume (at least
  implicitly) that $A$ is symmetric.  This assumption is usually
  without loss of generality because
  \begin{equation*}
    X' A X = X'(A/2 + A'/2)X
  \end{equation*}
  almost surely, and $X'(A/2 + A'/2)X$ is symmetric by construction.

\item Here's a representative result for quadratic forms that
  illustrates a useful trick.
  \begin{thm}
    Let $X$ be an $n × 1$ vector of random variables, and
    let $A$ be an $n × n$ symmetric matrix.  If $\E X = μ$ and $\var(X) =
    Σ$, then
    \begin{equation}
      \E X'AX = \tr(A Σ) + μ'Aμ.
    \end{equation}
  \end{thm}

  The proof uses three concepts that show up often: a scalar equals
  its own trace, linearity of the expectation, and a property of the
  trace: $\tr(XY) = \tr(YX)$ as long as both are conformable (the last
  property is difficult to see, so write out the matrix operations to
  convince yourself it is true).
  \begin{align*}
    \E(X'AX) &= \tr(\E(X'AX)) \\
    &= \E(\tr(X'AX)) \\
    &= \E(\tr(AXX')) \\
    &= \tr(\E(AXX')) \\
    &= \tr(A \E(XX')) \\
    &= \tr(A (\var(X) + μμ')) \\
    &= \tr(AΣ) + \tr(A μμ') \\
    &= \tr(AΣ) + μ'Aμ.
  \end{align*}

\item Similar results obviously exist for other moments, but the
  algebra isn't quite as nice (and doesn't illuminate any key
  technique other than raw tenacity).  Here's an example for the
  variance under a few simplifying assumptions (this particular
  statement comes from \citealp{SL03})
  \begin{thm}
    let $X₁,...,X_n$ be independent random variables with means
    $θ₁,...,θ_n$, and the same 2nd, 3rd and 4th central moments $μ₂$,
    $μ₃$, $μ₄$.  If $A$ is an $n × n$ symmetric matrix and $a$ is a
    column vector of the diagonal elements of $A$, then
    \begin{equation*}
      \var(X'AX) = 
      (μ₄ - 3 μ₂²)a'a + 2 μ₂² \tr(A²) + 4 μ₂ θ'A² θ + 4 μ₃ θ' A a.
    \end{equation*}
  \end{thm}
  To prove the result, take
  \begin{equation*}
    X'A X = ∑_{ij} ((X_i - θ_i) + θ_i) ((X_j - θ_j) + θ_j) A_{ij},
  \end{equation*}
  multiply out and take the variance.

\end{itemize}

\section{Gaussian random variables}

\begin{itemize}

\item The \emph{Normal distribution} is particularly important when
  studying linear regression.  To the extent that we have finite
  sample results, many will hold only if the errors are Normal.
  Moreover, whenever we use the Central Limit Theorem, we get an
  approximately normal random variable.

\item If $X$ is an $N(μ, Σ)$ random vector, its density is
  \begin{equation*}
    f(x) = \frac{1}{\sqrt{2ⁿ πⁿ \det(Σ)}} exp(-1/2(x - μ)'Σ^{-1}(x- μ)).
  \end{equation*}
  The parameters $μ$ and $Σ$ can be shown to be the mean and variance
  of the r.v. and completely determine its distribution.

  [you should draw the shape of the density and some elliptical
  contour plots]

  If $X$ is a $k$-dimensional multivariate normal with mean $μ$
  and variance $Σ$, $A X + b$ is also multivariate normal for any
  constant $n × k$ matrix $A$ and $n$-vector $b$.

  If $X$ is multivariate normal with mean $μ$ and variance $Σ$,
  and $Σ^{1/2}$ is a symmetric matrix such that $Σ^{1/2} Σ^{1/2} = Σ$,
  then $(Σ^{1/2})^{-1} (X - μ)$ is multivariate standard normal.

\item Independence of normal random variables is especially easy: if
  $X$ and $Y$ are uncorrelated normal random vectors, then they are
  independent.  The result follows from simply multplying out the
  square terms in the density function and then factoring it.

  Marginal and conditional distributions are also especially easy to
  work with.  If 
  \begin{equation*}
    (X₁,X₂) ∼ N((μ₁,μ₂), Σ)
  \end{equation*}
  with
  \begin{equation*}
    Σ = \begin{pmatrix}
      Σ_{11} & Σ_{12} \\ Σ_{12}' & Σ_{22}
    \end{pmatrix}
  \end{equation*}
  then $X₁ ∼ N(μ₁, Σ_{11})$, $X₂ ∼ N(μ₂, Σ_{22})$, and
  \begin{equation*}    
    X₁ ∣ X₂ ∼ N(μ₁ + Σ_{12} Σ_{22}^{-1} (X₂ - μ₂),
               Σ_{11} - Σ_{12}'Σ_{22}^{-1} Σ_{12}).
  \end{equation*}
  Notice that the conditional mean of $X₁$ depends on $X₂$, but the
  conditional variance doesn't.

\item The fact that zero correlation implies independence also makes
  it easy to determine whether functions of normal r.v.s are
  independent.  Linear combinations and quadratic forms of normal
  r.v.s will come up often, so we'll single those results out.  For
  the next results, let $X$ be a $k$-dimensional standard normal
  random vector (in practice, we will often work with $Σ^{-1/2} X$).
  \begin{enumerate}
  \item If $A$ is a $k × j$ matrix and $B$ a $k × l$ matrix such that
    $A'B = 0$, then $A'X$ and $B'X$ are independent.
  \item If $P$ is a $k × k$ idempotent matrix and $PB = 0$ then $X'PX$
    and $B'X$ are independent (an idempotent matrix is a square and
    symmetric matrix that satisfies $P = PP$).
  \item If $P$ and $Q$ are idempotent and $PQ = 0$ then $X'PX$ and
    $X'QX$ are independent.
  \end{enumerate}
  
\item Idempotent matrices come up often, as they represent projections
  in $\RRⁿ$.  If $z$ is a point in $\RRⁿ$ and $x₁,...,x_k$ is a set of
  vectors in $\RRⁿ$, we may want to project $z$ into the space spanned
  by $x₁,...,x_k$.  This amounts to finding a linear operator $P$ so
  that $P z$ is a linear combination of the $x_i$'s and $P z$ is as
  close as possible to $z$ subject to the first constraint.

  We'll worry about making sure that $Pz$ is a linear combination of
  the $x_i$'s later—that's going to be the focus of linear regression.
  But to ensure that $P z$ is as close as possible to $z$, we just
  need to have $z - Pz$ and $Pz$ perpendicular, so $(z - Pz)' Pz = 0$.
  But this requires that $z'(I - P)'P z = 0$ for any $z$, so $(I -
  P)'P = 0$, which amounts to our condition that $P = PP$.

  An idempotent matrix, $P$, has the property that all of its
  eigenvalues must be zero or one, so $\rank(P) = \tr(P)$.

\item Two statistics that are particularly relevant to the normal
  distribution are the sample mean, $\Xb = (1/n) ∑_{i=1}^n X_i$, and
  the sample variance, $S² = (1/(n-1)) ∑_{i=1}^n (X_i - \Xb)²$.  Note
  that we can write $\Xb$ as a linear combination of the $X_i$'s:
  \begin{equation*}
    \Xb = (1/n,...,1/n) · (X₁,...,X_n)' = (1/n) ι'X,
  \end{equation*}
  where $ι$ is a vector of $n$ ones, and
  \begin{equation*}
    S² = (1/(n-1)) (X - ι \Xb)'(X - ι \Xb) = (1/(n-1)) X'(I - (1/n) ιι')X.
  \end{equation*}
  If $X ∼ N(μ, σ² I)$ then we have $\Xb ∼ N(μ, (σ²/n) I)$.  Also,
  since $(I - (1/n) ιι')$ is idempotent, we know right away that $\Xb$
  and $S²$ are independent.

\item Some derived distributions are important

\item %
  \begin{defn}
    Let $X₁,...,X_k$ be independent standard normal.  The
    chi-squared distribution with $k$ degrees of freedom is the
    distribution of $∑_{i=1}^k X²_i$.
  \end{defn}

  \begin{thm}
    Let $X ∼ N(0, I_n)$ and let $P$ be a symmetric $n × n$ matrix.
    Then $X'PX$ is chi-squared with $k$ degrees of freedom iff $P$ is
    idempotent with rank $k$.
  \end{thm}

  \begin{thm}
    if $X$ and $Y$ are independent chi-squared with $df_X$ and $df_Y$
    degrees of freedom, then $X + Y$ is chi-square with $df_X + df_Y$
    degrees of freedom.
  \end{thm}

  If $X ∼ N(μ, σ² I)$ then $S²$ is chi-square with $n-1$ degrees of
  freedom.

\item %
  \begin{defn}
    If $X$ is a standard normal and $Y$ is a chi-squared with $n$ dof
    and $X$ and $Y$ are independent, then $X / \sqrt{Y/n}$ is a $t(n)$
    random variable.
  \end{defn}
  From the quadratic form results, this is going to come up when $Z$
  is multivariate standard normal and $X = B'Z$, $Y = Z'PZ$, and $B'P
  = 0$.

\item %
  \begin{defn}
    If $X$ and $Y$ are independent chi-squared with $df_X$ and $df_Y$
    degrees of freedom, then $\frac{X/df_X}{Y/df_Y}$ is an $F(df_X,
    df_Y)$ random variable.
  \end{defn}

  Expect this to come up when $Z$ is multivariate standard normal and
  $X = Z'QZ$, $Y = Z'PZ$ and $Q$ and $P$ are idempotent and orthogonal.

\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../probability"
%%% End: 
%  LocalWords:  nonnegative kth skewness
