% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Multivariate random vectors}%
\addcontentsline{toc}{part}{Multivariate random vectors}

The borel sigma field on $\RR^k$, denoted $\BB^k$ is the smallest
sigma-field containing the closed rectangles:
\[\{[a₁,b₁] × ⋯ ×[a_k,b_k] : - ∞ ≤ a_i ≤ b_i ≤ ∞\}\]

\section{multivariate distributions}
\subsection{marginal distribution}

\begin{itemize}
\item since a random vector is just a vector of random variables, each
       element has its own distribution
\item these are called ``marginal distributions''
\end{itemize}

\subsection{joint distribution}

\begin{itemize}
\item different elements can depend on each other
\item this dependence isn't reflected in the marginal distributions
\item is reflecteded in the ``joint distribution'' of the entire vector
\end{itemize}

\paragraph{definition}
      The joint distribution, $F_X$, of the $k$-dimensional random
      vector $X$ is define as the function 
      \[ F_X(x₁,...,x_k) = P(X₁ ≤ x₁,...,X_k ≤ x_k) \]

\paragraph{relationship between joint and marginal}
\begin{itemize}
\item marginal distributions tell you less about the random vector
\item can get marginals from joint, but not vice-versa.
\begin{itemize}
\item different joint distributions can have the same marginal
          distributions.
\end{itemize}
\item formula (for 1st marginal, similar for others)
        \[ F_X(x) = F_X(x, ∞, ...,∞) \]
\begin{itemize}
\item follows immediately from the definition of both sides
  \[ \Pr[X₁ ≤ x] = \Pr[X₁ ≤ x, X₂ ≤ ∞, ..., X_k ≤ ∞] \]
\end{itemize}
\item to summarize:
\begin{itemize}
\item the joint distribution determines the marginals
\item the marginals do not determine the joint (maybe do for hw)
\end{itemize}
\end{itemize}

\section{multivariate densities}
\subsection{joint and marginal distributions and densities}

      Same as before: we want to define weights on points, not just on
      regions.

\paragraph{definitions}
\begin{itemize}
\item marginal densities are the densities of the individual elements
\item joint densities describe the simultaneous behavior of all of
        the elements
\begin{itemize}
\item pmf: The probability mass function of a discrete random vector,
  denoted $f_X(x₁,..., x_k)$, is given by
  \[f_X(x₁,...,x_k) = \Pr[X₁ = x₁,...,X_k = x_k]\]
\item pdf: the probability density function of a continuous random
  variable is the function $f_X(·)$ that satisfies
  \begin{equation*}
    F_X(c₁,...,c_k) = ∫_{-∞}^{c₁} ⋯ ∫_{-∞}^{c_k} f(y₁,...,y_k) dy₁ ... dy_k
  \end{equation*}
\end{itemize}
\end{itemize}

\paragraph{properties}
\begin{itemize}
\item usual properties hold
\begin{itemize}
\item densities are positive
\item densities integrate up to 1.
\end{itemize}
\item can get marginal density from joint
  \[ f_{X}(x) = ∫ f_X(x, y₂,...,y_k) dy₂ ... dy_k \]
\begin{itemize}
\item follows from writing distribution in terms of integral of
          the density
\end{itemize}
\end{itemize}

\subsection{transformations of random vectors}

\begin{itemize}
\item a general theory is very painful to work with (particularly
        because of dimension changes)
\item The textbook presents the result for bivariate transformations
\item The extension to higher-dimension random vectors is
        straightforward
\end{itemize}

\paragraph{Jacobian}
      Let $g: \RR^k → \RR^k$ be a differentiable function.  The
      Jacobian of this function is the matrix of partial derivatives
      \[ J(x) =
      \begin{pmatrix} ∂g₁(x)/∂x₁ & ∂g₁(x)/∂x₂ & ⋯ & ∂g₁/∂x_k \\
      ⋮ \\
      ∂g_k/∂x₁ & ⋯ & ⋯ & ∂g_k/∂x_k
      \end{pmatrix} \]

\paragraph{result \citep[B.7.7]{Gre12}}
Suppose that $X$ is a random vector in $\RR^k$ with joint density
$f_X$ and that $g: \RR^k → \RR^k$ is one-to-one from the support of
$X$ to its image and let $Y = g(X)$.  Then the joint density of $Y$ is
\[ f_Y(y) = f_X(g^{-1}(y)) \abs(\det( J(y) ))\]
(where $J(y)$ is the Jacobian of $g^{-1}$) in the image of the support
of $X$ and is zero elsewhere.
\begin{itemize}
\item the restriction that $g$ be one-to-one can be relaxed.
\end{itemize}

\paragraph{transformations to lower dimension}
      the requirement that $g$ be from $\RR^k$ to $\RR^k$ is restrictive.
      For transformations from a higher dimension to a lower dimension,
      it's best to create an artificial transformation that's easy to
      work out, then integrate out the other dimension.

\subsection{independence}

\begin{itemize}
\item remember how we defined independence for events
\begin{itemize}
\item Two events $A₁$ and $A₂$ are independent if $P(A₁∩A₂) = P(A₁)×P(A₂)$.
\end{itemize}
\item same idea for random variables, but now events are intervals in $\RR$
\begin{itemize}
\item Two random variables $X₁$ and $X₂$ are \emph{independent} if
  \[\Pr[X₁ ∈ I₁ ∩ X₂ ∈ I₂] = \Pr[X₁ ∈ I₁] \Pr[X₂ ∈ I₂]\] for any
  intervals $I₁$ and $I₂$.
\end{itemize}
\item implications:
\begin{itemize}
\item Two random variables with marginal distributions $F₁$ and
         $F₂$ and joint distribution function $F_X$ are independent iff
         $F_X(x₁, x₂) = F₁(x₁) F₂(x₂)$
\item Two random variables with marginal densities $f₁$ and $f₂$
         and joint density $f_X$ are independent iff 
         $f_X(x₁,x₂) = f₁(x₁) f₂(x₂)$
\item if $X₁$ and $X₂$ are independent, so are $g(X₁)$ and
         $h(X₂)$ for any (measurable) functions $g$ and $h$
\begin{itemize}
\item proof:
  \begin{align*}
    \Pr[g(X₁) ∈ I₁ ∩ h(X₂) ∈ I₂]
    &= \Pr[X₁ ∈ g^{-1}(X₁) ∩ X₂ ∈ h^{-1}(X₂)] \\
    &= \Pr[X₁ ∈ g^{-1}(X₁)] \Pr[X₂ ∈ h^{-1}(X₂)] \\
    &= \Pr[g(X₁) ∈ I₁] \Pr[h(X₂) ∈ I₂]
  \end{align*}
  where $g^{-1}(I₁) = \{x : g(x) ∈ I₁\}$, we don't need $g$ to be invertible.
\end{itemize}
\end{itemize}
\end{itemize}

\section{Moments}
\subsection{Expectation for a random vector}

\begin{itemize}
\item $\E X = (\E X₁,...,\E X_k)$
\item $\E g(X₁,...,X_k) = ∫ g(x₁,...,x_k) f_X(x₁,...,x_k) dx₁ ⋯ dx_k$
\item If $X$ and $Y$ are independent with finite expectations,
  $\E XY = \E X \E Y$ (proof automatic).
\end{itemize}

\subsection{second moments}

\paragraph{Variance}
\begin{itemize}
\item for vectors, $\var(X) = \E(X - \E X) (X - \E X)'$
\begin{itemize}
\item this is positive semi definite
\begin{itemize}
\item for any vector $α$, $α'\var(X)α ≥ 0$
\item proof: 
  \begin{align*}
    α'\var(X)α &= α' \E((X - \E X)(X - \E X)') α \\
    &= \E α'(X - \E X)(X - \E X)'α \\
    &= \E (α - \E(α'X))² \\
    &= \var(α'X) ≥ 0
  \end{align*}
\end{itemize}
\item Let $X$ be a random $k$-vector with vcv $Σ$ and let $A$ be a
  deterministic $j × k$ matrix.  Then $A X$ has vcv $A Σ A'$.
\end{itemize}
\end{itemize}

\paragraph{Covariance}
\begin{itemize}
\item $\cov(X,Y) = \E(X - \E X) (Y - \E Y) = \E XY - \E X \E Y$
\begin{itemize}
\item obviously, $\cov(X,X) = \var(X)$
\end{itemize}
\item diagonal elements of vcv matrix of $X$ are the variances of individual elements of $X$
\item we can see that the off-diagonal elements are the covariances
\item for vectors $X$ and $Y$, $\cov(X,Y) = \E(X - \E X)(Y - \E Y)'$
\item $\var(X)$ is positive semi definite
\begin{itemize}
\item i.e. for any nonzero vector $α$, $α'\var(X)α ≥ 0$
\item proof:
  \begin{align*}
   α'\var(X) α
   &= α' \E((X - \E X)(X - \E X)') α \\
   &= \E(α'(X - \E X)(X - \E X)' α) \\
   &= \E((α'(X - \E X))²) \\
   &≥ 0
  \end{align*}
\end{itemize}
\end{itemize}

\paragraph{Correlation}
\begin{itemize}
\item Definition: $\corr(X, Y) = \cov(X,Y) / \sqrt{\var(X)} \sqrt{\var(Y)}$
\item this is between -1 and 1, which will be an implication of the Cauchy-Schwarz inequality
\end{itemize}

\paragraph{Cauchy-schwarz inequality}
\begin{itemize}
\item Let $X$ and $Y$ be random variables.  Then $\E (|X Y|) ≤
  \sqrt{\E X²} \sqrt{\E Y²}$
\item we're going to show that this is equivalent to something like $0
  ≤ something²$
\item proof:
\begin{itemize}
\item we want to show $(\E |XY|)² ≤ \E X² \E Y²$
\item equivalent to $0 ≤ \E X² - (\E |X Y|)² / \E Y²$ (as long
            as $Y$ is not identically zero)
  \begin{align*}
    \E X² - (\E |X Y|)² / \E Y²
    &= \E X² - 2 (\E |XY|)²/ \E Y² + (\E |XY|)²/ \E Y² \\
    &= \E X² - 2(\E |XY| / \E Y²) \E |XY| + (\E |XY|/\E Y²)² \E Y² \\
    &= \E(X² - 2(\E |XY| / \E Y²) |X| |Y| + (\E |XY|/\E Y²)² Y²) \\
    &= \E(|X| - (\E |XY|/\E Y²) |Y|)²
  \end{align*}
\end{itemize}
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../probability"
%%% End: 