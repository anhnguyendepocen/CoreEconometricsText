% Copyright Â© 2013, Gray Calhoun.  Permission is granted to copy,
% distribute and/or modify this document under the terms of the GNU
% Free Documentation License, Version 1.3 or any later version
% published by the Free Software Foundation; with no Invariant
% Sections, no Front-Cover Texts, and no Back-Cover Texts.  A copy of
% the license is included in the section entitled "GNU Free
% Documentation License."

\chapter{Systems of equations}

\begin{itemize}
\item covered on
\begin{itemize}
\item \textit{2009-11-16 Mon}, \textit{2009-11-18 Wed}
\item \textit{2010-12-07 Tue}, \textit{2010-12-09 Thu}
\item \textit{2011-12-08 Thu}
\end{itemize}
\item Discussed in chapter 10.1 and 10.2 of Greene and 3.7 in Kleiber
     and Zeileis
\end{itemize}
\section{Introduction}
\label{sec-1}

\begin{itemize}
\item There are lots of panel implications to what we're discussing,
      but we won't cover them
\item Basic idea:
\begin{itemize}
\item Suppose you have two relationships you are interested in
        (return data, say)
        \[r_{jt} - r_{ft} = \alpha_j + \beta_j (r_{mt} - r_{ft}) +
        \epsilon_{it}\]
\begin{itemize}
\item $j = 1,2$
\item $t = 1,2,3,\dots$
\item $r_{it}$ is the return on asset $i$
\item $r_{ft}$ is the risk-free rate of return (ie T-bills)
\item $r_{mt}$ is the return on the entire market.
\end{itemize}
\end{itemize}
\item here's the question: should you do OLS on each equation
      individually, or do something different?
\begin{itemize}
\item we'll derive a new estimator
\item called seemingly unrelated regression
\item obviously, if the errors are exogenous you can do ols and it
        will be fine; the question here is if we can do something
        better than OLS.
\end{itemize}
\end{itemize}
\section{setup}
\label{sec-2}
\subsection{notation}
\label{sec-2-1}

\begin{itemize}
\item We want to write these two models as
       \[y_1 = X_1 \beta_1 + \epsilon_1\]
       and
       \[y_2 = X_2 \beta_2 + \epsilon_2\]
\begin{itemize}
\item each $y_i$ is $n\times 1$
\end{itemize}
\item alternatively
       \[ \binom{y_{1i}}{y_{2i}} =
       \binom{x_{1i}'\beta_1}{x_{2i}'\beta_2} + 
       \binom{\epsilon_{1i}}{ \epsilon_{2i}}\]
       for $i = 1,\dots,n$.
\end{itemize}
\subsection{assumptions}
\label{sec-2-2}

\begin{description}
\item[Exogeneity] \[E(\binom{\varepsilon_{1i}}{\varepsilon_{2i}} \mid X) = 0\] for all $i$
\item[homoskedasticity] \[E(\binom{\epsilon_{1i}}{\epsilon_{2i}} \binom{\epsilon_{1i}}{\epsilon_{2i}}' \mid X ) = \Sigma\]
\item[uncorrelated errors] \[E(\binom{\epsilon_{1i}}{\epsilon_{2i}}
          \binom{\epsilon_{1j}}{\epsilon_{2j}}' \mid X ) = 0\] if $i \neq j$.
\begin{itemize}
\item allow errors to be correlated across equations but not across
         observations
\end{itemize}
\item[usual] assume that the usual technical conditions on the
                regressors and errors hold to allow for us to apply
                laws of large numbers, etc.
\end{description}
\subsection{estimation with known covariance matrix}
\label{sec-2-3}

\begin{itemize}
\item We want to rewrite the equations as
       \[ \binom{\mathbf{y}_1}{\mathbf{y}_2} = (\begin{matrix} \mathbf{X}_1
       & 0 \\ 0 & \mathbf{X}_2  \end{matrix})
       \binom{\beta_1}{\beta_2} +
      \binom{\mathbf{\varepsilon}_1}{\mathbf{\varepsilon}_2} \]
\begin{itemize}
\item ie $\mathbf{y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$
\end{itemize}
\item Remarkable thing is that we can estimate this model by GLS
\begin{itemize}
\item Note that $E(\mathbf{\epsilon} \mid \mathbf{X}) = 0$
\item Moreover, \[E(\mathbf{\epsilon} \mathbf{\epsilon}' \mid \mathbf{X}) =
         \Sigma \otimes I = (\begin{matrix} \sigma_{11}^2 I &
         \sigma_{12} I \\ \sigma_{21} I & \sigma_{22}^2
         I \end{matrix})\]
\item So the model satisfies the requirements for GLS regression to
         be BLUE, even though the dependent variable is different for
         the two equations.
\end{itemize}
\item Assume that $\Sigma$ is known for now, 
        \[\hat\beta_{GLS} = (X'(\Sigma \otimes I)^{-1}X)^{-1}X'(\Sigma
        \otimes I)^{-1}Y\]
\begin{itemize}
\item simplify this further:
\begin{itemize}
\item $(\Sigma \otimes I)^{-1} = \Sigma^{-1} \otimes I$
\begin{itemize}
\item Let $\gamma_{ij}$ denote the $i,j$ element of $\Sigma^{-1}$
\end{itemize}
\item as a result
           \[\hat\beta_{SUR} = (\begin{matrix}
            \gamma_{11} X_1'X_1 & \gamma_{12} X_1'X_2 \\
            & \gamma_{22} X_2'X_2 \end{matrix})^{-1} 
            (\begin{matrix} \gamma_{11} X_1'y_1 + \gamma_{12} X_1'y_2
           \\ \gamma_{12} X_2'y_1 + \gamma_{22}
           X_2'y_2\end{matrix}) \]
\end{itemize}
\end{itemize}
\item variance-covariance matrix is found as usual:
        \[var(\hat\beta_{GLS} \mid X) = (X'(\Sigma \otimes I)^{-1}X)^{-1}\]
\begin{itemize}
\item if you don't see this right away, please work through the
         math on your own.  The logic is identical to OLS or GLS.
\end{itemize}
\end{itemize}
\subsection{estimation with unknown covariance}
\label{sec-2-4}

\begin{itemize}
\item Big difference from single-equation case
\begin{itemize}
\item with single equations, we were concerned about heteroskedasticity
\item here, we assume homoskedasticity and uncorrelated observations
\item allow correlation across equations.
\end{itemize}
\item We can estimate \$$\Sigma$ pretty easily:
\begin{itemize}
\item let $(\hat\epsilon_{i1}, \hat\epsilon_{i2})$ be the pair of
         residuals for observation $i$ from the OLS estimation
\begin{itemize}
\item \textbf{remember} OLS is consistent
\item so $(\hat\epsilon_{i1}, \hat\epsilon_{i2})$ is consistent for
           $(\epsilon_{i1}, \epsilon_{i2})$
\end{itemize}
\item LLN would impliy that \[ n^{-1} \sum_i (\epsilon_{i1},
         \epsilon_{i2})' (\epsilon_{i1}, \epsilon_{i2}) \to \Sigma\] in
         probability
\end{itemize}
\item Consistency of the residuals implies \[\hat\Sigma = n^{-1}
       \sum_i (\hat\epsilon_{i1},
       \hat\epsilon_{i2})'(\hat\epsilon_{i1},\hat \epsilon_{i2})\to
       \Sigma \quad i.p.\]
\item plug in to GLS formula for a feasible estimator
\end{itemize}
\subsection{Simplification of SUR}
\label{sec-2-5}

     The SUR becomes the OLS estimator in two specific and informative
     cases
\begin{itemize}
\item errors are uncorrelated across models
\item all of the same regressors appear in both models
\begin{itemize}
\item VARs in macro, for example
\end{itemize}
\end{itemize}
\paragraph{zero correlation}
\label{sec-2-5-1}

\begin{itemize}
\item formula for SUR estimator:
       \[\hat\beta_{SUR} = (X'(\Sigma \otimes I)^{-1}X)^{-1}X'(\Sigma
       \otimes I)^{-1}Y\]
\item Suppose that $\sigma_{12} = 0$, so
        \[ \Sigma \otimes I = (\begin{matrix} \sigma_{11}^2 I & 0 \\ 0
        & \sigma_{22}^2 I \end{matrix})\]
\begin{itemize}
\item First part becomes \[(X'( \Sigma \otimes I)^{-1} X)
          = (\begin{matrix} X_1'X_1 / \sigma_{11}^2 & 0 \\ 0 & X_2'X_2
          / \sigma_{22}^2 \end{matrix}) \]
\item Second part becomes 
          \[X'(\Sigma \otimes I)^{-1}Y = \binom{X_1' y_1 /
          \sigma_{11}^2}{X_2'y_2 / \sigma_{22}^2}\]
\item $\hat\beta_{SUR} =
          \binom{(X_1'X_1)^{-1}X_1'y_1}{(X_2'X_2)^{-1}X_2'y_2} =
          \binom{\hat\beta_{1,ols}}{\hat\beta_{2,ols}}$
\end{itemize}
\end{itemize}
\paragraph{Identical regressors in both models}
\label{sec-2-5-2}

\begin{itemize}
\item Let $X_1$ and $X_2$ both equal $X$
\item Use the simplified form of the SUR estimator:
        \[\hat\beta_{SUR} = (\begin{matrix} \gamma_{11} X_1'X_1 &
        \gamma_{12} X_1'X_2 \\ & \gamma_{22}
        X_2'X_2 \end{matrix})^{-1} (\begin{matrix} \gamma_{11}
        X_1'y_1 + \gamma_{12} X_1'y_2 \\ \gamma_{12} X_2'y_1 +
        \gamma_{22} X_2'y_2\end{matrix}) = \]
\item \[= (\begin{matrix} \gamma_{11} X'X & \gamma_{12} X'X \\ &
        \gamma_{22}X'X \end{matrix})^{-1} (\begin{matrix} \gamma_{11}
        X'y_1 + \gamma_{12}X'y_2 \\ \gamma_{12} X'y_1 + \gamma_{22}
        X'y_2\end{matrix})\]
\item First term becomes $(\Sigma^{-1} \otimes X'X)^{-1}$ which
        equals $\Sigma \otimes (X'X)^{-1}$
\item second term becomes \[\binom{X'X
        (\gamma_{11}\hat\beta_{1,ols} + \gamma_{12}\hat\beta_{2,ols})}{X'X
        (\gamma_{21}\hat\beta_{1,ols} +
        \gamma_{22}\hat\beta_{2,ols})}   \]
\begin{itemize}
\item Premultipy each $X'y_j$ with $X'X(X'X)^{-1}$
\end{itemize}
\item Multiply through and everything cancels, giving
        \[\hat\beta_{SUR} = \binom{\hat\beta_{1,ols}}{\hat\beta_{2,ols}}\]
\end{itemize}
\subsection{Hypothesis testing}
\label{sec-2-6}
\paragraph{similarities to OLS/GLS}
\label{sec-2-6-1}

\begin{itemize}
\item $\hat\beta_{SUR}$ is the estimate of $\beta_1$ and $\beta_2$
        stacked on top of eachother
\item We can test hypotheses about the $\beta$ as usual
\begin{itemize}
\item hypotheses about a single equation
          \[ \beta_{1,1} = 0 \]
          for example
\item hypotheses about coefficients from different equations
\begin{itemize}
\item For example:
\begin{itemize}
\item $Y_1$ is log consumption of one good (imagine an individual)
\item $Y_2$ is log consumption of another, similar good
\item Regressors include log prices
\item may want to test that the elasticty of both goods are
              the same
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}
\paragraph{nature of hypothesis}
\label{sec-2-6-2}

\begin{itemize}
\item We can write both sorts of hypotheses as
        \[ R \beta = q \]
\item the variance of $R\hat\beta - R\beta$ is given by $R(X'(\Sigma
        \otimes I)^{-1}X)^{-1}R'$
\item Under the null, 
        \[(R\hat\beta - q)'\left(R(X'(\Sigma \otimes
        I)^{-1}X)R'\right)^{-1}(R\hat\beta - q)\] is asymptotically
        chi-square with $J$-degrees of freedom
\item Since $\hat\Sigma$ is consistent for $\Sigma$, so is
        \[(R\hat\beta - q)'\left(R(X'(\hat\Sigma \otimes
        I)^{-1}X)R'\right)^{-1}(R\hat\beta - q)\]
\end{itemize}