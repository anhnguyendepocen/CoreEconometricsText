% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Finite sample properties}%
\addcontentsline{toc}{part}{Finite sample properties}

\section{algebraic properties}
\subsection{draw picture to explain multicollinearity}

start with writing $Y$ and $X$ explicitly (for n = 3)
\begin{itemize}
\item Y = (1.5, 3, 0.5)
\item X is given by

  \begin{center}
    \begin{tabular}{rrr}
      1  &  3  &    2  \\
      1  &  1  &    1  \\
      1  &  2  &  1.5  \\
    \end{tabular}
  \end{center}

\item draw plot and show that the third column is in the plane
  generated by the first two
\item obviously, we're not going to be able to estimate the
  coefficients separately
\item we can still estimate the fitted values and they're not going to
  be any different than if we dropped one of the columns.
\end{itemize}

\subsection{draw picture to explain adding an observation}
start with the first two observations from above, with just the 2nd
column of $X$.
\begin{itemize}
\item draw projection in $\RR²$.
\item add third observation and draw $\RR³$.
  \begin{itemize}
  \item it seems like there should be a way to update the estimate of
    $β$ using the previous one and the new observation
  \item there is (you get to derive it for homework, but you don't
    need to know the formula by memory).
  \end{itemize}
\end{itemize}

\subsection{draw a picture to explain adding a variable}

\begin{itemize}
\item suppose you regress $Y$ on $X₁$ and get an estimate $\tilde β₁$,
  then decide to regress $Y$ on $X₁$ and $X₂$, giving $\βh₁$ and
  $\βh₂$.
\item Frisch-Waugh-Lovel Theorem (useful for getting coefficent
  estimates for $X₂$
  \begin{itemize}
  \item let $e = Y - X₁\tilde β$.
  \item define $Z = (I - X₁(X₁'X₁)X₁')X₂$
    \begin{itemize}
    \item residuals from regressing each column of $X₂$ on $X₁$.
    \end{itemize}
  \item $\βh₂ = (Z'Z)^{-1}Z'e$
  \end{itemize}
\item similar formulae exist for getting $\βh₁$ from $\tilde β₁$
\item can be useful with massive datasets
  \begin{itemize}
  \item or for understanding code
  \end{itemize}
\item Implication: what if $X₁$ and $X₂$ are orthogonal?
  \begin{itemize}
  \item reduces to OLS on the second regressor
  \end{itemize}
\end{itemize}

\subsection{R-square}

\begin{itemize}
\item draw picture again.
\item may want to know how far $Y$ is from $\Yh$
\item a good comparison is how far relative to the distance between
  $Y$ and $ι \Yb$
\item ie the ratio 
  $\frac{ \lVert Y - \Yh \rVert₂²}{\lVert Y - ι \Yb \rVert²}$
  \begin{itemize}
  \item centered R-squared
  \item if model contains a constant, this will be between zero and
    one.
  \end{itemize}
\end{itemize}


\section{Properties of quadratic forms}

\begin{itemize}

\item Quadratic forms will come up a lot when we study linear
  regression.  If $X$ be a random $k$-vector and let $A$ be a $k × k$
  deterministic matrix then $X'A X$ is a (random) quadratic form.
  These are (relatively) easy to study because they can be written as
  summations:
  \begin{equation*}
    X'AX = ∑_{i,j} X_i X_j A_{ij}.
  \end{equation*}

\item Usually, when we discuss quadratic forms, we assume (at least
  implicitly) that $A$ is symmetric.  This assumption is usually
  without loss of generality because
  \begin{equation*}
    X' A X = X'(A/2 + A'/2)X
  \end{equation*}
  almost surely, and $X'(A/2 + A'/2)X$ is symmetric by construction.

\item Here's a representative result for quadratic forms that
  illustrates a useful trick.
  \begin{thm}
    Let $X$ be an $n × 1$ vector of random variables, and
    let $A$ be an $n × n$ symmetric matrix.  If $\E X = μ$ and $\var(X) =
    Σ$, then
    \begin{equation}
      \E X'AX = \tr(A Σ) + μ'Aμ.
    \end{equation}
  \end{thm}

  The proof uses three concepts that show up often: a scalar equals
  its own trace, linearity of the expectation, and a property of the
  trace: $\tr(XY) = \tr(YX)$ as long as both are conformable (the last
  property is difficult to see, so write out the matrix operations to
  convince yourself it is true).
  \begin{align*}
    \E(X'AX) &= \tr(\E(X'AX)) \\
    &= \E(\tr(X'AX)) \\
    &= \E(\tr(AXX')) \\
    &= \tr(\E(AXX')) \\
    &= \tr(A \E(XX')) \\
    &= \tr(A (\var(X) + μμ')) \\
    &= \tr(AΣ) + \tr(A μμ') \\
    &= \tr(AΣ) + μ'Aμ.
  \end{align*}

\item Similar results obviously exist for other moments, but the
  algebra isn't quite as nice (and doesn't illuminate any key
  technique other than raw tenacity).  Here's an example for the
  variance under a few simplifying assumptions (this particular
  statement comes from \citealp{SL03})
  \begin{thm}
    let $X₁,...,X_n$ be independent random variables with means
    $θ₁,...,θ_n$, and the same 2nd, 3rd and 4th central moments $μ₂$,
    $μ₃$, $μ₄$.  If $A$ is an $n × n$ symmetric matrix and $a$ is a
    column vector of the diagonal elements of $A$, then
    \begin{equation*}
      \var(X'AX) = 
      (μ₄ - 3 μ₂²)a'a + 2 μ₂² \tr(A²) + 4 μ₂ θ'A² θ + 4 μ₃ θ' A a.
    \end{equation*}
  \end{thm}
  To prove the result, take
  \begin{equation*}
    X'A X = ∑_{ij} ((X_i - θ_i) + θ_i) ((X_j - θ_j) + θ_j) A_{ij},
  \end{equation*}
  multiply out and take the variance.

\end{itemize}

\section{Gaussian random variables}

\begin{itemize}

\item The \emph{Normal distribution} is particularly important when
  studying linear regression.  To the extent that we have finite
  sample results, many will hold only if the errors are Normal.
  Moreover, whenever we use the Central Limit Theorem, we get an
  approximately normal random variable.

\item If $X$ is an $N(μ, Σ)$ random vector, its density is
  \begin{equation*}
    f(x) = \frac{1}{\sqrt{2ⁿ πⁿ \det(Σ)}} exp(-1/2(x - μ)'Σ^{-1}(x- μ)).
  \end{equation*}
  The parameters $μ$ and $Σ$ can be shown to be the mean and variance
  of the r.v. and completely determine its distribution.

  [you should draw the shape of the density and some elliptical
  contour plots]

  If $X$ is a $k$-dimensional multivariate normal with mean $μ$
  and variance $Σ$, $A X + b$ is also multivariate normal for any
  constant $n × k$ matrix $A$ and $n$-vector $b$.

  If $X$ is multivariate normal with mean $μ$ and variance $Σ$,
  and $Σ^{1/2}$ is a symmetric matrix such that $Σ^{1/2} Σ^{1/2} = Σ$,
  then $(Σ^{1/2})^{-1} (X - μ)$ is multivariate standard normal.

\item Independence of normal random variables is especially easy: if
  $X$ and $Y$ are uncorrelated normal random vectors, then they are
  independent.  The result follows from simply multplying out the
  square terms in the density function and then factoring it.

  Marginal and conditional distributions are also especially easy to
  work with.  If 
  \begin{equation*}
    (X₁,X₂) ∼ N((μ₁,μ₂), Σ)
  \end{equation*}
  with
  \begin{equation*}
    Σ = \begin{pmatrix}
      Σ_{11} & Σ_{12} \\ Σ_{12}' & Σ_{22}
    \end{pmatrix}
  \end{equation*}
  then $X₁ ∼ N(μ₁, Σ_{11})$, $X₂ ∼ N(μ₂, Σ_{22})$, and
  \begin{equation*}    
    X₁ ∣ X₂ ∼ N(μ₁ + Σ_{12} Σ_{22}^{-1} (X₂ - μ₂),
               Σ_{11} - Σ_{12}'Σ_{22}^{-1} Σ_{12}).
  \end{equation*}
  Notice that the conditional mean of $X₁$ depends on $X₂$, but the
  conditional variance doesn't.

\item The fact that zero correlation implies independence also makes
  it easy to determine whether functions of normal r.v.s are
  independent.  Linear combinations and quadratic forms of normal
  r.v.s will come up often, so we'll single those results out.  For
  the next results, let $X$ be a $k$-dimensional standard normal
  random vector (in practice, we will often work with $Σ^{-1/2} X$).
  \begin{enumerate}
  \item If $A$ is a $k × j$ matrix and $B$ a $k × l$ matrix such that
    $A'B = 0$, then $A'X$ and $B'X$ are independent.
  \item If $P$ is a $k × k$ idempotent matrix and $PB = 0$ then $X'PX$
    and $B'X$ are independent (an idempotent matrix is a square and
    symmetric matrix that satisfies $P = PP$).
  \item If $P$ and $Q$ are idempotent and $PQ = 0$ then $X'PX$ and
    $X'QX$ are independent.
  \end{enumerate}
  
\item Idempotent matrices come up often, as they represent projections
  in $\RRⁿ$.  If $z$ is a point in $\RRⁿ$ and $x₁,...,x_k$ is a set of
  vectors in $\RRⁿ$, we may want to project $z$ into the space spanned
  by $x₁,...,x_k$.  This amounts to finding a linear operator $P$ so
  that $P z$ is a linear combination of the $x_i$'s and $P z$ is as
  close as possible to $z$ subject to the first constraint.

  We'll worry about making sure that $Pz$ is a linear combination of
  the $x_i$'s later—that's going to be the focus of linear regression.
  But to ensure that $P z$ is as close as possible to $z$, we just
  need to have $z - Pz$ and $Pz$ perpendicular, so $(z - Pz)' Pz = 0$.
  But this requires that $z'(I - P)'P z = 0$ for any $z$, so $(I -
  P)'P = 0$, which amounts to our condition that $P = PP$.

  An idempotent matrix, $P$, has the property that all of its
  eigenvalues must be zero or one, so $\rank(P) = \tr(P)$.

\item Two statistics that are particularly relevant to the normal
  distribution are the sample mean, $\Xb = (1/n) ∑_{i=1}^n X_i$, and
  the sample variance, $S² = (1/(n-1)) ∑_{i=1}^n (X_i - \Xb)²$.  Note
  that we can write $\Xb$ as a linear combination of the $X_i$'s:
  \begin{equation*}
    \Xb = (1/n,...,1/n) · (X₁,...,X_n)' = (1/n) ι'X,
  \end{equation*}
  where $ι$ is a vector of $n$ ones, and
  \begin{equation*}
    S² = (1/(n-1)) (X - ι \Xb)'(X - ι \Xb) = (1/(n-1)) X'(I - (1/n) ιι')X.
  \end{equation*}
  If $X ∼ N(μ, σ² I)$ then we have $\Xb ∼ N(μ, (σ²/n) I)$.  Also,
  since $(I - (1/n) ιι')$ is idempotent, we know right away that $\Xb$
  and $S²$ are independent.

\item Some derived distributions are important

\item %
  \begin{defn}
    Let $X₁,...,X_k$ be independent standard normal.  The
    chi-squared distribution with $k$ degrees of freedom is the
    distribution of $∑_{i=1}^k X²_i$.
  \end{defn}

  \begin{thm}
    Let $X ∼ N(0, I_n)$ and let $P$ be a symmetric $n × n$ matrix.
    Then $X'PX$ is chi-squared with $k$ degrees of freedom iff $P$ is
    idempotent with rank $k$.
  \end{thm}

  \begin{thm}
    if $X$ and $Y$ are independent chi-squared with $df_X$ and $df_Y$
    degrees of freedom, then $X + Y$ is chi-square with $df_X + df_Y$
    degrees of freedom.
  \end{thm}

  If $X ∼ N(μ, σ² I)$ then $S²$ is chi-square with $n-1$ degrees of
  freedom.

\item %
  \begin{defn}
    If $X$ is a standard normal and $Y$ is a chi-squared with $n$ dof
    and $X$ and $Y$ are independent, then $X / \sqrt{Y/n}$ is a $t(n)$
    random variable.
  \end{defn}
  From the quadratic form results, this is going to come up when $Z$
  is multivariate standard normal and $X = B'Z$, $Y = Z'PZ$, and $B'P
  = 0$.

\item %
  \begin{defn}
    If $X$ and $Y$ are independent chi-squared with $df_X$ and $df_Y$
    degrees of freedom, then $\frac{X/df_X}{Y/df_Y}$ is an $F(df_X,
    df_Y)$ random variable.
  \end{defn}

  Expect this to come up when $Z$ is multivariate standard normal and
  $X = Z'QZ$, $Y = Z'PZ$ and $Q$ and $P$ are idempotent and orthogonal.

\end{itemize}


\section{Optimality}

\begin{itemize}

\item Example of the Cramer-Rao lower bound
\begin{itemize}
\item usual setup
\begin{itemize}
\item $(X_i,Y_i) ∼ i.i.d.$
\item $Y_i ∣ X_i ∼ N(β₀ + β₁ X_i, σ²)$
\item $X_i ∼ f$
\item $\βh = (X'X)^{-1} X'Y$ with
\begin{itemize}
\item $X = \begin{pmatrix} 1 & X₁ \\ ⋮ \\ 1 & X_n \end{pmatrix}$
\item $Y = (Y₁, ..., Y_n)$
\end{itemize}
\end{itemize}
\item $\βh$ is unbiased:
\begin{itemize}
\item even stronger, $\E(\βh ∣ X) = β$ (which implies unbiasedness)
  \begin{align*}
    \E(\βh ∣ X) &= \E((X'X)^{-1} X'Y ∣ X) \\
    &= (X'X)^{-1}X' \E(Y ∣ X) \\
    &= (X'X)^{-1}X'Xβ \\
    &= β
  \end{align*}
\end{itemize}
\item We can calculate conditional and unconditional variance of $\βh$:
  \begin{align*}
    \var(\βh ∣ X) &= \E((\βh - β)(\βh - β)' ∣ X) \\
    &= \E((X'X)^{-1}X'\ep\ep'X(X'X)^{-1} ∣ X) \\
    &= (X'X)^{-1}X' \E(\ep\ep' ∣ X) X(X'X)^{-1} \\
    &= σ² (X'X)^{-1}
  \end{align*}
\item $\var(\βh) = \E \var(\βh ∣ X) + \var(\E(\βh ∣ X)) = σ² \E(X'X)^{-1}$
\end{itemize}
\item Is this the best unbiased estimator?
\begin{itemize}
\item unconditional exercise:
\begin{itemize}
\item we know that any unbiased estimator has variance greater than or equal to
  \begin{equation*}
    \begin{split}
      \Big[\E \Big[\tfrac{d}{dθ} \log & L(θ; X)) (\tfrac{d}{dθ} \log L(θ; X))'\Big]\Big]^{-1} \\
      &= \Big[σ^{-4} ∑_i \E (y_i - β₀ - β₁ x_i ∣ x_i)² \begin{pmatrix} 1 & x_i \\ x_i & x²_i \end{pmatrix}\Big]^{-1} \\
      &= σ² (\E X'X)^{-1}
    \end{split}
  \end{equation*}
\item so we don't know.
\item But, this equals the variance if $X$ is deterministic
\item Suggests that we co this conditional on $X$
\end{itemize}
\item conditional exercise
\begin{itemize}
\item we know that any estimator $b$ s.t. $\E(b ∣ X) = β$ has conditional variance greater than or equal to
\begin{equation*}
  \Big[\E\Big(\tfrac{d}{dθ} \log L(θ; X)) (\tfrac{d}{dθ} \log L(θ; X))' ∣ X\Big)\Big]^{-1}
  = σ² (X'X)^{-1}
\end{equation*}
\item so $\βh$ achieves lower bound conditional on $X$
\item holds unconditonally as well, too.
\end{itemize}
\end{itemize}
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../linearregression"
%%% End: 
