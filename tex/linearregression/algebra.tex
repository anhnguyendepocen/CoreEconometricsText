% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Algebraic properties of OLS}%
\addcontentsline{toc}{part}{Algebraic properties of OLS}

\section{algebraic properties}
\subsection{draw picture to explain multicollinearity}

start with writing $Y$ and $X$ explicitly (for n = 3)
\begin{itemize}
\item Y = (1.5, 3, 0.5)
\item X is given by

  \begin{center}
    \begin{tabular}{rrr}
      1  &  3  &    2  \\
      1  &  1  &    1  \\
      1  &  2  &  1.5  \\
    \end{tabular}
  \end{center}

\item draw plot and show that the third column is in the plane
  generated by the first two
\item obviously, we're not going to be able to estimate the
  coefficients separately
\item we can still estimate the fitted values and they're not going to
  be any different than if we dropped one of the columns.
\end{itemize}

\subsection{draw picture to explain adding an observation}
start with the first two observations from above, with just the 2nd
column of $X$.
\begin{itemize}
\item draw projection in $\RR²$.
\item add third observation and draw $\RR³$.
  \begin{itemize}
  \item it seems like there should be a way to update the estimate of
    $β$ using the previous one and the new observation
  \item there is (you get to derive it for homework, but you don't
    need to know the formula by memory).
  \end{itemize}
\end{itemize}

\subsection{draw a picture to explain adding a variable}

\begin{itemize}
\item suppose you regress $Y$ on $X₁$ and get an estimate $\tilde β₁$,
  then decide to regress $Y$ on $X₁$ and $X₂$, giving $\βh₁$ and
  $\βh₂$.
\item Frisch-Waugh-Lovel Theorem (useful for getting coefficent
  estimates for $X₂$
  \begin{itemize}
  \item let $e = Y - X₁\tilde β$.
  \item define $Z = (I - X₁(X₁'X₁)X₁')X₂$
    \begin{itemize}
    \item residuals from regressing each column of $X₂$ on $X₁$.
    \end{itemize}
  \item $\βh₂ = (Z'Z)^{-1}Z'e$
  \end{itemize}
\item similar formulae exist for getting $\βh₁$ from $\tilde β₁$
\item can be useful with massive datasets
  \begin{itemize}
  \item or for understanding code
  \end{itemize}
\item Implication: what if $X₁$ and $X₂$ are orthogonal?
  \begin{itemize}
  \item reduces to OLS on the second regressor
  \end{itemize}
\end{itemize}

\subsection{R-square}

\begin{itemize}
\item draw picture again.
\item may want to know how far $Y$ is from $\Yh$
\item a good comparison is how far relative to the distance between
  $Y$ and $ι \Yb$
\item ie the ratio 
  $\frac{ \lVert Y - \Yh \rVert₂²}{\lVert Y - ι \Yb \rVert²}$
  \begin{itemize}
  \item centered R-squared
  \item if model contains a constant, this will be between zero and
    one.
  \end{itemize}
\end{itemize}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../linearregression"
%%% End: 
