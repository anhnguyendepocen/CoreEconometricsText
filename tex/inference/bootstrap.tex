% Copyright © 2013, Gray Calhoun.  Permission is granted to copy,
% distribute and/or modify this document under the terms of the GNU
% Free Documentation License, Version 1.3 or any later version
% published by the Free Software Foundation; with no Invariant
% Sections, no Front-Cover Texts, and no Back-Cover Texts.  A copy of
% the license is included in the section entitled "GNU Free
% Documentation License."

\chapter{Bootstrap}

\begin{itemize}
\item Plan to cover \textit{2010-10-12 Tue}, \textit{2011-10-21 Fri}
\item Basic premise: (and use the computer for this, obviously)
\begin{itemize}
\item say we have a statistic $\bar X$, $n = 100$, data are iid.
\begin{itemize}
\item n <- 100
\item X <- rlogis(n) + 2
\end{itemize}
\item LLN says that $\bar X$ is going to be close to $E X_1$ as long
       as $n$ is large (which it is)
\item CLT says that $\sqrt{n} (\bar X - E X_1)$ is going to be
       approximately $N(0, var X_1)$
\item All of this holds regardless of the distribution of the
       $X_i$-s.
\item Do some histograms for normal, uniform, and exponential distributions
\begin{itemize}
\item nsim = 2000
\item hist(replicate(nsim, mean(rnorm(n))), 100)
\item hist(replicate(nsim, mean(runif(n))), 100)
\item hist(replicate(nsim, mean(rexp(n))), 100)
\end{itemize}
\item So, if we want to know things about the distribution of $\bar
       X$, we have a few options
\begin{itemize}
\item use the approximate normal distribution
\item choose an arbitrary distribution that is convenient to work
         with, and calculate the \underline{exact} distribution of $\bar X$ if
         that were the correct distribution
\begin{itemize}
\item assuming the X's are normal would probably be the easiest here
\item could assume any distribution and use the computer to
           calculate the distribution of $\bar X$ numerically
\begin{description}
\item[→] plot(ecdf(replicate(20, mean(rexp(n)))))
\item[→] plot(ecdf(replicate(2000, mean(rexp(n)))))
\end{description}
\end{itemize}
\item choose an abitrary distribution that has some optimality
         properties. (ie is likely to be closest to the true unknown
         distribution).
\begin{itemize}
\item so, we observe X and don't know it's distribution.  What
           distribution is likely to be closest to the unknown F?
\item Imagine that we're estimating F.
\item $\hat F$ = empirical distribution of the X's
\begin{description}
\item[→] plot(ecdf(X))
\end{description}
\item So, at most basic level, we can use the computer to
           calculate the distribution, etc, of $\bar X$ under the
           assumption that the true distribution of the Xs is $\hat F$
\begin{description}
\item[→] sometimes we know more about the distribution, or don't
             believe in iid, or need to impose more constraints, and
             then we'll use different $\hat F$-s
\item[→] talk more about this later
\end{description}
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}
\section{variance estimation}
\label{sec-1}

    So, keep the same setup and suppose that we want to estimate the
    variance of $\bar X$
\subsection{bootstrap approach:}
\label{sec-1-1}

\begin{itemize}
\item calculate the empirical distribution function, $\hat F$
\item draw a random sample of size 100 (n) from $\hat F$ with
         replacement and calculate the mean on that sample
\item draw many more random samples of size 100, and calculate the
         mean for each one, until you have $B$ total values of the mean
         (B should be large).
\begin{itemize}
\item call them $\bar X_1^*,\dots, \bar X_B^*$
\end{itemize}
\item calculate the variance of the bootstrap means.
\end{itemize}
\subsection{code}
\label{sec-1-2}

\begin{itemize}
\item drawing a random sample from $\hat F$ is easy:
\begin{itemize}
\item sample(X, n, replace = TRUE)
\end{itemize}
\item calculating the mean is easy
\begin{itemize}
\item mean(sample(X, n, replace = TRUE))
\item do a few times so they see it's random
\end{itemize}
\item doing this B times is easy
\begin{itemize}
\item B <- 600
\item replicate(B, mean(sample(X, n, replace = TRUE)))
\end{itemize}
\item calculating the variance is easy
\begin{itemize}
\item var(replicate(B, mean(sample(X, n, replace = TRUE))))
\end{itemize}
\end{itemize}
\subsection{comments}
\label{sec-1-3}

\begin{itemize}
\item obviously, this isn't a huge advantage over
\begin{itemize}
\item var(X) / n
\end{itemize}
\item suppose that the statistic of interest is $(\bar X)^(1.2)$
\begin{itemize}
\item bootstrap approach doesn't change
\begin{itemize}
\item var(replicate(B, mean(sample(X, n, replace = TRUE))$^1$.2))
\end{itemize}
\item calculating variance directly is now going to be hard
\item probably need to use delta method to get approximate variance
\end{itemize}
\item the bootstrap approximation is usually more accurate than
       other approximations (see Hall's book, ``the bootstrap and the
       edgeworth expansion for details)
\item doesn't always work (need conditions like you need for delta
       method to work)
\end{itemize}
\section{hypothesis testing}
\label{sec-2}

\begin{itemize}
\item might want to use bootstrap to test the null hypothesis that 
      $E X_1 = 0$ (say) against two-sided alternative
\item so you want to use the bootstrap to approximate the distribution
      of $\bar X$ under the null hypothesis.
\end{itemize}
\subsection{naive previous approach (there is an error here!)}
\label{sec-2-1}

\begin{itemize}
\item draw $B$ samples from $\hat F$ and calculate $\bar X_1^*,\dots
       \bar X_B^*$
\begin{itemize}
\item the empirical distribution of the $\bar X_i^*$ approximates
         the unknown distribution of $\bar X$
\item plot(ecdf(replicate(B, mean(sample(X, n, replace=TRUE)))))
\end{itemize}
\item reject the null if $\bar X$ (the original mean) is in the tails
       (ie less than the $\alpha/2$-percentile or greater than the
       $1-\alpha/2$-percentile
\begin{itemize}
\item quantile(replicate(B, mean(sample(X, n, replace=TRUE))), 
         c(0.025, 0.975))
\item mean(X)
\end{itemize}
\end{itemize}
\subsection{correct approach}
\label{sec-2-2}

\begin{itemize}
\item we fail to reject, but the mean is basically 2, and the
       variance is small
\item look at empirical distribution of bootstrap means again
\begin{itemize}
\item plot(ecdf(replicate(B, mean(sample(X, n, replace=TRUE)))))
\end{itemize}
\item is this anything like the distribution under the null that $E
       X_1 = 0$?
\begin{itemize}
\item of course not
\item the mean is nowhere near zero
\end{itemize}
\item so, the key idea is that we have to estimate the distribution
       of F while imposing the null hypothesis.
\begin{itemize}
\item here it's easy:
\begin{itemize}
\item don't sample from $X_1,\dots,X_n$
\item sample from $X_1 - \bar X, X_2 - \bar X,\dots, X_n - \bar X$
\begin{itemize}
\item plot(ecdf(X - mean(X)))
\end{itemize}
\item guaranteed to have mean zero, regardless of the true mean
           of the random variables.
\end{itemize}
\end{itemize}
\item computationally:
\begin{itemize}
\item to get the null distribution:
\begin{itemize}
\item plot(ecdf(replicate(B, mean(sample(X - mean(X), n, replace = TRUE)))))
\end{itemize}
\item now testing gives us:
\begin{itemize}
\item quantile(replicate(B, mean(sample(X - mean(X), n, replace = TRUE))), c(0.025, 0.975))
\end{itemize}
\item this basically rejects
\end{itemize}
\end{itemize}
\subsection{parting words (need to check this!)}
\label{sec-2-3}

\begin{itemize}
\item main analogy: population is to sample as sample is to bootstrap
\begin{itemize}
\item we are creating a ``bootstrap world'' that we can analyze
         exactly
\begin{itemize}
\item we can see the sample; so if we take that as the ``true''
           distribution, we can calculate everything we want
\end{itemize}
\item this ``bootstrap world'' is an approximation to the real world.
\end{itemize}
\item for testing, usually works better if you ``studentize'' the
       statistic first
\begin{itemize}
\item sample from $X - \bar X / \hat\sigma$ instead of $X - \bar X$
\item compare to $\bar X / \hat\sigma$
\end{itemize}
\item again, doesn't always work
\begin{itemize}
\item rough rule of thumb: works when the delta-method does
\begin{itemize}
\item normal statistics, smooth transformations
\end{itemize}
\item when it doesn't work, there are other computationally
         intensive techniques that can
\begin{itemize}
\item subsampling
\end{itemize}
\end{itemize}
\item There are lots of issues we don't touch on
\begin{itemize}
\item bootstrap with heterogeneous or dependent data
\item Davidson and Hinkley (1997) \emph{Bootstrap Methods and their
    Application} is a good resource, and has an R package to do a lot
  of bootstraps.
\end{itemize}
\end{itemize}